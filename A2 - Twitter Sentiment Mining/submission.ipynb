{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.13 64-bit ('col772_a2': conda)"
    },
    "interpreter": {
      "hash": "47824db9596938d92c445fad5d5791b4eb715b27479fa3e5bf7a5307ac59afb6"
    },
    "colab": {
      "name": "submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOJ3T0WWDGRo",
        "outputId": "9edc3ef1-4550-4024-d3f9-e08fc3a0d3d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij7awMq6n3mA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDF0OMEqn3mB"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjv0vD98n3mC",
        "outputId": "1215a54f-57e1-4978-8a66-31905f6c5d5d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import ngrams, FreqDist\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX7SViZ8n3mD"
      },
      "source": [
        "data_neg = pd.read_csv('/content/drive/MyDrive/COL772_A2/training_negative.csv', encoding='ISO-8859-1')\n",
        "data_pos = pd.read_csv('/content/drive/MyDrive/COL772_A2/training_positive.csv', encoding='ISO-8859-1')"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHmk4NfMn3mD"
      },
      "source": [
        "data = data_neg.append(data_pos)\n",
        "data = data.iloc[:,1:]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OeM4QErjn3mD",
        "outputId": "ba06c3ae-ded8-440e-a6d5-f275124de4e6"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>799995</th>\n",
              "      <td>4</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799996</th>\n",
              "      <td>4</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799997</th>\n",
              "      <td>4</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799998</th>\n",
              "      <td>4</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799999</th>\n",
              "      <td>4</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet\n",
              "799995         4  Just woke up. Having no school is the best fee...\n",
              "799996         4  TheWDB.com - Very cool to hear old Walt interv...\n",
              "799997         4  Are you ready for your MoJo Makeover? Ask me f...\n",
              "799998         4  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "799999         4  happy #charitytuesday @theNSPCC @SparksCharity..."
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZGO7-aen3mE"
      },
      "source": [
        "## Sampling Data for Trying out Approaches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Vw9pS_JFn3mF",
        "outputId": "155cd77d-53d0-4e3a-a8db-36d93c22c565"
      },
      "source": [
        "df = data.sample(frac=0.05, random_state=1)\n",
        "# df = data.iloc[:30000]\n",
        "df.head()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet\n",
              "514293         0  i miss nikki nu nu already  shes always there ...\n",
              "142282         0  So I had a dream last night. I  remember a sig...\n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...\n",
              "649503         0                               it is raining again \n",
              "610789         0          @MissKeriBaby wish I was in LA right now "
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BrvtpU7FRvyX",
        "outputId": "9ceca2b9-3e0a-461c-e40e-db8c69781835"
      },
      "source": [
        "df['Polarity'] = np.where(df['Polarity'] == 4, 1, 0)\n",
        "df.head()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet\n",
              "514293         0  i miss nikki nu nu already  shes always there ...\n",
              "142282         0  So I had a dream last night. I  remember a sig...\n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...\n",
              "649503         0                               it is raining again \n",
              "610789         0          @MissKeriBaby wish I was in LA right now "
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JvYcGmUn3mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040d7b37-c0cb-4e2c-add7-72c2d63ec733"
      },
      "source": [
        "df['Polarity'].value_counts()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    40161\n",
              "0    39839\n",
              "Name: Polarity, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhQDTGn9TBCU"
      },
      "source": [
        ""
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHgFsPywTLUq"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_n15VzRTP0M",
        "outputId": "6e57da7f-094c-4c66-da21-8fc2f5989975"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb-3Cs4YTHlV",
        "outputId": "91705731-09df-44cf-bf65-fbe7a430aea3"
      },
      "source": [
        "example = 'The movie was awesome.'\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "sid.polarity_scores(example)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.6249, 'neg': 0.0, 'neu': 0.423, 'pos': 0.577}"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y96_jAoATA6e"
      },
      "source": [
        "# df[['neg', 'neu', 'pos', 'compound']] = df['Tweet'].apply(sid.polarity_scores).apply(pd.Series)\n",
        "# df.head()"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fw0woabTAyl"
      },
      "source": [
        "# df['Polarity_Vader'] = np.where(df['compound'] > 0, 1, 0)\n",
        "# df.head()"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_448sLTSjJC"
      },
      "source": [
        "# (sum(df['Polarity'] == df['Polarity_Vader'])/len(df))*100"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ChXN_JdUO-H"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aERkZDswUG96"
      },
      "source": [
        "# for var in ['pos', 'neg', 'neu', 'compound']:\n",
        "#     plt.figure(figsize=(12,4))\n",
        "#     sns.distplot(df.query(\"Polarity==1\")[var], bins=30, kde=False, \n",
        "#                  color='green', label='Positive')\n",
        "#     sns.distplot(df.query(\"Polarity==0\")[var], bins=30, kde=False, \n",
        "#                  color='red', label='Negative')\n",
        "#     plt.legend()\n",
        "#     plt.title(f'Histogram of {var} by true sentiment');"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpBGwQyUG2O"
      },
      "source": [
        ""
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4CR6ElnUGue"
      },
      "source": [
        ""
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyKQ93H8UGnF"
      },
      "source": [
        ""
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB2-ZFyun3mF"
      },
      "source": [
        "## Pre-Processing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eOthrAcn3mF"
      },
      "source": [
        "def clean_text(tweet):\n",
        "    tweet = tweet.lower()                                   # Converting to lower case\n",
        "    tweet = re.sub(r'\\b\\w+@[^\\s]+', ' <MAIL> ', tweet)             # Removing email IDs\n",
        "    tweet = re.sub(r'@[^\\s]+', ' <MENTION> ', tweet)                  # Removing mentions\n",
        "    tweet = re.sub(r'https?:\\/[^\\s]+', ' <URL> ', tweet)          # Removing URLs\n",
        "    tweet = re.sub(r'www.[^\\s]+', ' <WEBSITE> ', tweet)               # Removing Websites\n",
        "    tweet = re.sub(r'\\b\\w+.com', ' <WEBSITE> ', tweet)             # Removing email IDs\n",
        "    tweet = re.sub(r'#', ' <HASHTAG> ', tweet)                         # Removing hashtags\n",
        "    tweet = re.sub(r'_', ' ', tweet)                        # Sometimes hashtags are done with _ representing break between two words\n",
        "    tweet = re.sub(r'\\.{2,}', ' ', tweet)                   # Removing sentence separators\n",
        "    tweet = re.sub(r\"[0-9]+\",' ', tweet)                    # Removing numbers as they do not indicate sentiment\n",
        "    tweet = re.sub(r\"\\bamp\\b\", ' ', tweet)                  # Removing &amp signs mis-translated\n",
        "    tweet = re.sub(r\"\\bquot\\b\", ' ', tweet)  \n",
        "    tweet = re.sub(r\"\\b\\w+;[^\\s]+\\b\", ' ', tweet)  \n",
        "    if len(tweet) == 0:\n",
        "      tweet = 'None'\n",
        "    return ' '.join(tweet.split())"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fjrceg8SIep"
      },
      "source": [
        "def remove_punc(tweet):\n",
        "    tweet = re.sub(r\"[^\\w'\\s]+\",'', tweet)                  # Removing punctuations apart from clitic\n",
        "    return tweet"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2BSMAXPen3mG",
        "outputId": "d9e72051-280b-4f35-bd9b-0612070ca79c"
      },
      "source": [
        "clean_text(\"I am &amp rachit1jain@gmail.com n't #doing_exceptionally good hello.com &gt;&gt:D\") "
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i am & <MAIL> n't <HASHTAG> doing exceptionally good <WEBSITE> &\""
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "-JkhpCcIOmni",
        "outputId": "162f4d28-636f-438d-8c57-c897ba61ca59"
      },
      "source": [
        "clean_text('@')"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'@'"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWiUBKi4n3mH"
      },
      "source": [
        "def tweet_word_tokenizer(tweet):\n",
        "    # return word_tokenize(tweet)\n",
        "    return tweet.split(' ')"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1575G2F4n3mH"
      },
      "source": [
        "clitics = {\n",
        "    \"nt\": 'not',\n",
        "    \"ve\": 'have',\n",
        "    \"s\": 'is',\n",
        "    \"m\": 'am',\n",
        "    \"re\": 'are',\n",
        "    \"ll\": 'will',\n",
        "    'd': 'would',\n",
        "    \"bout\": 'about',\n",
        "    'didnt': 'did not',\n",
        "    'havent': 'have not',\n",
        "    'hasnt': 'has not',\n",
        "    'wont': 'will not',\n",
        "    'wouldnt': 'will not',\n",
        "    'shouldnt': 'should not',\n",
        "}"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2SrN9-hn3mH"
      },
      "source": [
        "# # count = 0\n",
        "# def handle_clitics(tweet):\n",
        "#     # global count\n",
        "#     # count += 1\n",
        "#     for i in range(len(tweet)):\n",
        "#         if tweet[i] in clitics.keys():\n",
        "#             tweet[i] = clitics[tweet[i]]\n",
        "#     return tweet"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBJFKCpfIGc9"
      },
      "source": [
        "def handle_clitics(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"\\bdon't\\b\", \"do not\", phrase)\n",
        "    phrase = re.sub(r\"\\bdoesn't\\b\", \"does not\", phrase)\n",
        "    phrase = re.sub(r\"\\bdidn't\\b\", \"did not\", phrase)\n",
        "    phrase = re.sub(r\"\\bdidnt\\b\", \"did not\", phrase)\n",
        "    phrase = re.sub(r\"\\bhasn't\\b\", \"has not\", phrase)\n",
        "    phrase = re.sub(r\"\\bhaven't\\b\", \"have not\", phrase)\n",
        "    phrase = re.sub(r\"\\bhavent\\b\", \"have not\", phrase)\n",
        "    phrase = re.sub(r\"\\bhadn't\\b\", \"had not\", phrase)\n",
        "    phrase = re.sub(r\"\\bwon't\\b\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"\\bwouldn't\\b\", \"would not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "\n",
        "    # using regular expressions to expand the contractions\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    return phrase"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIwuwU5nn3mH"
      },
      "source": [
        "stop = 0\n",
        "def stopword_removal(tweet):\n",
        "    # stoplist = stopwords.words('english')\n",
        "    stoplist = []\n",
        "    manual_stoplist = ['retweet', 'retwet', 'rt', 'oh', 'dm', 'mt', 'ht', 'ff', 'shoulda', 'woulda', 'coulda', 'might', 'im', 'tb', 'mysql', 'hah', \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\n",
        "                  \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
        "                  \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"nor\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\",\n",
        "                  \"t\", \"just\", \"don\", \"now\", 'tweet', 'x', 'f', 'go', 'get', 'give']\n",
        "    stoplist.append(manual_stoplist)\n",
        "    global stop\n",
        "    stop += 1\n",
        "    if stop % 100 == 0:\n",
        "      print(stop)\n",
        "    # stopwords = stopwords.words('english')\n",
        "    tweet = [word for word in tweet if word not in stoplist]\n",
        "    if len(tweet) == 0:\n",
        "        tweet = ['None']\n",
        "    return tweet"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9le-iVv_MGow"
      },
      "source": [
        "# stopwords.words('english')"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzlKv6Tqn3mI"
      },
      "source": [
        "short_forms = {\n",
        "    'n': 'and',\n",
        "    'ya': 'you',\n",
        "    'luv': 'love',\n",
        "    'lol': 'laugh',\n",
        "    'k': 'okay',\n",
        "    'na': 'no',\n",
        "    'ily': 'love',\n",
        "    'im': 'am',\n",
        "    'morn': 'morning',\n",
        "    'nght': 'night',\n",
        "    'no': 'not',\n",
        "    'Ill': 'will',\n",
        "    'shoulda': 'should have'\n",
        "    }"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmHgKMHcn3mI"
      },
      "source": [
        "def handle_shortforms(tweet):\n",
        "    temp = ''\n",
        "    for word in tweet.split():\n",
        "        if word in short_forms.keys():\n",
        "            temp = temp + ' ' + short_forms[word]\n",
        "        else:\n",
        "            temp = temp + ' ' + word\n",
        "    return ' '.join(temp.split())"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbptB4Iyn3mI"
      },
      "source": [
        "# handle_shortforms(['I','am','lol','in','practice'])"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "UVwFK1SwLLGn",
        "outputId": "8c9c3cad-6410-4858-a357-531e6893ced2"
      },
      "source": [
        "handle_shortforms('I am a good boy shoulda gone')"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am a good boy should have gone'"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Jmy6C2TbQw"
      },
      "source": [
        ""
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhLV9cvETbKJ"
      },
      "source": [
        "## Maintaining only letters within a tweet and removing every other information since not indicative of sentiment\n",
        "def maintain_letters(tweet):\n",
        "    tweet = re.sub(r'[^a-zA-Z]', ' ', tweet)      # since lowering has already been done\n",
        "    return ' '.join(tweet.split())"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8PJMwCsuRipT",
        "outputId": "7d7e98cb-544c-49ce-d892-cef3c29705d4"
      },
      "source": [
        "maintain_letters('i am a good boy. hero is @terohja 909')"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i am a good boy hero is terohja'"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IS3yJElT8sO"
      },
      "source": [
        ""
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzAZlswVRikR"
      },
      "source": [
        "### TO BE MODIFIED ####\n",
        "# Emoticons store a lot of information\n",
        "emo_info = {\n",
        "    # positive emoticons\n",
        "    \":‑)\": \" <EMOJI> \",\n",
        "    \":)\": \" <EMOJI> \",\n",
        "    \";)\": \" <EMOJI> \",\n",
        "    \":-}\": \" <EMOJI> \",\n",
        "    \"=]\": \" <EMOJI> \",\n",
        "    \"=)\": \" <EMOJI> \",\n",
        "    \";d\": \" <EMOJI> \",\n",
        "    \":d\": \" <EMOJI> \",\n",
        "    \":dd\": \" <EMOJI> \",\n",
        "    \"xd\": \" <EMOJI> \",\n",
        "    \"<3\": \" <EMOJI> \",\n",
        "\n",
        "    # negativve emoticons\n",
        "    \":‑(\": \" <EMOJI> \",\n",
        "    \":‑[\": \" <EMOJI> \",\n",
        "    \":(\": \" <EMOJI> \",\n",
        "    \"=(\": \" <EMOJI> \",\n",
        "    \"=/\": \" <EMOJI> \",\n",
        "    \":{\": \" <EMOJI> \",\n",
        "    \":/\": \" <EMOJI> \",\n",
        "    \":|\": \" <EMOJI> \",\n",
        "    \":-/\": \" <EMOJI> \",\n",
        "    \":o\": \" <EMOJI> \"\n",
        "\n",
        "}"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xvTvPst4W74"
      },
      "source": [
        "# def remove_emoji(tweet):\n",
        "#     \":p\": \" tease \",\n",
        "#     \"xp\": \" tease \""
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZnTnjYIRifJ"
      },
      "source": [
        "### TO BE MODIFIED ####\n",
        "emo_info_order = [k for (k_len, k) in reversed(sorted([(len(k), k) for k in emo_info.keys()]))]"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efqoiUqgRiZ6"
      },
      "source": [
        "### TO BE MODIFIED ####\n",
        "def emo_repl(phrase):\n",
        "    for k in emo_info_order:\n",
        "        phrase = phrase.replace(k, emo_info[k])\n",
        "    return phrase"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGR_onqjRmrl"
      },
      "source": [
        ""
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4BzEcEsn3mI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bd97d010-ca05-4002-ddf8-468af390073c"
      },
      "source": [
        "df['Tweet_regex'] = df['Tweet'].apply(clean_text)\n",
        "df.head()"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                        Tweet_regex\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night. i remember a sign...\n",
              "403727         0  ...  <MENTION> ohh poor sickly you (((hugs)) hope y...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...               <MENTION> wish i was in la right now\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdC3YLqLALfj",
        "outputId": "e8a3bb95-84f0-4199-f858-555d7396f581"
      },
      "source": [
        "# EDA\n",
        "df[df['Polarity'] == 0]['Tweet_regex'].str.contains('<MENTION>').value_counts()"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    24832\n",
              "True     15007\n",
              "Name: Tweet_regex, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tc-dv3mALYG",
        "outputId": "5fd54710-5ded-41e0-f209-1e9e6e997fa6"
      },
      "source": [
        "df[df['Polarity'] == 4]['Tweet_regex'].str.contains('<MENTION>').value_counts()"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], Name: Tweet_regex, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUbl4XrSALQY",
        "outputId": "a4ca6722-8dc3-4866-ffdf-198b8970ecd6"
      },
      "source": [
        "# EDA\n",
        "df[df['Polarity'] == 0]['Tweet_regex'].str.contains('<WEBSITE>').value_counts()"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    37821\n",
              "True      2018\n",
              "Name: Tweet_regex, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGKXhexIALJH",
        "outputId": "59493790-1e1c-4574-c56c-b5fc13302db2"
      },
      "source": [
        "# EDA\n",
        "df[df['Polarity'] == 4]['Tweet_regex'].str.contains('<WEBSITE>').value_counts()"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], Name: Tweet_regex, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yVFDX2yA-1I"
      },
      "source": [
        ""
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_1H6HT1A-tT"
      },
      "source": [
        ""
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMFcCwWAA-ju"
      },
      "source": [
        ""
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98U8V2LoAKVp"
      },
      "source": [
        ""
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uQmC6MnKSU4z",
        "outputId": "82c24d24-2743-41d9-bfbf-b4cb92544df5"
      },
      "source": [
        "df['Tweet_emoji'] = df['Tweet_regex'].apply(emo_repl)\n",
        "df.head()"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                        Tweet_emoji\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night. i remember a sign...\n",
              "403727         0  ...  <MENTION> ohh poor sickly you (((hugs)) hope y...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...               <MENTION> wish i was in la right now\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Gm3kWdMAS7NF",
        "outputId": "dc92f3c3-fa3b-4d55-a8fa-c7fa4553e5ea"
      },
      "source": [
        "df['Tweet_nopunc'] = df['Tweet_emoji'].apply(remove_punc)\n",
        "# df['Tweet_nopunc'] = df['Tweet_regex'].apply(remove_punc)         # NOT USING EMOJI\n",
        "df.head()"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                       Tweet_nopunc\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night i remember a sign ...\n",
              "403727         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...                 MENTION wish i was in la right now\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "g8qHipQmJTN-",
        "outputId": "35a43faa-78cf-4d79-b87a-9f0fa69deb40"
      },
      "source": [
        "df['Tweet_clitics'] = df['Tweet_nopunc'].apply(handle_clitics)\n",
        "df.head()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                      Tweet_clitics\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night i remember a sign ...\n",
              "403727         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...                 MENTION wish i was in la right now\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz_VX3C8n3mJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "34ece64b-9790-49c9-dbf9-2084cb98cfd4"
      },
      "source": [
        "df['Tweet_shortforms'] = df['Tweet_clitics'].apply(handle_shortforms)\n",
        "df.head()"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                   Tweet_shortforms\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night i remember a sign ...\n",
              "403727         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...                 MENTION wish i was in la right now\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "80twWXsWTYSc",
        "outputId": "e0d5bc0e-4751-4fbb-d137-ece146abcfcb"
      },
      "source": [
        "df['Tweet_pure_string'] = df['Tweet_shortforms'].apply(maintain_letters)\n",
        "df.head()"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                  Tweet_pure_string\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night i remember a sign ...\n",
              "403727         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...                 MENTION wish i was in la right now\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "jIce-XrfJzM3",
        "outputId": "e90a1f7c-b9ca-4d58-a6b8-f3f74cc2f168"
      },
      "source": [
        "df['Tweet_token'] = df['Tweet_pure_string'].apply(tweet_word_tokenizer)\n",
        "df.head()"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                        Tweet_token\n",
              "514293         0  ...  [i, miss, nikki, nu, nu, already, shes, always...\n",
              "142282         0  ...  [so, i, had, a, dream, last, night, i, remembe...\n",
              "403727         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "649503         0  ...                           [it, is, raining, again]\n",
              "610789         0  ...        [MENTION, wish, i, was, in, la, right, now]\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn8KcPyQn3mK"
      },
      "source": [
        "# Was absolutely useless to use\n",
        "stem = 0\n",
        "def stemmer(tweet):\n",
        "    global stem\n",
        "    stem += 1\n",
        "    if stem % 1000:\n",
        "      print(stem)\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    tweet = [porter_stemmer.stem(word) for word in tweet]\n",
        "    return tweet"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqp-VCQin3mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57078e7d-20a6-44e6-c1c5-4a711fd24005"
      },
      "source": [
        "stemmer(['I','am','playing','making', 'what','I','do'])"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'play', 'make', 'what', 'I', 'do']"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOOQ4c5Mn3mK"
      },
      "source": [
        "# df['Tweet_stem'] = df['Tweet_token'].apply(stemmer)\n",
        "# df.head()"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb1Gs1MXh9fl"
      },
      "source": [
        "def make_sentences(df, col, title):\n",
        "    df[title] = df[col].apply(lambda x:' '.join([i for i in x]))\n",
        "    return df"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQpOgGAxn3mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "d3afe25f-b264-481a-cc8f-ba1de1320508"
      },
      "source": [
        "# df = make_sentences(df, 'Tweet_stem', 'Tweet_sent')\n",
        "# df.head()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Tweet_stem'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-6506c019c179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tweet_stem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tweet_sent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-204-d21c4ff6ba0f>\u001b[0m in \u001b[0;36mmake_sentences\u001b[0;34m(df, col, title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Tweet_stem'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "MDsuR7ZgXOwr",
        "outputId": "76015cb1-e869-4e2c-d747-a412f4eed106"
      },
      "source": [
        "df = make_sentences(df, 'Tweet_token', 'Tweet_final_sent')\n",
        "df.head()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                   Tweet_final_sent\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night i remember a sign ...\n",
              "403727         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...                 MENTION wish i was in la right now\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvvOdXfnhhRN"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7RZ-m11hg__"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_eo4YzXn3mL"
      },
      "source": [
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:         \n",
        "        return None"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzpRmJQvn3mL"
      },
      "source": [
        "#### TO BE MODIFIED ######\n",
        "count = 0\n",
        "def pos_tagging(tweet):\n",
        "    global count\n",
        "    count += 1\n",
        "    if count % 100 == 0:\n",
        "      print(count)\n",
        "    # tweet = nltk.pos_tag(tweet) \n",
        "    tweet = nltk.pos_tag([i for i in tweet if i])\n",
        "    return tweet"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBHCcvl6kLQ5"
      },
      "source": [
        ""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBiJMHxPOVd",
        "outputId": "1d360348-1df3-407b-9f15-24d8fcb20b78"
      },
      "source": [
        "pos_tagging(['','I','am','good'])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('good', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TPMJd4On3mL"
      },
      "source": [
        "### TO BE MODIFIED ################\n",
        "def tweet_lemmatizer(tweet):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = []\n",
        "    pos_wordnet = list(map(lambda x: (x[0], pos_tagger(x[1])), tweet))\n",
        "    for word, tag in pos_wordnet:\n",
        "        if tag is None:\n",
        "            lemmatized.append(word)\n",
        "        else:       \n",
        "            lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
        "    lemmatized = ' '.join(lemmatized)\n",
        "    lemmatized_sent = ', '.join(lemmatized)\n",
        "    # print(list(lemmatized.split()))\n",
        "    return list(lemmatized.split())"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IYdhTT6n3mL"
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eriBp7Gln3mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ce09c5c-d06e-49f8-c27f-d9f132b789e6"
      },
      "source": [
        "df['Tweet_pos'] = df['Tweet_token'].apply(pos_tagging)\n",
        "df.head()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "31300\n",
            "31400\n",
            "31500\n",
            "31600\n",
            "31700\n",
            "31800\n",
            "31900\n",
            "32000\n",
            "32100\n",
            "32200\n",
            "32300\n",
            "32400\n",
            "32500\n",
            "32600\n",
            "32700\n",
            "32800\n",
            "32900\n",
            "33000\n",
            "33100\n",
            "33200\n",
            "33300\n",
            "33400\n",
            "33500\n",
            "33600\n",
            "33700\n",
            "33800\n",
            "33900\n",
            "34000\n",
            "34100\n",
            "34200\n",
            "34300\n",
            "34400\n",
            "34500\n",
            "34600\n",
            "34700\n",
            "34800\n",
            "34900\n",
            "35000\n",
            "35100\n",
            "35200\n",
            "35300\n",
            "35400\n",
            "35500\n",
            "35600\n",
            "35700\n",
            "35800\n",
            "35900\n",
            "36000\n",
            "36100\n",
            "36200\n",
            "36300\n",
            "36400\n",
            "36500\n",
            "36600\n",
            "36700\n",
            "36800\n",
            "36900\n",
            "37000\n",
            "37100\n",
            "37200\n",
            "37300\n",
            "37400\n",
            "37500\n",
            "37600\n",
            "37700\n",
            "37800\n",
            "37900\n",
            "38000\n",
            "38100\n",
            "38200\n",
            "38300\n",
            "38400\n",
            "38500\n",
            "38600\n",
            "38700\n",
            "38800\n",
            "38900\n",
            "39000\n",
            "39100\n",
            "39200\n",
            "39300\n",
            "39400\n",
            "39500\n",
            "39600\n",
            "39700\n",
            "39800\n",
            "39900\n",
            "40000\n",
            "40100\n",
            "40200\n",
            "40300\n",
            "40400\n",
            "40500\n",
            "40600\n",
            "40700\n",
            "40800\n",
            "40900\n",
            "41000\n",
            "41100\n",
            "41200\n",
            "41300\n",
            "41400\n",
            "41500\n",
            "41600\n",
            "41700\n",
            "41800\n",
            "41900\n",
            "42000\n",
            "42100\n",
            "42200\n",
            "42300\n",
            "42400\n",
            "42500\n",
            "42600\n",
            "42700\n",
            "42800\n",
            "42900\n",
            "43000\n",
            "43100\n",
            "43200\n",
            "43300\n",
            "43400\n",
            "43500\n",
            "43600\n",
            "43700\n",
            "43800\n",
            "43900\n",
            "44000\n",
            "44100\n",
            "44200\n",
            "44300\n",
            "44400\n",
            "44500\n",
            "44600\n",
            "44700\n",
            "44800\n",
            "44900\n",
            "45000\n",
            "45100\n",
            "45200\n",
            "45300\n",
            "45400\n",
            "45500\n",
            "45600\n",
            "45700\n",
            "45800\n",
            "45900\n",
            "46000\n",
            "46100\n",
            "46200\n",
            "46300\n",
            "46400\n",
            "46500\n",
            "46600\n",
            "46700\n",
            "46800\n",
            "46900\n",
            "47000\n",
            "47100\n",
            "47200\n",
            "47300\n",
            "47400\n",
            "47500\n",
            "47600\n",
            "47700\n",
            "47800\n",
            "47900\n",
            "48000\n",
            "48100\n",
            "48200\n",
            "48300\n",
            "48400\n",
            "48500\n",
            "48600\n",
            "48700\n",
            "48800\n",
            "48900\n",
            "49000\n",
            "49100\n",
            "49200\n",
            "49300\n",
            "49400\n",
            "49500\n",
            "49600\n",
            "49700\n",
            "49800\n",
            "49900\n",
            "50000\n",
            "50100\n",
            "50200\n",
            "50300\n",
            "50400\n",
            "50500\n",
            "50600\n",
            "50700\n",
            "50800\n",
            "50900\n",
            "51000\n",
            "51100\n",
            "51200\n",
            "51300\n",
            "51400\n",
            "51500\n",
            "51600\n",
            "51700\n",
            "51800\n",
            "51900\n",
            "52000\n",
            "52100\n",
            "52200\n",
            "52300\n",
            "52400\n",
            "52500\n",
            "52600\n",
            "52700\n",
            "52800\n",
            "52900\n",
            "53000\n",
            "53100\n",
            "53200\n",
            "53300\n",
            "53400\n",
            "53500\n",
            "53600\n",
            "53700\n",
            "53800\n",
            "53900\n",
            "54000\n",
            "54100\n",
            "54200\n",
            "54300\n",
            "54400\n",
            "54500\n",
            "54600\n",
            "54700\n",
            "54800\n",
            "54900\n",
            "55000\n",
            "55100\n",
            "55200\n",
            "55300\n",
            "55400\n",
            "55500\n",
            "55600\n",
            "55700\n",
            "55800\n",
            "55900\n",
            "56000\n",
            "56100\n",
            "56200\n",
            "56300\n",
            "56400\n",
            "56500\n",
            "56600\n",
            "56700\n",
            "56800\n",
            "56900\n",
            "57000\n",
            "57100\n",
            "57200\n",
            "57300\n",
            "57400\n",
            "57500\n",
            "57600\n",
            "57700\n",
            "57800\n",
            "57900\n",
            "58000\n",
            "58100\n",
            "58200\n",
            "58300\n",
            "58400\n",
            "58500\n",
            "58600\n",
            "58700\n",
            "58800\n",
            "58900\n",
            "59000\n",
            "59100\n",
            "59200\n",
            "59300\n",
            "59400\n",
            "59500\n",
            "59600\n",
            "59700\n",
            "59800\n",
            "59900\n",
            "60000\n",
            "60100\n",
            "60200\n",
            "60300\n",
            "60400\n",
            "60500\n",
            "60600\n",
            "60700\n",
            "60800\n",
            "60900\n",
            "61000\n",
            "61100\n",
            "61200\n",
            "61300\n",
            "61400\n",
            "61500\n",
            "61600\n",
            "61700\n",
            "61800\n",
            "61900\n",
            "62000\n",
            "62100\n",
            "62200\n",
            "62300\n",
            "62400\n",
            "62500\n",
            "62600\n",
            "62700\n",
            "62800\n",
            "62900\n",
            "63000\n",
            "63100\n",
            "63200\n",
            "63300\n",
            "63400\n",
            "63500\n",
            "63600\n",
            "63700\n",
            "63800\n",
            "63900\n",
            "64000\n",
            "64100\n",
            "64200\n",
            "64300\n",
            "64400\n",
            "64500\n",
            "64600\n",
            "64700\n",
            "64800\n",
            "64900\n",
            "65000\n",
            "65100\n",
            "65200\n",
            "65300\n",
            "65400\n",
            "65500\n",
            "65600\n",
            "65700\n",
            "65800\n",
            "65900\n",
            "66000\n",
            "66100\n",
            "66200\n",
            "66300\n",
            "66400\n",
            "66500\n",
            "66600\n",
            "66700\n",
            "66800\n",
            "66900\n",
            "67000\n",
            "67100\n",
            "67200\n",
            "67300\n",
            "67400\n",
            "67500\n",
            "67600\n",
            "67700\n",
            "67800\n",
            "67900\n",
            "68000\n",
            "68100\n",
            "68200\n",
            "68300\n",
            "68400\n",
            "68500\n",
            "68600\n",
            "68700\n",
            "68800\n",
            "68900\n",
            "69000\n",
            "69100\n",
            "69200\n",
            "69300\n",
            "69400\n",
            "69500\n",
            "69600\n",
            "69700\n",
            "69800\n",
            "69900\n",
            "70000\n",
            "70100\n",
            "70200\n",
            "70300\n",
            "70400\n",
            "70500\n",
            "70600\n",
            "70700\n",
            "70800\n",
            "70900\n",
            "71000\n",
            "71100\n",
            "71200\n",
            "71300\n",
            "71400\n",
            "71500\n",
            "71600\n",
            "71700\n",
            "71800\n",
            "71900\n",
            "72000\n",
            "72100\n",
            "72200\n",
            "72300\n",
            "72400\n",
            "72500\n",
            "72600\n",
            "72700\n",
            "72800\n",
            "72900\n",
            "73000\n",
            "73100\n",
            "73200\n",
            "73300\n",
            "73400\n",
            "73500\n",
            "73600\n",
            "73700\n",
            "73800\n",
            "73900\n",
            "74000\n",
            "74100\n",
            "74200\n",
            "74300\n",
            "74400\n",
            "74500\n",
            "74600\n",
            "74700\n",
            "74800\n",
            "74900\n",
            "75000\n",
            "75100\n",
            "75200\n",
            "75300\n",
            "75400\n",
            "75500\n",
            "75600\n",
            "75700\n",
            "75800\n",
            "75900\n",
            "76000\n",
            "76100\n",
            "76200\n",
            "76300\n",
            "76400\n",
            "76500\n",
            "76600\n",
            "76700\n",
            "76800\n",
            "76900\n",
            "77000\n",
            "77100\n",
            "77200\n",
            "77300\n",
            "77400\n",
            "77500\n",
            "77600\n",
            "77700\n",
            "77800\n",
            "77900\n",
            "78000\n",
            "78100\n",
            "78200\n",
            "78300\n",
            "78400\n",
            "78500\n",
            "78600\n",
            "78700\n",
            "78800\n",
            "78900\n",
            "79000\n",
            "79100\n",
            "79200\n",
            "79300\n",
            "79400\n",
            "79500\n",
            "79600\n",
            "79700\n",
            "79800\n",
            "79900\n",
            "80000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_stem</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "      <th>Tweet_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, alreadi, she, alway, ...</td>\n",
              "      <td>i miss nikki nu nu alreadi she alway there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>miss nikki nu nu alreadi she alway there when ...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>miss nikki nu nu alreadi she alway there when ...</td>\n",
              "      <td>[(i, NN), (miss, VBP), (nikki, JJ), (nu, JJ), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, rememb,...</td>\n",
              "      <td>so i had a dream last night i rememb a sign wh...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>so had dream last night rememb sign which clea...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>so had dream last night rememb sign which clea...</td>\n",
              "      <td>[(so, RB), (i, JJ), (had, VBD), (a, DT), (drea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[(MENTION, NNP), (ohh, PRP), (poor, JJ), (sick...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, rain, again]</td>\n",
              "      <td>it is rain again</td>\n",
              "      <td>[it, is, rain, again]</td>\n",
              "      <td>[it, is, rain, again, NEGATIVE, NEGATIVE, NEGA...</td>\n",
              "      <td>it is rain again</td>\n",
              "      <td>[it, is, rain, again, NEGATIVE, NEGATIVE, NEGA...</td>\n",
              "      <td>it is rain again NEGATIVE NEGATIVE NEGATIVE NE...</td>\n",
              "      <td>[(it, PRP), (is, VBZ), (raining, VBG), (again,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>[mention, wish, i, wa, in, la, right, now]</td>\n",
              "      <td>mention wish i wa in la right now</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now]</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now, NEGATI...</td>\n",
              "      <td>mention wish wa in la right now</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now, NEGATI...</td>\n",
              "      <td>mention wish wa in la right now NEGATIVE NEGAT...</td>\n",
              "      <td>[(MENTION, NNP), (wish, NN), (i, NN), (was, VB...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                                          Tweet_pos\n",
              "0         0  ...  [(i, NN), (miss, VBP), (nikki, JJ), (nu, JJ), ...\n",
              "1         0  ...  [(so, RB), (i, JJ), (had, VBD), (a, DT), (drea...\n",
              "2         0  ...  [(MENTION, NNP), (ohh, PRP), (poor, JJ), (sick...\n",
              "3         0  ...  [(it, PRP), (is, VBZ), (raining, VBG), (again,...\n",
              "4         0  ...  [(MENTION, NNP), (wish, NN), (i, NN), (was, VB...\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tmYuK9li481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e21e6be7-66c6-482a-db4d-6b238d50ce4c"
      },
      "source": [
        "df['Tweet_pos'] = df['Tweet_token'].apply(pos_tagging)\n",
        "df.head()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80100\n",
            "80200\n",
            "80300\n",
            "80400\n",
            "80500\n",
            "80600\n",
            "80700\n",
            "80800\n",
            "80900\n",
            "81000\n",
            "81100\n",
            "81200\n",
            "81300\n",
            "81400\n",
            "81500\n",
            "81600\n",
            "81700\n",
            "81800\n",
            "81900\n",
            "82000\n",
            "82100\n",
            "82200\n",
            "82300\n",
            "82400\n",
            "82500\n",
            "82600\n",
            "82700\n",
            "82800\n",
            "82900\n",
            "83000\n",
            "83100\n",
            "83200\n",
            "83300\n",
            "83400\n",
            "83500\n",
            "83600\n",
            "83700\n",
            "83800\n",
            "83900\n",
            "84000\n",
            "84100\n",
            "84200\n",
            "84300\n",
            "84400\n",
            "84500\n",
            "84600\n",
            "84700\n",
            "84800\n",
            "84900\n",
            "85000\n",
            "85100\n",
            "85200\n",
            "85300\n",
            "85400\n",
            "85500\n",
            "85600\n",
            "85700\n",
            "85800\n",
            "85900\n",
            "86000\n",
            "86100\n",
            "86200\n",
            "86300\n",
            "86400\n",
            "86500\n",
            "86600\n",
            "86700\n",
            "86800\n",
            "86900\n",
            "87000\n",
            "87100\n",
            "87200\n",
            "87300\n",
            "87400\n",
            "87500\n",
            "87600\n",
            "87700\n",
            "87800\n",
            "87900\n",
            "88000\n",
            "88100\n",
            "88200\n",
            "88300\n",
            "88400\n",
            "88500\n",
            "88600\n",
            "88700\n",
            "88800\n",
            "88900\n",
            "89000\n",
            "89100\n",
            "89200\n",
            "89300\n",
            "89400\n",
            "89500\n",
            "89600\n",
            "89700\n",
            "89800\n",
            "89900\n",
            "90000\n",
            "90100\n",
            "90200\n",
            "90300\n",
            "90400\n",
            "90500\n",
            "90600\n",
            "90700\n",
            "90800\n",
            "90900\n",
            "91000\n",
            "91100\n",
            "91200\n",
            "91300\n",
            "91400\n",
            "91500\n",
            "91600\n",
            "91700\n",
            "91800\n",
            "91900\n",
            "92000\n",
            "92100\n",
            "92200\n",
            "92300\n",
            "92400\n",
            "92500\n",
            "92600\n",
            "92700\n",
            "92800\n",
            "92900\n",
            "93000\n",
            "93100\n",
            "93200\n",
            "93300\n",
            "93400\n",
            "93500\n",
            "93600\n",
            "93700\n",
            "93800\n",
            "93900\n",
            "94000\n",
            "94100\n",
            "94200\n",
            "94300\n",
            "94400\n",
            "94500\n",
            "94600\n",
            "94700\n",
            "94800\n",
            "94900\n",
            "95000\n",
            "95100\n",
            "95200\n",
            "95300\n",
            "95400\n",
            "95500\n",
            "95600\n",
            "95700\n",
            "95800\n",
            "95900\n",
            "96000\n",
            "96100\n",
            "96200\n",
            "96300\n",
            "96400\n",
            "96500\n",
            "96600\n",
            "96700\n",
            "96800\n",
            "96900\n",
            "97000\n",
            "97100\n",
            "97200\n",
            "97300\n",
            "97400\n",
            "97500\n",
            "97600\n",
            "97700\n",
            "97800\n",
            "97900\n",
            "98000\n",
            "98100\n",
            "98200\n",
            "98300\n",
            "98400\n",
            "98500\n",
            "98600\n",
            "98700\n",
            "98800\n",
            "98900\n",
            "99000\n",
            "99100\n",
            "99200\n",
            "99300\n",
            "99400\n",
            "99500\n",
            "99600\n",
            "99700\n",
            "99800\n",
            "99900\n",
            "100000\n",
            "100100\n",
            "100200\n",
            "100300\n",
            "100400\n",
            "100500\n",
            "100600\n",
            "100700\n",
            "100800\n",
            "100900\n",
            "101000\n",
            "101100\n",
            "101200\n",
            "101300\n",
            "101400\n",
            "101500\n",
            "101600\n",
            "101700\n",
            "101800\n",
            "101900\n",
            "102000\n",
            "102100\n",
            "102200\n",
            "102300\n",
            "102400\n",
            "102500\n",
            "102600\n",
            "102700\n",
            "102800\n",
            "102900\n",
            "103000\n",
            "103100\n",
            "103200\n",
            "103300\n",
            "103400\n",
            "103500\n",
            "103600\n",
            "103700\n",
            "103800\n",
            "103900\n",
            "104000\n",
            "104100\n",
            "104200\n",
            "104300\n",
            "104400\n",
            "104500\n",
            "104600\n",
            "104700\n",
            "104800\n",
            "104900\n",
            "105000\n",
            "105100\n",
            "105200\n",
            "105300\n",
            "105400\n",
            "105500\n",
            "105600\n",
            "105700\n",
            "105800\n",
            "105900\n",
            "106000\n",
            "106100\n",
            "106200\n",
            "106300\n",
            "106400\n",
            "106500\n",
            "106600\n",
            "106700\n",
            "106800\n",
            "106900\n",
            "107000\n",
            "107100\n",
            "107200\n",
            "107300\n",
            "107400\n",
            "107500\n",
            "107600\n",
            "107700\n",
            "107800\n",
            "107900\n",
            "108000\n",
            "108100\n",
            "108200\n",
            "108300\n",
            "108400\n",
            "108500\n",
            "108600\n",
            "108700\n",
            "108800\n",
            "108900\n",
            "109000\n",
            "109100\n",
            "109200\n",
            "109300\n",
            "109400\n",
            "109500\n",
            "109600\n",
            "109700\n",
            "109800\n",
            "109900\n",
            "110000\n",
            "110100\n",
            "110200\n",
            "110300\n",
            "110400\n",
            "110500\n",
            "110600\n",
            "110700\n",
            "110800\n",
            "110900\n",
            "111000\n",
            "111100\n",
            "111200\n",
            "111300\n",
            "111400\n",
            "111500\n",
            "111600\n",
            "111700\n",
            "111800\n",
            "111900\n",
            "112000\n",
            "112100\n",
            "112200\n",
            "112300\n",
            "112400\n",
            "112500\n",
            "112600\n",
            "112700\n",
            "112800\n",
            "112900\n",
            "113000\n",
            "113100\n",
            "113200\n",
            "113300\n",
            "113400\n",
            "113500\n",
            "113600\n",
            "113700\n",
            "113800\n",
            "113900\n",
            "114000\n",
            "114100\n",
            "114200\n",
            "114300\n",
            "114400\n",
            "114500\n",
            "114600\n",
            "114700\n",
            "114800\n",
            "114900\n",
            "115000\n",
            "115100\n",
            "115200\n",
            "115300\n",
            "115400\n",
            "115500\n",
            "115600\n",
            "115700\n",
            "115800\n",
            "115900\n",
            "116000\n",
            "116100\n",
            "116200\n",
            "116300\n",
            "116400\n",
            "116500\n",
            "116600\n",
            "116700\n",
            "116800\n",
            "116900\n",
            "117000\n",
            "117100\n",
            "117200\n",
            "117300\n",
            "117400\n",
            "117500\n",
            "117600\n",
            "117700\n",
            "117800\n",
            "117900\n",
            "118000\n",
            "118100\n",
            "118200\n",
            "118300\n",
            "118400\n",
            "118500\n",
            "118600\n",
            "118700\n",
            "118800\n",
            "118900\n",
            "119000\n",
            "119100\n",
            "119200\n",
            "119300\n",
            "119400\n",
            "119500\n",
            "119600\n",
            "119700\n",
            "119800\n",
            "119900\n",
            "120000\n",
            "120100\n",
            "120200\n",
            "120300\n",
            "120400\n",
            "120500\n",
            "120600\n",
            "120700\n",
            "120800\n",
            "120900\n",
            "121000\n",
            "121100\n",
            "121200\n",
            "121300\n",
            "121400\n",
            "121500\n",
            "121600\n",
            "121700\n",
            "121800\n",
            "121900\n",
            "122000\n",
            "122100\n",
            "122200\n",
            "122300\n",
            "122400\n",
            "122500\n",
            "122600\n",
            "122700\n",
            "122800\n",
            "122900\n",
            "123000\n",
            "123100\n",
            "123200\n",
            "123300\n",
            "123400\n",
            "123500\n",
            "123600\n",
            "123700\n",
            "123800\n",
            "123900\n",
            "124000\n",
            "124100\n",
            "124200\n",
            "124300\n",
            "124400\n",
            "124500\n",
            "124600\n",
            "124700\n",
            "124800\n",
            "124900\n",
            "125000\n",
            "125100\n",
            "125200\n",
            "125300\n",
            "125400\n",
            "125500\n",
            "125600\n",
            "125700\n",
            "125800\n",
            "125900\n",
            "126000\n",
            "126100\n",
            "126200\n",
            "126300\n",
            "126400\n",
            "126500\n",
            "126600\n",
            "126700\n",
            "126800\n",
            "126900\n",
            "127000\n",
            "127100\n",
            "127200\n",
            "127300\n",
            "127400\n",
            "127500\n",
            "127600\n",
            "127700\n",
            "127800\n",
            "127900\n",
            "128000\n",
            "128100\n",
            "128200\n",
            "128300\n",
            "128400\n",
            "128500\n",
            "128600\n",
            "128700\n",
            "128800\n",
            "128900\n",
            "129000\n",
            "129100\n",
            "129200\n",
            "129300\n",
            "129400\n",
            "129500\n",
            "129600\n",
            "129700\n",
            "129800\n",
            "129900\n",
            "130000\n",
            "130100\n",
            "130200\n",
            "130300\n",
            "130400\n",
            "130500\n",
            "130600\n",
            "130700\n",
            "130800\n",
            "130900\n",
            "131000\n",
            "131100\n",
            "131200\n",
            "131300\n",
            "131400\n",
            "131500\n",
            "131600\n",
            "131700\n",
            "131800\n",
            "131900\n",
            "132000\n",
            "132100\n",
            "132200\n",
            "132300\n",
            "132400\n",
            "132500\n",
            "132600\n",
            "132700\n",
            "132800\n",
            "132900\n",
            "133000\n",
            "133100\n",
            "133200\n",
            "133300\n",
            "133400\n",
            "133500\n",
            "133600\n",
            "133700\n",
            "133800\n",
            "133900\n",
            "134000\n",
            "134100\n",
            "134200\n",
            "134300\n",
            "134400\n",
            "134500\n",
            "134600\n",
            "134700\n",
            "134800\n",
            "134900\n",
            "135000\n",
            "135100\n",
            "135200\n",
            "135300\n",
            "135400\n",
            "135500\n",
            "135600\n",
            "135700\n",
            "135800\n",
            "135900\n",
            "136000\n",
            "136100\n",
            "136200\n",
            "136300\n",
            "136400\n",
            "136500\n",
            "136600\n",
            "136700\n",
            "136800\n",
            "136900\n",
            "137000\n",
            "137100\n",
            "137200\n",
            "137300\n",
            "137400\n",
            "137500\n",
            "137600\n",
            "137700\n",
            "137800\n",
            "137900\n",
            "138000\n",
            "138100\n",
            "138200\n",
            "138300\n",
            "138400\n",
            "138500\n",
            "138600\n",
            "138700\n",
            "138800\n",
            "138900\n",
            "139000\n",
            "139100\n",
            "139200\n",
            "139300\n",
            "139400\n",
            "139500\n",
            "139600\n",
            "139700\n",
            "139800\n",
            "139900\n",
            "140000\n",
            "140100\n",
            "140200\n",
            "140300\n",
            "140400\n",
            "140500\n",
            "140600\n",
            "140700\n",
            "140800\n",
            "140900\n",
            "141000\n",
            "141100\n",
            "141200\n",
            "141300\n",
            "141400\n",
            "141500\n",
            "141600\n",
            "141700\n",
            "141800\n",
            "141900\n",
            "142000\n",
            "142100\n",
            "142200\n",
            "142300\n",
            "142400\n",
            "142500\n",
            "142600\n",
            "142700\n",
            "142800\n",
            "142900\n",
            "143000\n",
            "143100\n",
            "143200\n",
            "143300\n",
            "143400\n",
            "143500\n",
            "143600\n",
            "143700\n",
            "143800\n",
            "143900\n",
            "144000\n",
            "144100\n",
            "144200\n",
            "144300\n",
            "144400\n",
            "144500\n",
            "144600\n",
            "144700\n",
            "144800\n",
            "144900\n",
            "145000\n",
            "145100\n",
            "145200\n",
            "145300\n",
            "145400\n",
            "145500\n",
            "145600\n",
            "145700\n",
            "145800\n",
            "145900\n",
            "146000\n",
            "146100\n",
            "146200\n",
            "146300\n",
            "146400\n",
            "146500\n",
            "146600\n",
            "146700\n",
            "146800\n",
            "146900\n",
            "147000\n",
            "147100\n",
            "147200\n",
            "147300\n",
            "147400\n",
            "147500\n",
            "147600\n",
            "147700\n",
            "147800\n",
            "147900\n",
            "148000\n",
            "148100\n",
            "148200\n",
            "148300\n",
            "148400\n",
            "148500\n",
            "148600\n",
            "148700\n",
            "148800\n",
            "148900\n",
            "149000\n",
            "149100\n",
            "149200\n",
            "149300\n",
            "149400\n",
            "149500\n",
            "149600\n",
            "149700\n",
            "149800\n",
            "149900\n",
            "150000\n",
            "150100\n",
            "150200\n",
            "150300\n",
            "150400\n",
            "150500\n",
            "150600\n",
            "150700\n",
            "150800\n",
            "150900\n",
            "151000\n",
            "151100\n",
            "151200\n",
            "151300\n",
            "151400\n",
            "151500\n",
            "151600\n",
            "151700\n",
            "151800\n",
            "151900\n",
            "152000\n",
            "152100\n",
            "152200\n",
            "152300\n",
            "152400\n",
            "152500\n",
            "152600\n",
            "152700\n",
            "152800\n",
            "152900\n",
            "153000\n",
            "153100\n",
            "153200\n",
            "153300\n",
            "153400\n",
            "153500\n",
            "153600\n",
            "153700\n",
            "153800\n",
            "153900\n",
            "154000\n",
            "154100\n",
            "154200\n",
            "154300\n",
            "154400\n",
            "154500\n",
            "154600\n",
            "154700\n",
            "154800\n",
            "154900\n",
            "155000\n",
            "155100\n",
            "155200\n",
            "155300\n",
            "155400\n",
            "155500\n",
            "155600\n",
            "155700\n",
            "155800\n",
            "155900\n",
            "156000\n",
            "156100\n",
            "156200\n",
            "156300\n",
            "156400\n",
            "156500\n",
            "156600\n",
            "156700\n",
            "156800\n",
            "156900\n",
            "157000\n",
            "157100\n",
            "157200\n",
            "157300\n",
            "157400\n",
            "157500\n",
            "157600\n",
            "157700\n",
            "157800\n",
            "157900\n",
            "158000\n",
            "158100\n",
            "158200\n",
            "158300\n",
            "158400\n",
            "158500\n",
            "158600\n",
            "158700\n",
            "158800\n",
            "158900\n",
            "159000\n",
            "159100\n",
            "159200\n",
            "159300\n",
            "159400\n",
            "159500\n",
            "159600\n",
            "159700\n",
            "159800\n",
            "159900\n",
            "160000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_stem</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "      <th>Tweet_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, alreadi, she, alway, ...</td>\n",
              "      <td>i miss nikki nu nu alreadi she alway there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>miss nikki nu nu alreadi she alway there when ...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>miss nikki nu nu alreadi she alway there when ...</td>\n",
              "      <td>[(i, NN), (miss, VBP), (nikki, JJ), (nu, JJ), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, rememb,...</td>\n",
              "      <td>so i had a dream last night i rememb a sign wh...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>so had dream last night rememb sign which clea...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>so had dream last night rememb sign which clea...</td>\n",
              "      <td>[(so, RB), (i, JJ), (had, VBD), (a, DT), (drea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[(MENTION, NNP), (ohh, PRP), (poor, JJ), (sick...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, rain, again]</td>\n",
              "      <td>it is rain again</td>\n",
              "      <td>[it, is, rain, again]</td>\n",
              "      <td>[it, is, rain, again, NEGATIVE, NEGATIVE, NEGA...</td>\n",
              "      <td>it is rain again</td>\n",
              "      <td>[it, is, rain, again, NEGATIVE, NEGATIVE, NEGA...</td>\n",
              "      <td>it is rain again NEGATIVE NEGATIVE NEGATIVE NE...</td>\n",
              "      <td>[(it, PRP), (is, VBZ), (raining, VBG), (again,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>[mention, wish, i, wa, in, la, right, now]</td>\n",
              "      <td>mention wish i wa in la right now</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now]</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now, NEGATI...</td>\n",
              "      <td>mention wish wa in la right now</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now, NEGATI...</td>\n",
              "      <td>mention wish wa in la right now NEGATIVE NEGAT...</td>\n",
              "      <td>[(MENTION, NNP), (wish, NN), (i, NN), (was, VB...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                                          Tweet_pos\n",
              "0         0  ...  [(i, NN), (miss, VBP), (nikki, JJ), (nu, JJ), ...\n",
              "1         0  ...  [(so, RB), (i, JJ), (had, VBD), (a, DT), (drea...\n",
              "2         0  ...  [(MENTION, NNP), (ohh, PRP), (poor, JJ), (sick...\n",
              "3         0  ...  [(it, PRP), (is, VBZ), (raining, VBG), (again,...\n",
              "4         0  ...  [(MENTION, NNP), (wish, NN), (i, NN), (was, VB...\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nITQsdJ0n3mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "d6028300-1f71-40bf-d16b-f1e1361cf7a8"
      },
      "source": [
        "df['Tweet_lemma'] = df['Tweet_pos'].apply(tweet_lemmatizer)\n",
        "df.head()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_stem</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "      <th>Tweet_pos</th>\n",
              "      <th>Tweet_lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, alreadi, she, alway, ...</td>\n",
              "      <td>i miss nikki nu nu alreadi she alway there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>miss nikki nu nu alreadi she alway there when ...</td>\n",
              "      <td>[miss, nikki, nu, nu, alreadi, she, alway, the...</td>\n",
              "      <td>miss nikki nu nu alreadi she alway there when ...</td>\n",
              "      <td>[(i, NN), (miss, VBP), (nikki, JJ), (nu, JJ), ...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, rememb,...</td>\n",
              "      <td>so i had a dream last night i rememb a sign wh...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>so had dream last night rememb sign which clea...</td>\n",
              "      <td>[so, had, dream, last, night, rememb, sign, wh...</td>\n",
              "      <td>so had dream last night rememb sign which clea...</td>\n",
              "      <td>[(so, RB), (i, JJ), (had, VBD), (a, DT), (drea...</td>\n",
              "      <td>[so, i, have, a, dream, last, night, i, rememb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[mention, ohh, poor, sickli, you, hug, hope, y...</td>\n",
              "      <td>mention ohh poor sickli you hug hope you feel ...</td>\n",
              "      <td>[(MENTION, NNP), (ohh, PRP), (poor, JJ), (sick...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hug, hope, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, rain, again]</td>\n",
              "      <td>it is rain again</td>\n",
              "      <td>[it, is, rain, again]</td>\n",
              "      <td>[it, is, rain, again, NEGATIVE, NEGATIVE, NEGA...</td>\n",
              "      <td>it is rain again</td>\n",
              "      <td>[it, is, rain, again, NEGATIVE, NEGATIVE, NEGA...</td>\n",
              "      <td>it is rain again NEGATIVE NEGATIVE NEGATIVE NE...</td>\n",
              "      <td>[(it, PRP), (is, VBZ), (raining, VBG), (again,...</td>\n",
              "      <td>[it, be, rain, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>[mention, wish, i, wa, in, la, right, now]</td>\n",
              "      <td>mention wish i wa in la right now</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now]</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now, NEGATI...</td>\n",
              "      <td>mention wish wa in la right now</td>\n",
              "      <td>[mention, wish, wa, in, la, right, now, NEGATI...</td>\n",
              "      <td>mention wish wa in la right now NEGATIVE NEGAT...</td>\n",
              "      <td>[(MENTION, NNP), (wish, NN), (i, NN), (was, VB...</td>\n",
              "      <td>[MENTION, wish, i, be, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                                        Tweet_lemma\n",
              "0         0  ...  [i, miss, nikki, nu, nu, already, shes, always...\n",
              "1         0  ...  [so, i, have, a, dream, last, night, i, rememb...\n",
              "2         0  ...  [MENTION, ohh, poor, sickly, you, hug, hope, y...\n",
              "3         0  ...                              [it, be, rain, again]\n",
              "4         0  ...         [MENTION, wish, i, be, in, la, right, now]\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhL62ykXI1x8"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xmeQN4n3mN"
      },
      "source": [
        "def make_sentences(df, col, title):\n",
        "    df[title] = df[col].apply(lambda x:' '.join([i for i in x]))\n",
        "    return df"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxPIAu_vn3mN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "8604e6bc-74e0-4788-999c-03430f7c0d5c"
      },
      "source": [
        "df = make_sentences(df, 'Tweet_token', 'Tweet_sent')\n",
        "df.head()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                         Tweet_sent\n",
              "514293         0  ...  i miss nikki nu nu already shes always there w...\n",
              "142282         0  ...  so i had a dream last night i remember a sign ...\n",
              "403727         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "649503         0  ...                                it is raining again\n",
              "610789         0  ...                 MENTION wish i was in la right now\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkfs5-v5WG1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84207548-1a18-4c07-ddad-f44a6aec1ee4"
      },
      "source": [
        "wordnet.synsets('hello')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('hello.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBOLGluLW0rs"
      },
      "source": [
        "## TO BE MODIFIED #####\n",
        "def normalisation_words(tweet):\n",
        "    tweet = tweet.replace(r'([a-z])\\1{1,}', r'\\1\\1')\n",
        "    # tweet = re.sub(r'(ha){1,}', r'laugh', tweet)\n",
        "    tweet = re.sub(r'h+a+[ha]+', r'laaaugh', tweet)     # To give more significance\n",
        "    # tweet = re.sub(r'(lol){1,}', r'laugh', tweet)\n",
        "    tweet = re.sub(r'l+o+[lo]+', r'laaaugh', tweet)\n",
        "    tweet = ' '.join([word if len(wordnet.synsets(word)) > 0 else re.sub(r'([a-z])\\1{1,}', r'\\1\\1', word) for word in tweet.split()])\n",
        "    tweet = re.sub(r'\\b([a-z])\\1{1,}', r' ', tweet)     # If only repeated letters are left, remove them\n",
        "    tweet = re.sub(r\"\\b[a-zA-Z]{1}\\b\", ' ', tweet)        # Removing single letters\n",
        "    # tweet = re.sub(r'lo+l+o+[^\\s]+', r'lol', tweet)\n",
        "    return tweet.split()"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4APiy1icXJ3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b6585c-2c4a-4340-9b8c-4b38585f91c6"
      },
      "source": [
        "normalisation_words('awww hahahhhahahahahahahhahaaaaaaaaaaahhhahaha ppeeee lollipop looooooool happppiest day lolllll lll lool bummer get david carr third day laugh')"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aww',\n",
              " 'laaugh',\n",
              " 'ee',\n",
              " 'laaughipop',\n",
              " 'laaugh',\n",
              " 'happiest',\n",
              " 'day',\n",
              " 'laaugh',\n",
              " 'laaugh',\n",
              " 'bummer',\n",
              " 'get',\n",
              " 'david',\n",
              " 'carr',\n",
              " 'third',\n",
              " 'day',\n",
              " 'laugh']"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TG5vlkdWCAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "207eb227-3ec7-400b-95f6-2dd7dcac0295"
      },
      "source": [
        "df['Tweet_normalised'] = df['Tweet_sent'].apply(normalisation_words)\n",
        "df.head()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                   Tweet_normalised\n",
              "514293         0  ...  [miss, nikki, nu, nu, already, shes, always, t...\n",
              "142282         0  ...  [so, had, dream, last, night, remember, sign, ...\n",
              "403727         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "649503         0  ...                           [it, is, raining, again]\n",
              "610789         0  ...           [MENTION, wish, was, in, la, right, now]\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzULD6oTIvlJ"
      },
      "source": [
        ""
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxYHa-0un3mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b15e96fe-41b3-408e-8cda-f06e9261480a"
      },
      "source": [
        "df['Tweet_stopword'] = df['Tweet_normalised'].apply(stopword_removal)\n",
        "df.head()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "31300\n",
            "31400\n",
            "31500\n",
            "31600\n",
            "31700\n",
            "31800\n",
            "31900\n",
            "32000\n",
            "32100\n",
            "32200\n",
            "32300\n",
            "32400\n",
            "32500\n",
            "32600\n",
            "32700\n",
            "32800\n",
            "32900\n",
            "33000\n",
            "33100\n",
            "33200\n",
            "33300\n",
            "33400\n",
            "33500\n",
            "33600\n",
            "33700\n",
            "33800\n",
            "33900\n",
            "34000\n",
            "34100\n",
            "34200\n",
            "34300\n",
            "34400\n",
            "34500\n",
            "34600\n",
            "34700\n",
            "34800\n",
            "34900\n",
            "35000\n",
            "35100\n",
            "35200\n",
            "35300\n",
            "35400\n",
            "35500\n",
            "35600\n",
            "35700\n",
            "35800\n",
            "35900\n",
            "36000\n",
            "36100\n",
            "36200\n",
            "36300\n",
            "36400\n",
            "36500\n",
            "36600\n",
            "36700\n",
            "36800\n",
            "36900\n",
            "37000\n",
            "37100\n",
            "37200\n",
            "37300\n",
            "37400\n",
            "37500\n",
            "37600\n",
            "37700\n",
            "37800\n",
            "37900\n",
            "38000\n",
            "38100\n",
            "38200\n",
            "38300\n",
            "38400\n",
            "38500\n",
            "38600\n",
            "38700\n",
            "38800\n",
            "38900\n",
            "39000\n",
            "39100\n",
            "39200\n",
            "39300\n",
            "39400\n",
            "39500\n",
            "39600\n",
            "39700\n",
            "39800\n",
            "39900\n",
            "40000\n",
            "40100\n",
            "40200\n",
            "40300\n",
            "40400\n",
            "40500\n",
            "40600\n",
            "40700\n",
            "40800\n",
            "40900\n",
            "41000\n",
            "41100\n",
            "41200\n",
            "41300\n",
            "41400\n",
            "41500\n",
            "41600\n",
            "41700\n",
            "41800\n",
            "41900\n",
            "42000\n",
            "42100\n",
            "42200\n",
            "42300\n",
            "42400\n",
            "42500\n",
            "42600\n",
            "42700\n",
            "42800\n",
            "42900\n",
            "43000\n",
            "43100\n",
            "43200\n",
            "43300\n",
            "43400\n",
            "43500\n",
            "43600\n",
            "43700\n",
            "43800\n",
            "43900\n",
            "44000\n",
            "44100\n",
            "44200\n",
            "44300\n",
            "44400\n",
            "44500\n",
            "44600\n",
            "44700\n",
            "44800\n",
            "44900\n",
            "45000\n",
            "45100\n",
            "45200\n",
            "45300\n",
            "45400\n",
            "45500\n",
            "45600\n",
            "45700\n",
            "45800\n",
            "45900\n",
            "46000\n",
            "46100\n",
            "46200\n",
            "46300\n",
            "46400\n",
            "46500\n",
            "46600\n",
            "46700\n",
            "46800\n",
            "46900\n",
            "47000\n",
            "47100\n",
            "47200\n",
            "47300\n",
            "47400\n",
            "47500\n",
            "47600\n",
            "47700\n",
            "47800\n",
            "47900\n",
            "48000\n",
            "48100\n",
            "48200\n",
            "48300\n",
            "48400\n",
            "48500\n",
            "48600\n",
            "48700\n",
            "48800\n",
            "48900\n",
            "49000\n",
            "49100\n",
            "49200\n",
            "49300\n",
            "49400\n",
            "49500\n",
            "49600\n",
            "49700\n",
            "49800\n",
            "49900\n",
            "50000\n",
            "50100\n",
            "50200\n",
            "50300\n",
            "50400\n",
            "50500\n",
            "50600\n",
            "50700\n",
            "50800\n",
            "50900\n",
            "51000\n",
            "51100\n",
            "51200\n",
            "51300\n",
            "51400\n",
            "51500\n",
            "51600\n",
            "51700\n",
            "51800\n",
            "51900\n",
            "52000\n",
            "52100\n",
            "52200\n",
            "52300\n",
            "52400\n",
            "52500\n",
            "52600\n",
            "52700\n",
            "52800\n",
            "52900\n",
            "53000\n",
            "53100\n",
            "53200\n",
            "53300\n",
            "53400\n",
            "53500\n",
            "53600\n",
            "53700\n",
            "53800\n",
            "53900\n",
            "54000\n",
            "54100\n",
            "54200\n",
            "54300\n",
            "54400\n",
            "54500\n",
            "54600\n",
            "54700\n",
            "54800\n",
            "54900\n",
            "55000\n",
            "55100\n",
            "55200\n",
            "55300\n",
            "55400\n",
            "55500\n",
            "55600\n",
            "55700\n",
            "55800\n",
            "55900\n",
            "56000\n",
            "56100\n",
            "56200\n",
            "56300\n",
            "56400\n",
            "56500\n",
            "56600\n",
            "56700\n",
            "56800\n",
            "56900\n",
            "57000\n",
            "57100\n",
            "57200\n",
            "57300\n",
            "57400\n",
            "57500\n",
            "57600\n",
            "57700\n",
            "57800\n",
            "57900\n",
            "58000\n",
            "58100\n",
            "58200\n",
            "58300\n",
            "58400\n",
            "58500\n",
            "58600\n",
            "58700\n",
            "58800\n",
            "58900\n",
            "59000\n",
            "59100\n",
            "59200\n",
            "59300\n",
            "59400\n",
            "59500\n",
            "59600\n",
            "59700\n",
            "59800\n",
            "59900\n",
            "60000\n",
            "60100\n",
            "60200\n",
            "60300\n",
            "60400\n",
            "60500\n",
            "60600\n",
            "60700\n",
            "60800\n",
            "60900\n",
            "61000\n",
            "61100\n",
            "61200\n",
            "61300\n",
            "61400\n",
            "61500\n",
            "61600\n",
            "61700\n",
            "61800\n",
            "61900\n",
            "62000\n",
            "62100\n",
            "62200\n",
            "62300\n",
            "62400\n",
            "62500\n",
            "62600\n",
            "62700\n",
            "62800\n",
            "62900\n",
            "63000\n",
            "63100\n",
            "63200\n",
            "63300\n",
            "63400\n",
            "63500\n",
            "63600\n",
            "63700\n",
            "63800\n",
            "63900\n",
            "64000\n",
            "64100\n",
            "64200\n",
            "64300\n",
            "64400\n",
            "64500\n",
            "64600\n",
            "64700\n",
            "64800\n",
            "64900\n",
            "65000\n",
            "65100\n",
            "65200\n",
            "65300\n",
            "65400\n",
            "65500\n",
            "65600\n",
            "65700\n",
            "65800\n",
            "65900\n",
            "66000\n",
            "66100\n",
            "66200\n",
            "66300\n",
            "66400\n",
            "66500\n",
            "66600\n",
            "66700\n",
            "66800\n",
            "66900\n",
            "67000\n",
            "67100\n",
            "67200\n",
            "67300\n",
            "67400\n",
            "67500\n",
            "67600\n",
            "67700\n",
            "67800\n",
            "67900\n",
            "68000\n",
            "68100\n",
            "68200\n",
            "68300\n",
            "68400\n",
            "68500\n",
            "68600\n",
            "68700\n",
            "68800\n",
            "68900\n",
            "69000\n",
            "69100\n",
            "69200\n",
            "69300\n",
            "69400\n",
            "69500\n",
            "69600\n",
            "69700\n",
            "69800\n",
            "69900\n",
            "70000\n",
            "70100\n",
            "70200\n",
            "70300\n",
            "70400\n",
            "70500\n",
            "70600\n",
            "70700\n",
            "70800\n",
            "70900\n",
            "71000\n",
            "71100\n",
            "71200\n",
            "71300\n",
            "71400\n",
            "71500\n",
            "71600\n",
            "71700\n",
            "71800\n",
            "71900\n",
            "72000\n",
            "72100\n",
            "72200\n",
            "72300\n",
            "72400\n",
            "72500\n",
            "72600\n",
            "72700\n",
            "72800\n",
            "72900\n",
            "73000\n",
            "73100\n",
            "73200\n",
            "73300\n",
            "73400\n",
            "73500\n",
            "73600\n",
            "73700\n",
            "73800\n",
            "73900\n",
            "74000\n",
            "74100\n",
            "74200\n",
            "74300\n",
            "74400\n",
            "74500\n",
            "74600\n",
            "74700\n",
            "74800\n",
            "74900\n",
            "75000\n",
            "75100\n",
            "75200\n",
            "75300\n",
            "75400\n",
            "75500\n",
            "75600\n",
            "75700\n",
            "75800\n",
            "75900\n",
            "76000\n",
            "76100\n",
            "76200\n",
            "76300\n",
            "76400\n",
            "76500\n",
            "76600\n",
            "76700\n",
            "76800\n",
            "76900\n",
            "77000\n",
            "77100\n",
            "77200\n",
            "77300\n",
            "77400\n",
            "77500\n",
            "77600\n",
            "77700\n",
            "77800\n",
            "77900\n",
            "78000\n",
            "78100\n",
            "78200\n",
            "78300\n",
            "78400\n",
            "78500\n",
            "78600\n",
            "78700\n",
            "78800\n",
            "78900\n",
            "79000\n",
            "79100\n",
            "79200\n",
            "79300\n",
            "79400\n",
            "79500\n",
            "79600\n",
            "79700\n",
            "79800\n",
            "79900\n",
            "80000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                     Tweet_stopword\n",
              "514293         0  ...  [miss, nikki, nu, nu, already, shes, always, t...\n",
              "142282         0  ...  [so, had, dream, last, night, remember, sign, ...\n",
              "403727         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "649503         0  ...                           [it, is, raining, again]\n",
              "610789         0  ...           [MENTION, wish, was, in, la, right, now]\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUlsuWqAZPXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "a0f414cc-c886-4425-c810-a49ec34bf56c"
      },
      "source": [
        "df = make_sentences(df, 'Tweet_stopword', 'Tweet_final_sent')\n",
        "df.head()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                     Tweet_stopword\n",
              "514293         0  ...  [miss, nikki, nu, nu, already, shes, always, t...\n",
              "142282         0  ...  [so, had, dream, last, night, remember, sign, ...\n",
              "403727         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "649503         0  ...                           [it, is, raining, again]\n",
              "610789         0  ...           [MENTION, wish, was, in, la, right, now]\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGZZH19KWw7X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJMDNZfdZPQm"
      },
      "source": [
        "df.drop(df[df[\"Tweet_final_sent\"] == ''].index, inplace=True)\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhBbxLJqZPKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "6cd76a4d-c70f-4df5-9777-e5947d617b90"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                                     Tweet_stopword\n",
              "0         0  ...  [miss, nikki, nu, nu, already, shes, always, t...\n",
              "1         0  ...  [so, had, dream, last, night, remember, sign, ...\n",
              "2         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "3         0  ...                           [it, is, raining, again]\n",
              "4         0  ...           [MENTION, wish, was, in, la, right, now]\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsjTY7SkykwY"
      },
      "source": [
        ""
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKai0rAZ24R"
      },
      "source": [
        ""
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ijUT5lZ2y-"
      },
      "source": [
        "# Think if you want to do stratify\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Tweet_final_sent'], df['Polarity'], stratify=df['Polarity'], test_size=0.1, random_state=2)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0gB20PRZ2tE"
      },
      "source": [
        "# X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zSQ68pkZ2nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bceaef-397c-4d90-82e2-9ba7676b6575"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72000,)"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HntiKSPaZ2iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82659e28-4ea8-4796-b38b-258e8f2a9d96"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000,)"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d4KbtchZ2dL"
      },
      "source": [
        "# X_dev.shape"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCxzxwIwbTOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda9b064-4884-461f-d145-447b71d82e7a"
      },
      "source": [
        "X_train\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36755    gorgeou day had cute mo old thi morn and sale ...\n",
              "69117    work doubl on friday leav you with much wors t...\n",
              "13517                             hate how bore monday are\n",
              "7955                              my tooth still isnt outt\n",
              "1281     now am sad wa talk about my babi brother who h...\n",
              "                               ...                        \n",
              "1851             mention ahh thank you it is fix now enjoy\n",
              "39409    walk over tri hashtag michelleobama is speach ...\n",
              "24603    my pooky here now let is see how long until he...\n",
              "79946    mention veri excit about my weekend in nyc che...\n",
              "51873    predepartur catastroph had to take ratbag to t...\n",
              "Name: Tweet_final_sent, Length: 72000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn_sVopqbTGF"
      },
      "source": [
        "import csv, collections\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk-2ljkObJh_"
      },
      "source": [
        "def load_sent_word_net():\n",
        "    sent_scores = collections.defaultdict(list)\n",
        "\n",
        "    with open(\"../content/drive/MyDrive/COL772_A2/SentiWordNet_3.0.0.txt\",\"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
        "\n",
        "        for line in reader:\n",
        "            if line[0].startswith(\"#\"):\n",
        "                continue\n",
        "            if len(line) == 1:\n",
        "                continue\n",
        "            POS, ID, PosScore, NegScore, SynsetTerms, Glos = line\n",
        "            if len(POS) == 0 or len(ID) == 0:\n",
        "                continue\n",
        "            for term in SynsetTerms.split(\" \"):\n",
        "                term = term.split('#')[0]\n",
        "                # print(term)\n",
        "                term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
        "                key = \"%s/%s\" % (POS, term)\n",
        "                # print(key)\n",
        "                sent_scores[key].append((float(PosScore), float(NegScore)))\n",
        "                # print(sent_scores)\n",
        "        for key, value in sent_scores.items():\n",
        "            sent_scores[key] = np.mean(value, axis=0)\n",
        "\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "sent_word_net = load_sent_word_net()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p2X3-DiZ2YP"
      },
      "source": [
        "class LinguisticVectorizer(BaseEstimator):\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return np.array(['sent_pos', 'sent_neg', 'nouns', 'adjectives', 'verbs', 'adverbs'])\n",
        "\n",
        "    def fit(self, documents, y=None):\n",
        "        return self\n",
        "\n",
        "    def _get_sentiments(self, d):\n",
        "        sent = tuple(d.split())\n",
        "        tagged = nltk.pos_tag(sent)\n",
        "\n",
        "        pos_vals = []\n",
        "        neg_vals = []\n",
        "\n",
        "        nouns = 0.\n",
        "        adjectives = 0.\n",
        "        verbs = 0.\n",
        "        adverbs = 0.\n",
        "\n",
        "        i = 0\n",
        "        for w, t in tagged:\n",
        "\n",
        "            p, n = 0, 0\n",
        "            sent_pos_type = None\n",
        "            if t.startswith(\"NN\"):\n",
        "                #noun\n",
        "                sent_pos_type = \"n\"\n",
        "                nouns += 1\n",
        "            elif t.startswith(\"JJ\"):\n",
        "                #adjective\n",
        "                sent_pos_type = \"a\"\n",
        "                adjectives += 1\n",
        "            elif t.startswith(\"VB\"):\n",
        "                #verb\n",
        "                sent_pos_type = \"v\"\n",
        "                verbs += 1\n",
        "            elif t.startswith(\"RB\"):\n",
        "                #adverb\n",
        "                sent_pos_type = \"r\"\n",
        "                adverbs += 1\n",
        "            else:\n",
        "                sent_pos_type = \"Nan\"\n",
        "\n",
        "                i += 1\n",
        "                l = len(sent) - i\n",
        "\n",
        "                if l == 0:\n",
        "                    l = 1\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            if sent_pos_type is not None:\n",
        "\n",
        "                sent_word = \"%s/%s\" % (sent_pos_type, w)\n",
        "\n",
        "                if sent_word in sent_word_net:\n",
        "                    p, n = sent_word_net[sent_word]\n",
        "                elif sent_word == \"Nan\":\n",
        "                    p, n = 0, 0\n",
        "\n",
        "                pos_vals.append(p)\n",
        "                neg_vals.append(n)\n",
        "\n",
        "        if i == 0:\n",
        "            l = len(sent)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        avg_pos_val = np.mean(pos_vals)\n",
        "        avg_neg_val = np.mean(neg_vals)\n",
        "\n",
        "        return [avg_pos_val, avg_neg_val, nouns / l, adjectives / l, verbs / l, adverbs / l]\n",
        "\n",
        "    # print(_get_sentiments('This be fantastic'))\n",
        "\n",
        "    def transform(self, documents):\n",
        "        pos_val, neg_val, nouns, adjectives, verbs, adverbs = np.array([self._get_sentiments(d) for d in documents]).T\n",
        "        result = np.array([pos_val, neg_val, nouns, adjectives, verbs, adverbs]).T\n",
        "\n",
        "        return result"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fmvMI_1Z2Sn"
      },
      "source": [
        ""
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGyQiQdZVK4b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRCvgmF8VKuF"
      },
      "source": [
        "# Define functions\n",
        "def create_baseline_models():\n",
        "    \"\"\"Create list of baseline models.\"\"\"\n",
        "    models = []\n",
        "    models.append(('log', LogisticRegression(random_state=123, \n",
        "                                             max_iter=1000)))\n",
        "    models.append(('sgd', SGDClassifier(random_state=123)))\n",
        "    models.append(('mnb', MultinomialNB()))\n",
        "    return models\n",
        "\n",
        "def assess(X, y, models, cv=5, scoring=['roc_auc', \n",
        "                                        'accuracy', \n",
        "                                        'f1']):\n",
        "    \"\"\"Provide summary of cross validation results for models.\"\"\"\n",
        "    results = pd.DataFrame()\n",
        "    for name, model in models:\n",
        "        result = pd.DataFrame(cross_validate(model, X, y, cv=cv, \n",
        "                                             scoring=scoring))\n",
        "        mean = result.mean().rename('{}_mean'.format)\n",
        "        std = result.std().rename('{}_std'.format)\n",
        "        results[name] = pd.concat([mean, std], axis=0)\n",
        "    return results.sort_index()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzRWuIs3W0x0",
        "outputId": "043dbb56-bb4b-4d22-c8ec-34e5fc651ffe"
      },
      "source": [
        "models = create_baseline_models()\n",
        "models"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('log',\n",
              "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                     intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                     multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                     random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                     warm_start=False)),\n",
              " ('sgd', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "                l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
              "                max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "                power_t=0.5, random_state=123, shuffle=True, tol=0.001,\n",
              "                validation_fraction=0.1, verbose=0, warm_start=False)),\n",
              " ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "D9oWz7bzVKlT",
        "outputId": "1576ed66-6df0-4fb9-8bce-97dbe56d1440"
      },
      "source": [
        "# Preprocess the data\n",
        "vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', \n",
        "                             stop_words='english', \n",
        "                             min_df=30, \n",
        "                             max_df=.7)\n",
        "X_train_simpler = vectoriser.fit_transform(X_train)\n",
        "# Assess the model\n",
        "assess(X_train_simpler, y_train, models)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>log</th>\n",
              "      <th>sgd</th>\n",
              "      <th>mnb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fit_time_mean</th>\n",
              "      <td>0.948748</td>\n",
              "      <td>0.125900</td>\n",
              "      <td>0.022557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fit_time_std</th>\n",
              "      <td>0.114444</td>\n",
              "      <td>0.005250</td>\n",
              "      <td>0.003963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>score_time_mean</th>\n",
              "      <td>0.016027</td>\n",
              "      <td>0.016216</td>\n",
              "      <td>0.022632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>score_time_std</th>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.004119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_accuracy_mean</th>\n",
              "      <td>0.746097</td>\n",
              "      <td>0.746458</td>\n",
              "      <td>0.739278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_accuracy_std</th>\n",
              "      <td>0.003333</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.002977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_f1_mean</th>\n",
              "      <td>0.752671</td>\n",
              "      <td>0.757212</td>\n",
              "      <td>0.744791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_f1_std</th>\n",
              "      <td>0.002869</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.001948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_roc_auc_mean</th>\n",
              "      <td>0.827126</td>\n",
              "      <td>0.824481</td>\n",
              "      <td>0.820055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_roc_auc_std</th>\n",
              "      <td>0.003550</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.004196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         log       sgd       mnb\n",
              "fit_time_mean       0.948748  0.125900  0.022557\n",
              "fit_time_std        0.114444  0.005250  0.003963\n",
              "score_time_mean     0.016027  0.016216  0.022632\n",
              "score_time_std      0.000369  0.000349  0.004119\n",
              "test_accuracy_mean  0.746097  0.746458  0.739278\n",
              "test_accuracy_std   0.003333  0.002156  0.002977\n",
              "test_f1_mean        0.752671  0.757212  0.744791\n",
              "test_f1_std         0.002869  0.001636  0.001948\n",
              "test_roc_auc_mean   0.827126  0.824481  0.820055\n",
              "test_roc_auc_std    0.003550  0.003068  0.004196"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqnWNoenVKcA"
      },
      "source": [
        "# Create a pipeline\n",
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+')),\n",
        "                 ('model', SGDClassifier(random_state=123))])\n",
        "# Prepare a random search\n",
        "param_distributions = {'vectoriser__min_df': np.arange(10, 1000, 10),\n",
        "                       'vectoriser__max_df': np.linspace(.2, 1, 40),\n",
        "                       'model__loss': ['log', 'hinge']}\n",
        "r_search = RandomizedSearchCV(estimator=pipe, param_distributions=param_distributions, \n",
        "                              n_iter=30, cv=5, n_jobs=-1, random_state=123)\n",
        "r_search.fit(X_train, y_train)\n",
        "# Save results to a dataframe\n",
        "r_search_results = pd.DataFrame(r_search.cv_results_).sort_values(by='rank_test_score')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jw2B2RwVKSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ddfe31b4-8d80-41ee-8ff7-00419e7805a5"
      },
      "source": [
        "columns = [col for col in r_search_results.columns \n",
        "           if re.search(r\"split|param_\", col)]\n",
        "r_summary = r_search_results[columns].copy()\n",
        "r_summary.columns = [re.sub(r'_test_score|param_', '', col) \n",
        "                     for col in r_summary.columns]\n",
        "columns = [col.split('__')[1] if '__' in col else col \n",
        "           for col in r_summary.columns ]\n",
        "r_summary.columns = columns\n",
        "r_summary.head()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>min_df</th>\n",
              "      <th>max_df</th>\n",
              "      <th>loss</th>\n",
              "      <th>split0</th>\n",
              "      <th>split1</th>\n",
              "      <th>split2</th>\n",
              "      <th>split3</th>\n",
              "      <th>split4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20</td>\n",
              "      <td>0.220513</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.768056</td>\n",
              "      <td>0.773611</td>\n",
              "      <td>0.774444</td>\n",
              "      <td>0.773819</td>\n",
              "      <td>0.774167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>0.938462</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.758750</td>\n",
              "      <td>0.761389</td>\n",
              "      <td>0.764236</td>\n",
              "      <td>0.759444</td>\n",
              "      <td>0.759514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100</td>\n",
              "      <td>0.528205</td>\n",
              "      <td>log</td>\n",
              "      <td>0.757431</td>\n",
              "      <td>0.762639</td>\n",
              "      <td>0.765139</td>\n",
              "      <td>0.754792</td>\n",
              "      <td>0.757569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>120</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.750972</td>\n",
              "      <td>0.756319</td>\n",
              "      <td>0.757153</td>\n",
              "      <td>0.754167</td>\n",
              "      <td>0.754306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>140</td>\n",
              "      <td>0.671795</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.751181</td>\n",
              "      <td>0.756528</td>\n",
              "      <td>0.757014</td>\n",
              "      <td>0.753958</td>\n",
              "      <td>0.754028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   min_df    max_df   loss    split0    split1    split2    split3    split4\n",
              "4      20  0.220513  hinge  0.768056  0.773611  0.774444  0.773819  0.774167\n",
              "1     100  0.938462  hinge  0.758750  0.761389  0.764236  0.759444  0.759514\n",
              "11    100  0.528205    log  0.757431  0.762639  0.765139  0.754792  0.757569\n",
              "18    120  0.241026  hinge  0.750972  0.756319  0.757153  0.754167  0.754306\n",
              "9     140  0.671795  hinge  0.751181  0.756528  0.757014  0.753958  0.754028"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsAUhN4MrDwT"
      },
      "source": [
        "# Create a pipeline\n",
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', max_df=.6)),\n",
        "                 ('model', SGDClassifier(random_state=123, loss='hinge'))])\n",
        "# Prepare a grid search\n",
        "param_grid = {'vectoriser__min_df': [30, 90, 150],\n",
        "              'vectoriser__ngram_range': [(1,1), (1,2)],\n",
        "              'vectoriser__stop_words': [None, 'english'],\n",
        "              'model__fit_intercept': [True, False]}\n",
        "g_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "g_search.fit(X_train, y_train)\n",
        "# Save results to a dataframe\n",
        "g_search_results = pd.DataFrame(g_search.cv_results_).sort_values(by='rank_test_score')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg0MPh1tYtgu"
      },
      "source": [
        "# columns = [col for col in g_search_results.columns \n",
        "#            if re.search(r\"split|param_\", col)]\n",
        "# g_summary = g_search_results[columns+['mean_test_score']].copy()\n",
        "# g_summary.columns = [re.sub(r'_test_score|param_', '', col) \n",
        "#                      for col in g_summary.columns]\n",
        "# columns = [col.split('__')[1] if '__' in col else col \n",
        "#            for col in g_summary.columns ]\n",
        "# g_summary.columns = columns\n",
        "# g_summary.head()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsRkcgE5YtYn"
      },
      "source": [
        "# # Create a long dataframe\n",
        "# g_summary_long = pd.melt(g_summary, \n",
        "#                          id_vars=['min_df', \n",
        "#                                   'ngram_range', \n",
        "#                                   'stop_words', \n",
        "#                                   'fit_intercept'], \n",
        "#                          value_vars=['split0', \n",
        "#                                      'split1', \n",
        "#                                      'split2', \n",
        "#                                      'split3', \n",
        "#                                      'split4'])\n",
        "# g_summary_long.replace({None: 'None'}, inplace=True)\n",
        "# # Plot performance\n",
        "# for param in ['ngram_range', 'stop_words', 'fit_intercept']:\n",
        "#     plt.figure(figsize=(8,4))\n",
        "#     plt.title(f'Performance by {param}')\n",
        "#     sns.boxplot(x='value', y=param, data=g_summary_long, orient='h')\n",
        "#     plt.xlim(.85, .95);"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-4OIo_rYtQk"
      },
      "source": [
        ""
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFLaH_xGaKaN"
      },
      "source": [
        ""
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWnj9zxtZifk",
        "outputId": "952e9274-6169-4b62-b1cc-8ef7d52137ed"
      },
      "source": [
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', min_df=30, max_df=.6, ngram_range=(1,2))),\n",
        "                 ('model', SGDClassifier(random_state=123, loss='hinge'))])\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectoriser',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.6, max_features=None,\n",
              "                                 min_df=30, ngram_range=(1, 2), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False, token_pat...\n",
              "                ('model',\n",
              "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=True, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='hinge',\n",
              "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l2', power_t=0.5, random_state=123,\n",
              "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CHCSjZZbZiXQ",
        "outputId": "58c87f28-a724-44a9-a476-92fbc7b231ab"
      },
      "source": [
        "coefs = pd.DataFrame(pipe['model'].coef_, \n",
        "                     columns=pipe['vectoriser'].get_feature_names())\n",
        "coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')\n",
        "coefs"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>not</th>\n",
              "      <td>-5.554187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miss</th>\n",
              "      <td>-5.223724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <td>-5.031330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wish</th>\n",
              "      <td>-3.460169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sorri</th>\n",
              "      <td>-3.267135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>happi</th>\n",
              "      <td>2.223506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>2.342490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>2.426037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thank</th>\n",
              "      <td>2.572014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not wait</th>\n",
              "      <td>3.232475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4900 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              coef\n",
              "not      -5.554187\n",
              "miss     -5.223724\n",
              "sad      -5.031330\n",
              "wish     -3.460169\n",
              "sorri    -3.267135\n",
              "...            ...\n",
              "happi     2.223506\n",
              "love      2.342490\n",
              "good      2.426037\n",
              "thank     2.572014\n",
              "not wait  3.232475\n",
              "\n",
              "[4900 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTbsfW1pNRoC"
      },
      "source": [
        "coef_pos_set = set(coefs.iloc[np.where(coefs['coef'] > 1)].index.tolist())\n",
        "coef_neg_set = set(coefs.iloc[np.where(coefs['coef'] < -1)].index.tolist())"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEtM5HvsSOj6",
        "outputId": "052b3306-ba0e-4852-8335-dd659018b39b"
      },
      "source": [
        "coef_pos_set"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actual',\n",
              " 'amaz',\n",
              " 'awesom',\n",
              " 'beauti',\n",
              " 'best',\n",
              " 'birthday',\n",
              " 'blast',\n",
              " 'bless',\n",
              " 'btw',\n",
              " 'cant wait',\n",
              " 'congrat',\n",
              " 'cool',\n",
              " 'cute',\n",
              " 'enjoy',\n",
              " 'excit',\n",
              " 'follow',\n",
              " 'fun',\n",
              " 'funni',\n",
              " 'glad',\n",
              " 'good',\n",
              " 'goodnight',\n",
              " 'gorgeou',\n",
              " 'great',\n",
              " 'ha ha',\n",
              " 'happi',\n",
              " 'hehe',\n",
              " 'heheh',\n",
              " 'hello',\n",
              " 'hey',\n",
              " 'it work',\n",
              " 'laugh',\n",
              " 'listen',\n",
              " 'love',\n",
              " 'lt',\n",
              " 'mind',\n",
              " 'name',\n",
              " 'new',\n",
              " 'nice',\n",
              " 'not bad',\n",
              " 'not need',\n",
              " 'not problem',\n",
              " 'not wait',\n",
              " 'not worri',\n",
              " 'perfect',\n",
              " 'pleasur',\n",
              " 'proud',\n",
              " 'readi',\n",
              " 'relax',\n",
              " 'smile',\n",
              " 'song',\n",
              " 'sure',\n",
              " 'sweet',\n",
              " 'thank',\n",
              " 'twitter',\n",
              " 'url',\n",
              " 'watch',\n",
              " 'with',\n",
              " 'with my',\n",
              " 'woohoo',\n",
              " 'work on',\n",
              " 'worth',\n",
              " 'yay',\n",
              " 'ye',\n",
              " 'you',\n",
              " 'you want',\n",
              " 'your'}"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7DiJOP1SRXs"
      },
      "source": [
        ""
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMPRBKbSSROu"
      },
      "source": [
        ""
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA6drjzDODFE"
      },
      "source": [
        "def words_freq(tweet):\n",
        "  num_pos = len(set(tweet).intersection(coef_pos_set))\n",
        "  num_neg = len(set(tweet).intersection(coef_neg_set))\n",
        "  \n",
        "  # If there exist positive words in the tweet\n",
        "  if num_pos:\n",
        "      for num in range(num_pos):\n",
        "          tweet.append('POSITIVE')\n",
        "  if num_neg:\n",
        "      for num in range(num_neg):\n",
        "          tweet.append('NEGATIVE')\n",
        "  return tweet"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2Vobuv9Nplj"
      },
      "source": [
        "# df['coef_pos'] = df['Twitter_final_sent'].str.contains('').value_counts()"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "YHBdfmM_N6au",
        "outputId": "2195bf87-0f62-46f0-e817-4fa26a4c5a5d"
      },
      "source": [
        "df['Tweet_lexicons'] = df['Tweet_stopword'].apply(words_freq)\n",
        "df.head()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                                     Tweet_lexicons\n",
              "0         0  ...  [miss, nikki, nu, nu, already, shes, always, t...\n",
              "1         0  ...  [so, had, dream, last, night, remember, sign, ...\n",
              "2         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "3         0  ...                           [it, is, raining, again]\n",
              "4         0  ...  [MENTION, wish, was, in, la, right, now, NEGAT...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Lxyj1GaQQvFE",
        "outputId": "bc429041-d98c-4c21-d9d4-512ae7676a93"
      },
      "source": [
        "df = make_sentences(df, 'Tweet_lexicons', 'Tweet_final_sent_lexicons')\n",
        "df.head()"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now NEGATIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                          Tweet_final_sent_lexicons\n",
              "0         0  ...  miss nikki nu nu already shes always there whe...\n",
              "1         0  ...  so had dream last night remember sign which cl...\n",
              "2         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "3         0  ...                                it is raining again\n",
              "4         0  ...          MENTION wish was in la right now NEGATIVE\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKe9mpRURIfj"
      },
      "source": [
        "# Think if you want to do stratify\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Tweet_final_sent_lexicons'], df['Polarity'], stratify=df['Polarity'], test_size=0.1, random_state=2)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGKxeOdbRIXg"
      },
      "source": [
        ""
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cej5UAk_RIPc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxugvRxtrMNl"
      },
      "source": [
        "train_pred = pipe.predict(X_train)\n",
        "print(classification_report(train_pred, \n",
        "                            y_train, \n",
        "                            target_names=['negative', 'positive']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23i-hga4aeUG",
        "outputId": "1b52622a-445e-4b1f-8c3d-34c8fc268109"
      },
      "source": [
        "test_pred = pipe.predict(X_test)\n",
        "print(classification_report(test_pred, \n",
        "                            y_test, \n",
        "                            target_names=['negative', 'positive']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.80      0.79     77463\n",
            "    positive       0.81      0.78      0.80     82537\n",
            "\n",
            "    accuracy                           0.79    160000\n",
            "   macro avg       0.79      0.79      0.79    160000\n",
            "weighted avg       0.79      0.79      0.79    160000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-FFs86_a3Af",
        "outputId": "172f6683-b010-43d7-a988-5925e9919704"
      },
      "source": [
        "for i in range(10):\n",
        "    lead = X_test.sample(1)\n",
        "    %timeit pipe.predict(lead)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 674 µs per loop\n",
            "1000 loops, best of 5: 680 µs per loop\n",
            "1000 loops, best of 5: 669 µs per loop\n",
            "1000 loops, best of 5: 677 µs per loop\n",
            "1000 loops, best of 5: 688 µs per loop\n",
            "1000 loops, best of 5: 653 µs per loop\n",
            "1000 loops, best of 5: 697 µs per loop\n",
            "1000 loops, best of 5: 672 µs per loop\n",
            "1000 loops, best of 5: 664 µs per loop\n",
            "1000 loops, best of 5: 671 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWn5Me4SZiFt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBeFQFF1sEWx"
      },
      "source": [
        "pos_words = coefs[coefs['coef']>0].index.tolist()\n",
        "neg_words = coefs[coefs['coef']<0].index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ8_Vpn6sqmP"
      },
      "source": [
        "pos_words_top = coefs[coefs['coef']>1].index.tolist()\n",
        "neg_words_top = coefs[coefs['coef']<-1].index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szs5eiVxsEON"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(pos_words_top, open('/content/drive/MyDrive/COL772_A2/pos_words.txt', 'wb'))\n",
        "pickle.dump(neg_words_top, open('/content/drive/MyDrive/COL772_A2/neg_words.txt', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJnKyqzcs5HI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNENQ2ymZ2Ni"
      },
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(min_df=5, ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "# clf = MultinomialNB(alpha=5)\n",
        "\n",
        "# pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH7OMqaWKSem",
        "outputId": "6f2dd8b7-7ffe-4db4-a11b-5506898cf597"
      },
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1,3))\n",
        "ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "clf = MultinomialNB(alpha=5)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=5, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwN-qih1Z2IQ"
      },
      "source": [
        "# pd.DataFrame(pipeline.predict(X_test)).value_counts()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuFA6EhZ1uU"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "y_pred_self = pipeline.predict(X_test)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMPfipGqg1hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ac22ce-e768-4150-e018-547c0e1e1698"
      },
      "source": [
        "print('F1 Score: ', f1_score(y_test, y_pred_self))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.774746687451286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0LkAWXIqrn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3384b2b6-52af-4486-e2bf-3921ee239a93"
      },
      "source": [
        "sum(y_pred_self == y_test)/len(y_test)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.78325"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCwnBy18f95X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd5849d-03c4-4334-a198-2ae527e50bb2"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_self).ravel()\n",
        "(tp, fp, tn, fn)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2982, 700, 3284, 1034)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6dCDLoXsIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a656743-3ffd-46fb-bf00-b2d4d806d662"
      },
      "source": [
        "confusion_matrix(y_test, y_pred_self)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3284,  700],\n",
              "       [1034, 2982]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj4qy5w5NvBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "0b7c48c5-b336-4633-fb73-0e15092c46ce"
      },
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='l1',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1,\n",
        "                         max_iter = 1000)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-d9f69d8cd92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# disable defaultdict behaviour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr5MV7UpRxHz",
        "outputId": "7e882593-8588-4ffc-bd7b-bccf97a690da"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()\n",
        "(tp, fp, tn, fn)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3025, 739, 3245, 991)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y7eKeCYR3K8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdIldvMRR2-S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAQud2jbRoBb",
        "outputId": "4a356fff-e0a1-4aab-93b6-4a128be2c41f"
      },
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='l2',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.789614356624666\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.793375"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhoOjgr-Rpun"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnnVdzBoOtF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "2ee6f025-1147-4be5-a8c9-b316b0493af7"
      },
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='elasticnet',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-372a99530568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1495\u001b[0m                     self.l1_ratio < 0 or self.l1_ratio > 1):\n\u001b[1;32m   1496\u001b[0m                         raise ValueError(\"l1_ratio must be between 0 and 1;\"\n\u001b[0;32m-> 1497\u001b[0;31m                                          \" got (l1_ratio=%r)\" % self.l1_ratio)\n\u001b[0m\u001b[1;32m   1498\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
            "\u001b[0;31mValueError\u001b[0m: l1_ratio must be between 0 and 1; got (l1_ratio=None)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlTcfz2OhEsw"
      },
      "source": [
        "pd.DataFrame(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33RaqvFIjAG",
        "outputId": "39360b36-ff65-4aba-9471-1695206b00f6"
      },
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(ngram_range=(1,3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "# # all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "# clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "\n",
        "# pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=2, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=0,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3yIwyQzMwdb",
        "outputId": "dc65eb45-eb60-4935-8573-26e33840eb6e"
      },
      "source": [
        "# y_pred_rf = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_rf))\n",
        "# sum(y_pred_rf == y_test)/len(y_test)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.698146595883394\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.594875"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOheGU4oMzQ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUFD3EEhGyn"
      },
      "source": [
        "res = pd.DataFrame({'Prediction': y_pred_self, 'True':y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRBJQ_9IhcKh"
      },
      "source": [
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xekhPSHoikJB"
      },
      "source": [
        "pd.set_option('max_colwidth', 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g-7gSM3hcEj"
      },
      "source": [
        "df.iloc[res[res['Prediction'] != res['True']].index][['Polarity', 'Tweet', 'Tweet_final_sent']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKXSUXW4zMKT"
      },
      "source": [
        "df.iloc[res[res['Prediction'] == res['True']].index][['Polarity', 'Tweet', 'Tweet_final_sent']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OXmr_78nC9P"
      },
      "source": [
        "df.iloc[16775]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBuK9q1IzOGM"
      },
      "source": [
        "# set(stopwords.words('english')).difference(['not', 'very'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdGtab6VnCvj"
      },
      "source": [
        "tfidf_ngrams = TfidfVectorizer(min_df=5, ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('tfidf', tfidf_ngrams)])\n",
        "clf = MultinomialNB(alpha=5)\n",
        "\n",
        "pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_nb = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_nb, pos_label=4))\n",
        "print(sum(y_pred_nb == y_test)/len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKK87GH3nu9J"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOMdfP6SnY68"
      },
      "source": [
        "tfidf_ngrams = TfidfVectorizer(min_df=20, ngram_range=(1, 1))\n",
        "ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='l2',\n",
        "                         solver='lbfgs',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1)\n",
        "\n",
        "pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr, pos_label=4))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZK3JHFUnYzt"
      },
      "source": [
        "tfidf_ngrams.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axsB6UZ4zsvH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qL19-cSZ1o3"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCb7XVMPZ1j0"
      },
      "source": [
        "pickle.dump(pipeline, open('/content/drive/MyDrive/COL772_A2/model_25.txt', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3jVKVs_Z1aP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iaXZEYjzQS7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snQUjqTZ-mCY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sLewfWpzQLx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtdKRlGfn3mN"
      },
      "source": [
        "pos_st = df[df['Polarity'] == 4]['Tweet_normalised'].apply(str.split).sum()\n",
        "neg_st = df[df['Polarity'] == 0]['Tweet_normalised'].apply(str.split).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdsJJR7L8Lly"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPGrxogYn3mN"
      },
      "source": [
        "pos_uni_freq = FreqDist(ngrams(pos_st, 1))\n",
        "neg_uni_freq = FreqDist(ngrams(neg_st, 1))\n",
        "pos_bi_freq = FreqDist(ngrams(pos_st, 2))\n",
        "neg_bi_freq = FreqDist(ngrams(neg_st, 2))\n",
        "pos_tri_freq = FreqDist(ngrams(pos_st, 3))\n",
        "neg_tri_freq = FreqDist(ngrams(neg_st, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83lJc7SGn3mO"
      },
      "source": [
        "pos_uni_top = pos_uni_freq.most_common(1000)\n",
        "neg_uni_top = neg_uni_freq.most_common(1000)\n",
        "pos_bi_top = pos_bi_freq.most_common(1000)\n",
        "neg_bi_top = neg_bi_freq.most_common(1000)\n",
        "pos_tri_top = pos_tri_freq.most_common(1000)\n",
        "neg_tri_top = neg_tri_freq.most_common(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LQYUjw2n3mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfaa15e-21d7-4bfa-c28e-c71ee24eefb7"
      },
      "source": [
        "pos_uni_top"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('get',), 3923),\n",
              " (('go',), 3437),\n",
              " (('laugh',), 3364),\n",
              " (('good',), 3222),\n",
              " (('love',), 2983),\n",
              " (('day',), 2701),\n",
              " (('like',), 2011),\n",
              " (('thanks',), 1767),\n",
              " (('time',), 1686),\n",
              " (('well',), 1591),\n",
              " (('u',), 1581),\n",
              " (('see',), 1562),\n",
              " (('today',), 1525),\n",
              " (('know',), 1443),\n",
              " (('work',), 1413),\n",
              " (('make',), 1411),\n",
              " (('one',), 1397),\n",
              " (('new',), 1373),\n",
              " (('think',), 1369),\n",
              " (('great',), 1326),\n",
              " (('night',), 1209),\n",
              " (('watch',), 1192),\n",
              " (('back',), 1185),\n",
              " (('look',), 1126),\n",
              " (('oh',), 1094),\n",
              " (('would',), 1056),\n",
              " (('twitter',), 1052),\n",
              " (('come',), 1041),\n",
              " (('morning',), 1016),\n",
              " (('happy',), 981),\n",
              " (('hope',), 963),\n",
              " (('really',), 915),\n",
              " (('fun',), 905),\n",
              " (('wait',), 898),\n",
              " (('much',), 886),\n",
              " (('want',), 876),\n",
              " (('say',), 871),\n",
              " (('need',), 854),\n",
              " (('nice',), 818),\n",
              " (('home',), 806),\n",
              " (('thank',), 805),\n",
              " (('take',), 761),\n",
              " (('hey',), 757),\n",
              " (('tomorrow',), 750),\n",
              " (('still',), 716),\n",
              " (('yeah',), 703),\n",
              " (('tweet',), 698),\n",
              " (('follow',), 698),\n",
              " (('yes',), 686),\n",
              " (('awesome',), 686),\n",
              " (('thing',), 675),\n",
              " (('right',), 670),\n",
              " (('feel',), 660),\n",
              " (('way',), 657),\n",
              " (('friend',), 648),\n",
              " (('last',), 644),\n",
              " (('tonight',), 629),\n",
              " (('yay',), 621),\n",
              " (('try',), 594),\n",
              " (('sleep',), 589),\n",
              " (('cool',), 568),\n",
              " (('best',), 564),\n",
              " (('let',), 562),\n",
              " (('movie',), 556),\n",
              " (('week',), 550),\n",
              " (('start',), 540),\n",
              " (('people',), 536),\n",
              " (('enjoy',), 529),\n",
              " (('guy',), 526),\n",
              " (('everyone',), 524),\n",
              " (('gona',), 516),\n",
              " (('could',), 509),\n",
              " (('song',), 501),\n",
              " (('show',), 498),\n",
              " (('girl',), 493),\n",
              " (('play',), 492),\n",
              " (('weekend',), 481),\n",
              " (('bed',), 480),\n",
              " (('soon',), 475),\n",
              " (('little',), 473),\n",
              " (('sure',), 470),\n",
              " (('first',), 468),\n",
              " (('though',), 466),\n",
              " (('ok',), 465),\n",
              " (('use',), 465),\n",
              " (('x',), 464),\n",
              " (('miss',), 461),\n",
              " (('next',), 455),\n",
              " (('please',), 451),\n",
              " (('lt',), 450),\n",
              " (('find',), 443),\n",
              " (('life',), 438),\n",
              " (('ready',), 430),\n",
              " (('tell',), 430),\n",
              " (('give',), 428),\n",
              " (('always',), 420),\n",
              " (('glad',), 416),\n",
              " (('sound',), 413),\n",
              " (('listen',), 412),\n",
              " (('lot',), 410),\n",
              " (('birthday',), 407),\n",
              " (('do',), 404),\n",
              " (('eat',), 403),\n",
              " (('wish',), 399),\n",
              " (('hour',), 395),\n",
              " (('keep',), 387),\n",
              " (('finally',), 386),\n",
              " (('even',), 383),\n",
              " (('pretty',), 380),\n",
              " (('check',), 379),\n",
              " (('long',), 379),\n",
              " (('ur',), 375),\n",
              " (('year',), 375),\n",
              " (('school',), 373),\n",
              " (('read',), 372),\n",
              " (('talk',), 371),\n",
              " (('us',), 368),\n",
              " (('so',), 366),\n",
              " (('never',), 364),\n",
              " (('help',), 363),\n",
              " (('big',), 359),\n",
              " (('hi',), 356),\n",
              " (('welcome',), 352),\n",
              " (('call',), 349),\n",
              " (('finish',), 348),\n",
              " (('ah',), 345),\n",
              " (('buy',), 342),\n",
              " (('maybe',), 341),\n",
              " (('another',), 338),\n",
              " (('something',), 338),\n",
              " (('pic',), 337),\n",
              " (('cute',), 334),\n",
              " (('amaze',), 334),\n",
              " (('wow',), 332),\n",
              " (('world',), 331),\n",
              " (('ever',), 331),\n",
              " (('excite',), 329),\n",
              " (('live',), 325),\n",
              " (('head',), 323),\n",
              " (('god',), 321),\n",
              " (('bad',), 321),\n",
              " (('party',), 320),\n",
              " (('beautiful',), 317),\n",
              " (('old',), 314),\n",
              " (('leave',), 314),\n",
              " (('aw',), 313),\n",
              " (('okay',), 313),\n",
              " (('summer',), 304),\n",
              " (('actually',), 304),\n",
              " (('later',), 302),\n",
              " (('house',), 301),\n",
              " (('man',), 299),\n",
              " (('game',), 296),\n",
              " (('baby',), 293),\n",
              " (('also',), 293),\n",
              " (('stuff',), 291),\n",
              " (('hear',), 290),\n",
              " (('follower',), 285),\n",
              " (('post',), 284),\n",
              " (('bit',), 283),\n",
              " (('music',), 283),\n",
              " (('already',), 282),\n",
              " (('luck',), 282),\n",
              " (('sorry',), 277),\n",
              " (('saw',), 276),\n",
              " (('meet',), 274),\n",
              " (('send',), 272),\n",
              " (('dont',), 272),\n",
              " (('thats',), 270),\n",
              " (('mean',), 270),\n",
              " (('sweet',), 270),\n",
              " (('hot',), 269),\n",
              " (('forward',), 266),\n",
              " (('funny',), 266),\n",
              " (('free',), 265),\n",
              " (('run',), 264),\n",
              " (('dinner',), 262),\n",
              " (('book',), 261),\n",
              " (('th',), 260),\n",
              " (('wana',), 257),\n",
              " (('cant',), 253),\n",
              " (('might',), 252),\n",
              " (('sunday',), 250),\n",
              " (('video',), 249),\n",
              " (('add',), 248),\n",
              " (('kid',), 248),\n",
              " (('sun',), 247),\n",
              " (('mom',), 245),\n",
              " (('someone',), 244),\n",
              " (('two',), 243),\n",
              " (('picture',), 242),\n",
              " (('ha',), 240),\n",
              " (('guess',), 235),\n",
              " (('hello',), 234),\n",
              " (('omg',), 234),\n",
              " (('nothing',), 233),\n",
              " (('put',), 232),\n",
              " (('job',), 228),\n",
              " (('many',), 226),\n",
              " (('friday',), 225),\n",
              " (('godnight',), 224),\n",
              " (('weather',), 223),\n",
              " (('yet',), 222),\n",
              " (('lunch',), 222),\n",
              " (('name',), 222),\n",
              " (('around',), 221),\n",
              " (('end',), 221),\n",
              " (('gota',), 220),\n",
              " (('may',), 220),\n",
              " (('write',), 219),\n",
              " (('win',), 218),\n",
              " (('late',), 218),\n",
              " (('away',), 217),\n",
              " (('hair',), 217),\n",
              " (('lovely',), 216),\n",
              " (('monday',), 215),\n",
              " (('ask',), 215),\n",
              " (('fan',), 215),\n",
              " (('w',), 215),\n",
              " (('early',), 214),\n",
              " (('must',), 213),\n",
              " (('phone',), 212),\n",
              " (('place',), 211),\n",
              " (('boy',), 210),\n",
              " (('family',), 210),\n",
              " (('hehe',), 210),\n",
              " (('p',), 209),\n",
              " (('rock',), 209),\n",
              " (('coffee',), 207),\n",
              " (('plan',), 205),\n",
              " (('r',), 204),\n",
              " (('update',), 204),\n",
              " (('least',), 201),\n",
              " (('smile',), 200),\n",
              " (('stay',), 198),\n",
              " (('xx',), 198),\n",
              " (('word',), 197),\n",
              " (('yesterday',), 195),\n",
              " (('sit',), 193),\n",
              " (('far',), 192),\n",
              " (('drink',), 192),\n",
              " (('every',), 191),\n",
              " (('food',), 190),\n",
              " (('wake',), 188),\n",
              " (('stop',), 186),\n",
              " (('almost',), 185),\n",
              " (('congrats',), 184),\n",
              " (('hm',), 183),\n",
              " (('bring',), 182),\n",
              " (('walk',), 182),\n",
              " (('dance',), 181),\n",
              " (('rain',), 181),\n",
              " (('till',), 180),\n",
              " (('mother',), 179),\n",
              " (('pay',), 177),\n",
              " (('hard',), 176),\n",
              " (('photo',), 175),\n",
              " (('everything',), 175),\n",
              " (('dream',), 175),\n",
              " (('blog',), 174),\n",
              " (('since',), 173),\n",
              " (('amazing',), 172),\n",
              " (('hopefully',), 171),\n",
              " (('break',), 171),\n",
              " (('change',), 171),\n",
              " (('mine',), 170),\n",
              " (('month',), 170),\n",
              " (('btw',), 169),\n",
              " (('anyone',), 169),\n",
              " (('st',), 167),\n",
              " (('fine',), 167),\n",
              " (('true',), 166),\n",
              " (('anything',), 165),\n",
              " (('car',), 164),\n",
              " (('minute',), 164),\n",
              " (('wonderful',), 163),\n",
              " (('real',), 163),\n",
              " (('course',), 161),\n",
              " (('b',), 161),\n",
              " (('tire',), 160),\n",
              " (('idea',), 159),\n",
              " (('class',), 158),\n",
              " (('hang',), 158),\n",
              " (('move',), 158),\n",
              " (('rest',), 155),\n",
              " (('tho',), 155),\n",
              " (('vote',), 154),\n",
              " (('probably',), 154),\n",
              " (('breakfast',), 154),\n",
              " (('beach',), 154),\n",
              " (('hit',), 153),\n",
              " (('room',), 153),\n",
              " (('kind',), 153),\n",
              " (('favorite',), 153),\n",
              " (('tv',), 152),\n",
              " (('xd',), 152),\n",
              " (('crazy',), 152),\n",
              " (('enough',), 151),\n",
              " (('train',), 149),\n",
              " (('believe',), 149),\n",
              " (('turn',), 149),\n",
              " (('reply',), 149),\n",
              " (('totally',), 148),\n",
              " (('visit',), 148),\n",
              " (('saturday',), 148),\n",
              " (('mind',), 147),\n",
              " (('join',), 147),\n",
              " (('la',), 147),\n",
              " (('happen',), 147),\n",
              " (('learn',), 146),\n",
              " (('ill',), 146),\n",
              " (('remember',), 145),\n",
              " (('m',), 145),\n",
              " (('busy',), 145),\n",
              " (('agree',), 144),\n",
              " (('full',), 144),\n",
              " (('link',), 143),\n",
              " (('definitely',), 143),\n",
              " (('dad',), 142),\n",
              " (('wear',), 142),\n",
              " (('lmao',), 142),\n",
              " (('shower',), 141),\n",
              " (('trip',), 141),\n",
              " (('hate',), 140),\n",
              " (('seem',), 140),\n",
              " (('drive',), 140),\n",
              " (('folowfriday',), 138),\n",
              " (('super',), 137),\n",
              " (('exam',), 137),\n",
              " (('brother',), 137),\n",
              " (('email',), 137),\n",
              " (('june',), 137),\n",
              " (('shop',), 135),\n",
              " (('lose',), 135),\n",
              " (('fuck',), 135),\n",
              " (('album',), 134),\n",
              " (('sister',), 134),\n",
              " (('study',), 134),\n",
              " (('clean',), 131),\n",
              " (('f',), 131),\n",
              " (('nite',), 131),\n",
              " (('problem',), 131),\n",
              " (('dude',), 131),\n",
              " (('pm',), 130),\n",
              " (('site',), 130),\n",
              " (('lady',), 129),\n",
              " (('share',), 129),\n",
              " (('dog',), 129),\n",
              " (('pick',), 129),\n",
              " (('face',), 128),\n",
              " (('star',), 128),\n",
              " (('damn',), 128),\n",
              " (('outside',), 128),\n",
              " (('sunny',), 126),\n",
              " (('catch',), 126),\n",
              " (('bore',), 126),\n",
              " (('kinda',), 125),\n",
              " (('part',), 125),\n",
              " (('g',), 125),\n",
              " (('wonder',), 125),\n",
              " (('whole',), 125),\n",
              " (('perfect',), 125),\n",
              " (('care',), 125),\n",
              " (('quite',), 124),\n",
              " (('proud',), 122),\n",
              " (('high',), 121),\n",
              " (('heart',), 121),\n",
              " (('c',), 121),\n",
              " (('else',), 121),\n",
              " (('final',), 121),\n",
              " (('worry',), 119),\n",
              " (('sing',), 119),\n",
              " (('None',), 119),\n",
              " (('message',), 119),\n",
              " (('lucky',), 119),\n",
              " (('anyway',), 118),\n",
              " (('open',), 118),\n",
              " (('award',), 117),\n",
              " (('half',), 117),\n",
              " (('cold',), 117),\n",
              " (('money',), 114),\n",
              " (('relax',), 113),\n",
              " (('concert',), 113),\n",
              " (('cause',), 112),\n",
              " (('season',), 112),\n",
              " (('couple',), 112),\n",
              " (('story',), 112),\n",
              " (('facebok',), 111),\n",
              " (('forget',), 111),\n",
              " (('bday',), 111),\n",
              " (('shit',), 111),\n",
              " (('list',), 110),\n",
              " (('person',), 110),\n",
              " (('afternoon',), 110),\n",
              " (('together',), 110),\n",
              " (('red',), 109),\n",
              " (('computer',), 109),\n",
              " (('yep',), 109),\n",
              " (('yea',), 108),\n",
              " (('hug',), 107),\n",
              " (('church',), 107),\n",
              " (('thought',), 107),\n",
              " (('ice',), 107),\n",
              " (('gt',), 107),\n",
              " (('sign',), 106),\n",
              " (('iphone',), 106),\n",
              " (('fall',), 106),\n",
              " (('ride',), 105),\n",
              " (('close',), 104),\n",
              " (('date',), 104),\n",
              " (('short',), 103),\n",
              " (('easy',), 103),\n",
              " (('figure',), 102),\n",
              " (('laughh',), 102),\n",
              " (('pack',), 102),\n",
              " (('news',), 101),\n",
              " (('text',), 101),\n",
              " (('point',), 101),\n",
              " (('dear',), 100),\n",
              " (('comment',), 100),\n",
              " (('thx',), 100),\n",
              " (('chocolate',), 100),\n",
              " (('sunshine',), 100),\n",
              " (('suck',), 100),\n",
              " (('bless',), 100),\n",
              " (('eye',), 100),\n",
              " (('band',), 99),\n",
              " (('online',), 99),\n",
              " (('lil',), 99),\n",
              " (('bet',), 98),\n",
              " (('top',), 98),\n",
              " (('google',), 97),\n",
              " (('dress',), 97),\n",
              " (('question',), 97),\n",
              " (('beer',), 97),\n",
              " (('ago',), 97),\n",
              " (('less',), 97),\n",
              " (('hand',), 96),\n",
              " (('able',), 96),\n",
              " (('worth',), 96),\n",
              " (('set',), 96),\n",
              " (('special',), 96),\n",
              " (('cheer',), 96),\n",
              " (('city',), 95),\n",
              " (('nd',), 95),\n",
              " (('holiday',), 95),\n",
              " (('sometimes',), 95),\n",
              " (('second',), 94),\n",
              " (('tom',), 94),\n",
              " (('excited',), 94),\n",
              " (('mr',), 94),\n",
              " (('shopping',), 94),\n",
              " (('moment',), 94),\n",
              " (('green',), 94),\n",
              " (('bye',), 93),\n",
              " (('l',), 93),\n",
              " (('bout',), 92),\n",
              " (('page',), 92),\n",
              " (('appreciate',), 92),\n",
              " (('line',), 92),\n",
              " (('weird',), 91),\n",
              " (('ticket',), 91),\n",
              " (('via',), 91),\n",
              " (('mtv',), 91),\n",
              " (('youtube',), 90),\n",
              " (('feeling',), 90),\n",
              " (('til',), 90),\n",
              " (('nap',), 90),\n",
              " (('sick',), 89),\n",
              " (('team',), 89),\n",
              " (('fix',), 88),\n",
              " (('reason',), 88),\n",
              " (('pl',), 88),\n",
              " (('test',), 88),\n",
              " (('goin',), 88),\n",
              " (('office',), 88),\n",
              " (('cake',), 88),\n",
              " (('wrong',), 88),\n",
              " (('cook',), 88),\n",
              " (('hop',), 87),\n",
              " (('cream',), 87),\n",
              " (('twilight',), 87),\n",
              " (('download',), 87),\n",
              " (('spend',), 87),\n",
              " (('woho',), 87),\n",
              " (('hell',), 86),\n",
              " (('yummy',), 86),\n",
              " (('chat',), 86),\n",
              " (('tour',), 86),\n",
              " (('e',), 86),\n",
              " (('record',), 85),\n",
              " (('decide',), 85),\n",
              " (('alright',), 85),\n",
              " (('instead',), 85),\n",
              " (('meeting',), 85),\n",
              " (('awake',), 85),\n",
              " (('kick',), 85),\n",
              " (('chill',), 84),\n",
              " (('support',), 84),\n",
              " (('park',), 84),\n",
              " (('pool',), 84),\n",
              " (('gym',), 84),\n",
              " (('black',), 84),\n",
              " (('episode',), 84),\n",
              " (('yum',), 84),\n",
              " (('vip',), 83),\n",
              " (('moon',), 83),\n",
              " (('si',), 83),\n",
              " (('without',), 83),\n",
              " (('garden',), 82),\n",
              " (('wed',), 82),\n",
              " (('either',), 81),\n",
              " (('david',), 81),\n",
              " (('interest',), 81),\n",
              " (('town',), 81),\n",
              " (('pizza',), 81),\n",
              " (('eh',), 80),\n",
              " (('miley',), 80),\n",
              " (('cd',), 80),\n",
              " (('blue',), 80),\n",
              " (('woke',), 80),\n",
              " (('em',), 80),\n",
              " (('order',), 80),\n",
              " (('project',), 80),\n",
              " (('gorgeous',), 80),\n",
              " (('film',), 79),\n",
              " (('jonas',), 79),\n",
              " (('as',), 79),\n",
              " (('answer',), 79),\n",
              " (('cuz',), 79),\n",
              " (('fly',), 78),\n",
              " (('side',), 78),\n",
              " (('save',), 78),\n",
              " (('fast',), 78),\n",
              " (('congratulation',), 77),\n",
              " (('son',), 77),\n",
              " (('cut',), 77),\n",
              " (('fantastic',), 77),\n",
              " (('beat',), 77),\n",
              " (('mum',), 76),\n",
              " (('plus',), 76),\n",
              " (('yup',), 76),\n",
              " (('water',), 76),\n",
              " (('da',), 76),\n",
              " (('warm',), 76),\n",
              " (('london',), 76),\n",
              " (('white',), 76),\n",
              " (('cousin',), 76),\n",
              " (('v',), 76),\n",
              " (('small',), 75),\n",
              " (('wine',), 75),\n",
              " (('begin',), 75),\n",
              " (('xxx',), 75),\n",
              " (('cat',), 75),\n",
              " (('woo',), 74),\n",
              " (('paper',), 74),\n",
              " (('vacation',), 74),\n",
              " (('heard',), 74),\n",
              " (('voice',), 73),\n",
              " (('number',), 73),\n",
              " (('light',), 73),\n",
              " (('different',), 73),\n",
              " (('fb',), 72),\n",
              " (('suppose',), 72),\n",
              " (('o',), 72),\n",
              " (('dm',), 72),\n",
              " (('cup',), 71),\n",
              " (('chance',), 71),\n",
              " (('tea',), 71),\n",
              " (('internet',), 71),\n",
              " (('um',), 71),\n",
              " (('deserve',), 70),\n",
              " (('twit',), 70),\n",
              " (('paint',), 70),\n",
              " (('type',), 70),\n",
              " (('xoxo',), 70),\n",
              " (('seriously',), 70),\n",
              " (('jealous',), 70),\n",
              " (('stick',), 70),\n",
              " (('hubby',), 70),\n",
              " (('whats',), 70),\n",
              " (('wo',), 69),\n",
              " (('shoe',), 69),\n",
              " (('ipod',), 69),\n",
              " (('babe',), 68),\n",
              " (('hurt',), 68),\n",
              " (('bq',), 68),\n",
              " (('ooh',), 68),\n",
              " (('wednesday',), 68),\n",
              " (('die',), 68),\n",
              " (('especially',), 68),\n",
              " (('laptop',), 68),\n",
              " (('graduation',), 68),\n",
              " (('shal',), 68),\n",
              " (('ive',), 68),\n",
              " (('interview',), 68),\n",
              " (('july',), 68),\n",
              " (('store',), 68),\n",
              " (('quote',), 68),\n",
              " (('woman',), 67),\n",
              " (('english',), 67),\n",
              " (('tune',), 67),\n",
              " (('interesting',), 67),\n",
              " (('min',), 67),\n",
              " (('ugh',), 67),\n",
              " (('absolutely',), 67),\n",
              " (('everybody',), 67),\n",
              " (('safe',), 67),\n",
              " (('de',), 66),\n",
              " (('website',), 66),\n",
              " (('whatever',), 66),\n",
              " (('xo',), 66),\n",
              " (('sad',), 66),\n",
              " (('speak',), 66),\n",
              " (('club',), 65),\n",
              " (('hilarious',), 65),\n",
              " (('shirt',), 65),\n",
              " (('deal',), 65),\n",
              " (('exactly',), 65),\n",
              " (('fabulous',), 65),\n",
              " (('thursday',), 65),\n",
              " (('apple',), 65),\n",
              " (('three',), 64),\n",
              " (('account',), 64),\n",
              " (('touch',), 64),\n",
              " (('cover',), 64),\n",
              " (('stand',), 64),\n",
              " (('mac',), 64),\n",
              " (('mention',), 63),\n",
              " (('load',), 63),\n",
              " (('french',), 63),\n",
              " (('celebrate',), 63),\n",
              " (('kill',), 63),\n",
              " (('finger',), 63),\n",
              " (('hold',), 63),\n",
              " (('taylor',), 63),\n",
              " (('huh',), 62),\n",
              " (('info',), 62),\n",
              " (('indeed',), 62),\n",
              " (('tuesday',), 62),\n",
              " (('guitar',), 62),\n",
              " (('kiss',), 62),\n",
              " (('group',), 62),\n",
              " (('radio',), 62),\n",
              " (('bro',), 62),\n",
              " (('wit',), 62),\n",
              " (('myspace',), 62),\n",
              " (('count',), 62),\n",
              " (('event',), 61),\n",
              " (('blast',), 61),\n",
              " (('past',), 61),\n",
              " (('excellent',), 61),\n",
              " (('shoot',), 61),\n",
              " (('dvd',), 61),\n",
              " (('mate',), 61),\n",
              " (('lazy',), 61),\n",
              " (('flight',), 61),\n",
              " (('sims',), 60),\n",
              " (('wot',), 60),\n",
              " (('huge',), 60),\n",
              " (('homework',), 60),\n",
              " (('hun',), 60),\n",
              " (('sense',), 60),\n",
              " (('reading',), 60),\n",
              " (('road',), 60),\n",
              " (('choice',), 60),\n",
              " (('mood',), 59),\n",
              " (('present',), 59),\n",
              " (('joke',), 59),\n",
              " (('web',), 59),\n",
              " (('hill',), 59),\n",
              " (('random',), 59),\n",
              " (('he',), 59),\n",
              " (('sweetie',), 59),\n",
              " (('yo',), 58),\n",
              " (('h',), 58),\n",
              " (('nah',), 58),\n",
              " (('understand',), 58),\n",
              " (('tip',), 58),\n",
              " (('sort',), 58),\n",
              " (('yr',), 58),\n",
              " (('sell',), 58),\n",
              " (('asleep',), 58),\n",
              " (('fire',), 58),\n",
              " (('wont',), 58),\n",
              " (('trek',), 57),\n",
              " (('bf',), 57),\n",
              " (('stupid',), 57),\n",
              " (('air',), 57),\n",
              " (('bitch',), 57),\n",
              " (('grow',), 57),\n",
              " (('lakers',), 57),\n",
              " (('surprise',), 57),\n",
              " (('profile',), 57),\n",
              " (('others',), 57),\n",
              " (('roll',), 57),\n",
              " (('forever',), 57),\n",
              " (('parent',), 57),\n",
              " (('wife',), 56),\n",
              " (('young',), 56),\n",
              " (('swim',), 56),\n",
              " (('pray',), 56),\n",
              " (('bike',), 56),\n",
              " (('chilin',), 56),\n",
              " (('power',), 56),\n",
              " (('tired',), 56),\n",
              " (('college',), 56),\n",
              " (('business',), 56),\n",
              " (('practice',), 56),\n",
              " (('chicken',), 56),\n",
              " (('peace',), 56),\n",
              " (('anymore',), 56),\n",
              " (('self',), 56),\n",
              " (('release',), 55),\n",
              " (('fresh',), 55),\n",
              " (('pass',), 55),\n",
              " (('drop',), 55),\n",
              " (('tan',), 55),\n",
              " (('shout',), 55),\n",
              " (('lay',), 55),\n",
              " (('uk',), 54),\n",
              " (('ahead',), 54),\n",
              " (('clothes',), 54),\n",
              " (('gig',), 54),\n",
              " (('delicious',), 54),\n",
              " (('sexy',), 54),\n",
              " (('include',), 54),\n",
              " (('rather',), 54),\n",
              " (('pop',), 54),\n",
              " (('child',), 54),\n",
              " (('complete',), 54),\n",
              " (('ima',), 54),\n",
              " (('bag',), 54),\n",
              " (('return',), 53),\n",
              " (('round',), 53),\n",
              " (('notice',), 53),\n",
              " (('bus',), 53),\n",
              " (('fav',), 53),\n",
              " (('smell',), 53),\n",
              " (('thanx',), 53),\n",
              " (('boyfriend',), 53),\n",
              " (('drunk',), 53),\n",
              " (('fact',), 52),\n",
              " (('age',), 52),\n",
              " (('officially',), 52),\n",
              " (('step',), 52),\n",
              " (('joe',), 52),\n",
              " (('body',), 52),\n",
              " (('taste',), 52),\n",
              " (('art',), 52),\n",
              " (('quiet',), 52),\n",
              " (('math',), 52),\n",
              " (('apparently',), 52),\n",
              " (('secret',), 51),\n",
              " (('bc',), 51),\n",
              " (('demi',), 51),\n",
              " (('alot',), 51),\n",
              " (('fit',), 51),\n",
              " (('watchin',), 51),\n",
              " (('travel',), 51),\n",
              " (('cheese',), 51),\n",
              " (('brilliant',), 51),\n",
              " (('john',), 51),\n",
              " (('fight',), 51),\n",
              " (('promise',), 50),\n",
              " (('background',), 50),\n",
              " (('favourite',), 50),\n",
              " (('bird',), 50),\n",
              " (('cross',), 50),\n",
              " (('godmorning',), 50),\n",
              " (('door',), 50),\n",
              " (('slow',), 50),\n",
              " (('glass',), 50),\n",
              " (('burn',), 50),\n",
              " (('ap',), 50),\n",
              " (('hanah',), 50),\n",
              " (('mommy',), 50),\n",
              " (('front',), 49),\n",
              " (('search',), 49),\n",
              " (('upload',), 49),\n",
              " (('mail',), 49),\n",
              " (('gift',), 49),\n",
              " (('epic',), 49),\n",
              " (('heh',), 49),\n",
              " (('version',), 49),\n",
              " (('aha',), 49),\n",
              " (('hungry',), 49),\n",
              " (('realize',), 49),\n",
              " (('pink',), 49),\n",
              " (('fair',), 49),\n",
              " (('crap',), 49),\n",
              " (('you',), 49),\n",
              " (('pas',), 48),\n",
              " (('nope',), 48),\n",
              " (('experience',), 48),\n",
              " (('ball',), 48),\n",
              " (('bear',), 48),\n",
              " (('etc',), 48),\n",
              " (('case',), 48),\n",
              " (('graduate',), 48),\n",
              " (('tweps',), 48),\n",
              " (('review',), 48),\n",
              " (('become',), 47),\n",
              " (('addict',), 47),\n",
              " (('king',), 47),\n",
              " (('hangover',), 47),\n",
              " (('silly',), 47),\n",
              " (('bar',), 47),\n",
              " (('j',), 47),\n",
              " (('matter',), 47),\n",
              " (('design',), 47),\n",
              " (('act',), 47),\n",
              " (('foot',), 47),\n",
              " (('peep',), 47),\n",
              " (('mad',), 47),\n",
              " (('along',), 47),\n",
              " (('except',), 47),\n",
              " (('camera',), 46),\n",
              " (('memory',), 46),\n",
              " (('getin',), 46),\n",
              " (('freak',), 46),\n",
              " (('def',), 46),\n",
              " (('fill',), 46),\n",
              " (('fat',), 46),\n",
              " (('company',), 46),\n",
              " (('wave',), 46),\n",
              " (('hotel',), 46),\n",
              " (('annoy',), 46),\n",
              " (('daughter',), 46),\n",
              " (('future',), 45),\n",
              " (('lame',), 45),\n",
              " (('important',), 45),\n",
              " (('camp',), 45),\n",
              " (('england',), 45),\n",
              " (('jk',), 45),\n",
              " (('state',), 45),\n",
              " (('chris',), 45),\n",
              " (('american',), 45),\n",
              " (('land',), 45),\n",
              " (('mall',), 44),\n",
              " (('nick',), 44),\n",
              " (('war',), 44),\n",
              " (('jump',), 44),\n",
              " (('color',), 44),\n",
              " (('card',), 44),\n",
              " (('ho',), 44),\n",
              " (('ai',), 44),\n",
              " (('dark',), 44),\n",
              " (('lately',), 44),\n",
              " (('earlier',), 44),\n",
              " (('topic',), 44),\n",
              " (('prom',), 44),\n",
              " (('ate',), 44),\n",
              " (('positive',), 44),\n",
              " (('adorable',), 44),\n",
              " (('article',), 44),\n",
              " (('evening',), 44),\n",
              " (('mcfly',), 44),\n",
              " (('sale',), 44),\n",
              " (('race',), 44),\n",
              " (('pc',), 44),\n",
              " (('anyways',), 44),\n",
              " (('bug',), 43),\n",
              " (('perhaps',), 43),\n",
              " (('alone',), 43),\n",
              " (('daddy',), 43),\n",
              " (('recommend',), 43),\n",
              " (('fam',), 43),\n",
              " (('doubt',), 43),\n",
              " (('sir',), 43),\n",
              " (('straight',), 43),\n",
              " (('currently',), 43),\n",
              " (('usually',), 43),\n",
              " (('bottle',), 43),\n",
              " (('buddy',), 43),\n",
              " (('track',), 43),\n",
              " (('pleasure',), 43),\n",
              " (('channel',), 43),\n",
              " (('gosh',), 43),\n",
              " (('spending',), 43),\n",
              " (('country',), 43),\n",
              " (('wall',), 43),\n",
              " (('treat',), 43),\n",
              " (('folk',), 43),\n",
              " (('mess',), 43),\n",
              " (('bath',), 43),\n",
              " (('island',), 42),\n",
              " (('sigh',), 42),\n",
              " (('lie',), 42),\n",
              " (('germany',), 42),\n",
              " (('remind',), 42),\n",
              " (('single',), 42),\n",
              " (('teach',), 42),\n",
              " (('boo',), 42),\n",
              " (('view',), 42),\n",
              " (('style',), 42),\n",
              " (('service',), 42),\n",
              " (('consider',), 42),\n",
              " (('puppy',), 42),\n",
              " (('usual',), 42),\n",
              " (('note',), 42),\n",
              " (('trailer',), 42),\n",
              " (('shin',), 41),\n",
              " (('nail',), 41),\n",
              " (('copy',), 41),\n",
              " (('price',), 41),\n",
              " (('hr',), 41),\n",
              " (('mile',), 41),\n",
              " (('dead',), 41),\n",
              " (('nearly',), 41),\n",
              " (('jus',), 41),\n",
              " (('expect',), 41),\n",
              " (('everyday',), 41),\n",
              " (('space',), 41),\n",
              " (('lesson',), 41),\n",
              " (('arrive',), 41),\n",
              " (('fail',), 41),\n",
              " (('rise',), 41),\n",
              " (('felt',), 41),\n",
              " (('station',), 41),\n",
              " (('near',), 41),\n",
              " (('prepare',), 41),\n",
              " (('wedding',), 41),\n",
              " (('doin',), 40),\n",
              " (('form',), 40),\n",
              " (('normal',), 40),\n",
              " (('canot',), 40),\n",
              " (('itunes',), 40),\n",
              " (('model',), 40),\n",
              " (('bake',), 40),\n",
              " (('pain',), 40),\n",
              " (('simple',), 40),\n",
              " (('low',), 40),\n",
              " (('create',), 40),\n",
              " (('studio',), 40),\n",
              " (('lake',), 40),\n",
              " (('window',), 40),\n",
              " (('gr',), 40),\n",
              " (('south',), 40),\n",
              " (('alaugh',), 40),\n",
              " (('cry',), 40),\n",
              " (('bright',), 40),\n",
              " (('sky',), 40),\n",
              " (('official',), 40),\n",
              " (('inside',), 40),\n",
              " (('anytime',), 39),\n",
              " (('toy',), 39),\n",
              " (('extra',), 39),\n",
              " (('draw',), 39),\n",
              " (('fish',), 39),\n",
              " (('men',), 39),\n",
              " (('angel',), 39),\n",
              " (('thinking',), 39),\n",
              " (('ops',), 39),\n",
              " (('sex',), 39),\n",
              " (('shot',), 39),\n",
              " (('manage',), 38),\n",
              " (('degree',), 38),\n",
              " (('thankyou',), 38),\n",
              " (('box',), 38),\n",
              " (('street',), 38),\n",
              " (('invite',), 38),\n",
              " (('airport',), 38),\n",
              " (('friends',), 38),\n",
              " (('print',), 38),\n",
              " (('direct',), 38),\n",
              " (('n',), 38),\n",
              " (('dany',), 38),\n",
              " (('stage',), 38),\n",
              " (('cheap',), 38),\n",
              " (('pull',), 38),\n",
              " (('unles',), 38),\n",
              " (('although',), 38),\n",
              " (('productive',), 38),\n",
              " (('sushi',), 38),\n",
              " (('poor',), 38),\n",
              " (('hero',), 38),\n",
              " (('result',), 38),\n",
              " (('hehehe',), 37),\n",
              " (('mark',), 37),\n",
              " (('anniversary',), 37),\n",
              " (('gay',), 37),\n",
              " (('magic',), 37),\n",
              " (('matt',), 37),\n",
              " (('august',), 37),\n",
              " (('click',), 37),\n",
              " (('detail',), 37),\n",
              " (('marathon',), 37),\n",
              " (('choose',), 37),\n",
              " (('quick',), 37),\n",
              " (('aka',), 37),\n",
              " (('tomorow',), 37),\n",
              " (('drinking',), 37),\n",
              " (('often',), 37),\n",
              " (('behind',), 37),\n",
              " (('michael',), 37),\n",
              " (('wash',), 37),\n",
              " (('lovin',), 37),\n",
              " (('rd',), 37),\n",
              " (('wop',), 37)]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYwMmhK7n3mO"
      },
      "source": [
        "def get_top_words(sent_list):\n",
        "    word_list = []\n",
        "    for i in range(len(sent_list)):\n",
        "        word_list.append(sent_list[i][0][0])\n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpIVe1En3mO"
      },
      "source": [
        "pos_uni_top_words = get_top_words(pos_uni_top)\n",
        "neg_uni_top_words = get_top_words(neg_uni_top)\n",
        "pos_bi_top_words = get_top_words(pos_bi_top)\n",
        "neg_bi_top_words = get_top_words(neg_bi_top)\n",
        "pos_tri_top_words = get_top_words(pos_tri_top)\n",
        "neg_tri_top_words = get_top_words(neg_tri_top)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_8TSTCln3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd09b523-5b78-4a97-a80c-c42a4687c3cc"
      },
      "source": [
        "print(len(pos_uni_top_words))\n",
        "print(len(neg_uni_top_words))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtR4fAUzn3mP"
      },
      "source": [
        "uni_top_common = set(set(pos_uni_top_words) & set(neg_uni_top_words))\n",
        "pos_best_words = list(set(pos_uni_top_words) - uni_top_common)\n",
        "neg_best_words = list(set(neg_uni_top_words) - uni_top_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViSkXIhKn3mP"
      },
      "source": [
        "uni_top_common_list = list(uni_top_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwgQaTDzn3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dfb6a5-36b0-440c-d601-f7a7f067a3d6"
      },
      "source": [
        "pos_best_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aha',\n",
              " 'usually',\n",
              " 'interesting',\n",
              " 'hilarious',\n",
              " 'nah',\n",
              " 'proud',\n",
              " 'choose',\n",
              " 'lovin',\n",
              " 'thanx',\n",
              " 'promise',\n",
              " 'draw',\n",
              " 'excited',\n",
              " 'thankyou',\n",
              " 'bath',\n",
              " 'appreciate',\n",
              " 'gorgeous',\n",
              " 'wo',\n",
              " 'ahead',\n",
              " 'sexy',\n",
              " 'wop',\n",
              " 'j',\n",
              " 'ai',\n",
              " 'positive',\n",
              " 'official',\n",
              " 'review',\n",
              " 'heh',\n",
              " 'quote',\n",
              " 'surprise',\n",
              " 'germany',\n",
              " 'hun',\n",
              " 'pizza',\n",
              " 'consider',\n",
              " 'john',\n",
              " 'sex',\n",
              " 'war',\n",
              " 'perfect',\n",
              " 'profile',\n",
              " 'topic',\n",
              " 'matt',\n",
              " 'choice',\n",
              " 'demi',\n",
              " 'fav',\n",
              " 'design',\n",
              " 'jk',\n",
              " 'rd',\n",
              " 'shal',\n",
              " 'anyways',\n",
              " 'indeed',\n",
              " 'dany',\n",
              " 'often',\n",
              " 'congratulation',\n",
              " 'paint',\n",
              " 'usual',\n",
              " 'etc',\n",
              " 'hanah',\n",
              " 'experience',\n",
              " 'adorable',\n",
              " 'shin',\n",
              " 'anniversary',\n",
              " 'aka',\n",
              " 'brilliant',\n",
              " 'sweetie',\n",
              " 'bake',\n",
              " 'unles',\n",
              " 'prepare',\n",
              " 'direct',\n",
              " 'count',\n",
              " 'folk',\n",
              " 'sky',\n",
              " 'studio',\n",
              " 'view',\n",
              " 'quiet',\n",
              " 'gift',\n",
              " 'tweps',\n",
              " 'everybody',\n",
              " 'space',\n",
              " 'color',\n",
              " 'fresh',\n",
              " 'fabulous',\n",
              " 'lake',\n",
              " 'joe',\n",
              " 'include',\n",
              " 'bright',\n",
              " 'peace',\n",
              " 'info',\n",
              " 'detail',\n",
              " 'chilin',\n",
              " 'treat',\n",
              " 'michael',\n",
              " 'doin',\n",
              " 'wave',\n",
              " 'american',\n",
              " 'tip',\n",
              " 'drinking',\n",
              " 'secret',\n",
              " 'king',\n",
              " 'fam',\n",
              " 'recommend',\n",
              " 'pop',\n",
              " 'england',\n",
              " 'price',\n",
              " 'dvd',\n",
              " 'mention',\n",
              " 'thx',\n",
              " 'silly',\n",
              " 'watchin',\n",
              " 'style',\n",
              " 'cross',\n",
              " 'marathon',\n",
              " 'self',\n",
              " 'bottle',\n",
              " 'pleasure',\n",
              " 'alright',\n",
              " 'shout',\n",
              " 'welcome',\n",
              " 'gig',\n",
              " 'sir',\n",
              " 'quick',\n",
              " 'nick',\n",
              " 'present',\n",
              " 'celebrate',\n",
              " 'taylor',\n",
              " 'currently',\n",
              " 'teach',\n",
              " 'straight',\n",
              " 'prom',\n",
              " 'spending',\n",
              " 'hero',\n",
              " 'nail',\n",
              " 'mcfly',\n",
              " 'model',\n",
              " 'rise',\n",
              " 'men',\n",
              " 'hehehe',\n",
              " 'create',\n",
              " 'epic',\n",
              " 'magic',\n",
              " 'bless',\n",
              " 'important',\n",
              " 'toy',\n",
              " 'fill',\n",
              " 'lesson',\n",
              " 'favourite',\n",
              " 'yum',\n",
              " 'reading',\n",
              " 'track',\n",
              " 'smile',\n",
              " 'blast',\n",
              " 'de',\n",
              " 'delicious',\n",
              " 'mate',\n",
              " 'trek',\n",
              " 'jump',\n",
              " 'friends',\n",
              " 'pink',\n",
              " 'godmorning',\n",
              " 'click',\n",
              " 'future',\n",
              " 'island',\n",
              " 'fantastic',\n",
              " 'vip',\n",
              " 'sense',\n",
              " 'woho',\n",
              " 'evening',\n",
              " 'article',\n",
              " 'twilight',\n",
              " 'yummy',\n",
              " 'ooh',\n",
              " 'background',\n",
              " 'anytime',\n",
              " 'tune',\n",
              " 'wall',\n",
              " 'woo',\n",
              " 'print',\n",
              " 'wot',\n",
              " 'copy',\n",
              " 'xo',\n",
              " 'stage',\n",
              " 'form',\n",
              " 'deserve',\n",
              " 'addict',\n",
              " 'congrats',\n",
              " 'alaugh',\n",
              " 'chris',\n",
              " 'wedding',\n",
              " 'mall',\n",
              " 'everyday',\n",
              " 'interest',\n",
              " 'simple',\n",
              " 'angel',\n",
              " 'trailer',\n",
              " 'excellent',\n",
              " 'def',\n",
              " 'productive',\n",
              " 'cheap',\n",
              " 'doubt',\n",
              " 'roll',\n",
              " 'perhaps',\n",
              " 'peep',\n",
              " 'folowfriday',\n",
              " 'he',\n",
              " 'sushi',\n",
              " 'step',\n",
              " 'land',\n",
              " 'graduate']"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDW7ZHEn3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d7b765-a462-4301-8a38-ef0e9042d480"
      },
      "source": [
        "neg_best_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['delay',\n",
              " 'completely',\n",
              " 'cough',\n",
              " 'disappointed',\n",
              " 'fell',\n",
              " 'cancel',\n",
              " 'thunder',\n",
              " 'dnt',\n",
              " 'kate',\n",
              " 'ew',\n",
              " 'piss',\n",
              " 'blow',\n",
              " 'terrible',\n",
              " 'uh',\n",
              " 'shift',\n",
              " 'lonely',\n",
              " 'throat',\n",
              " 'nightmare',\n",
              " 'fever',\n",
              " 'however',\n",
              " 'blood',\n",
              " 'confuse',\n",
              " 'stuck',\n",
              " 'starbucks',\n",
              " 'to',\n",
              " 'broken',\n",
              " 'traffic',\n",
              " 'trouble',\n",
              " 'sat',\n",
              " 'somewhere',\n",
              " 'unfortunately',\n",
              " 'screw',\n",
              " 'ring',\n",
              " 'ac',\n",
              " 'shitty',\n",
              " 'bank',\n",
              " 'rid',\n",
              " 'board',\n",
              " 'entire',\n",
              " 'couldnt',\n",
              " 'shes',\n",
              " 'painful',\n",
              " 'wasnt',\n",
              " 'wah',\n",
              " 'floor',\n",
              " 'rip',\n",
              " 'crappy',\n",
              " 'doctor',\n",
              " 'mouth',\n",
              " 'freeze',\n",
              " 'key',\n",
              " 'dread',\n",
              " 'blackberry',\n",
              " 'block',\n",
              " 'laundry',\n",
              " 'anywhere',\n",
              " 'co',\n",
              " 'exhaust',\n",
              " 'burnt',\n",
              " 'vet',\n",
              " 'bummer',\n",
              " 'blah',\n",
              " 'no',\n",
              " 'bo',\n",
              " 'atm',\n",
              " 'there',\n",
              " 'san',\n",
              " 'shut',\n",
              " 'slept',\n",
              " 'arm',\n",
              " 'tummy',\n",
              " 'accident',\n",
              " 'left',\n",
              " 'wat',\n",
              " 'france',\n",
              " 'knee',\n",
              " 'shame',\n",
              " 'ear',\n",
              " 'bored',\n",
              " 'guted',\n",
              " 'duno',\n",
              " 'science',\n",
              " 'sadly',\n",
              " 'ouch',\n",
              " 'leg',\n",
              " 'report',\n",
              " 'fml',\n",
              " 'aint',\n",
              " 'headache',\n",
              " 'assignment',\n",
              " 'darn',\n",
              " 'revision',\n",
              " 'depress',\n",
              " 'weight',\n",
              " 'schedule',\n",
              " 'doesnt',\n",
              " 'gah',\n",
              " 'bill',\n",
              " 'bloody',\n",
              " 'none',\n",
              " 'hat',\n",
              " 'size',\n",
              " 'horrible',\n",
              " 'upset',\n",
              " 'apart',\n",
              " 'swear',\n",
              " 'cell',\n",
              " 'wtf',\n",
              " 'middle',\n",
              " 'delete',\n",
              " 'forgot',\n",
              " 'throw',\n",
              " 'due',\n",
              " 'match',\n",
              " 'dc',\n",
              " 'argh',\n",
              " 'sunburn',\n",
              " 'nyc',\n",
              " 'cost',\n",
              " 'battery',\n",
              " 'bite',\n",
              " 'barely',\n",
              " 'ow',\n",
              " 'neck',\n",
              " 'jon',\n",
              " 'sleepy',\n",
              " 'nobody',\n",
              " 'loss',\n",
              " 'outa',\n",
              " 'file',\n",
              " 'serious',\n",
              " 'stomach',\n",
              " 'disappoint',\n",
              " 'damit',\n",
              " 'allow',\n",
              " 'major',\n",
              " 'sore',\n",
              " 'error',\n",
              " 'death',\n",
              " 'me',\n",
              " 'kno',\n",
              " 'felin',\n",
              " 'realy',\n",
              " 'crash',\n",
              " 'boring',\n",
              " 'longer',\n",
              " 'dang',\n",
              " 'heat',\n",
              " 'thru',\n",
              " 'waste',\n",
              " 'history',\n",
              " 'sum',\n",
              " 'bum',\n",
              " 'issue',\n",
              " 'milk',\n",
              " 'plane',\n",
              " 'swine',\n",
              " 'feed',\n",
              " 'goodbye',\n",
              " 'upgrade',\n",
              " 'suffer',\n",
              " 'flu',\n",
              " 'fault',\n",
              " 'isnt',\n",
              " 'tear',\n",
              " 'revise',\n",
              " 'awful',\n",
              " 'twetdeck',\n",
              " 'essay',\n",
              " 'empty',\n",
              " 'bb',\n",
              " 'wet',\n",
              " 'series',\n",
              " 'afraid',\n",
              " 'tried',\n",
              " 'piece',\n",
              " 'scary',\n",
              " 'hospital',\n",
              " 'afford',\n",
              " 'smh',\n",
              " 'kitty',\n",
              " 'bah',\n",
              " 'nervous',\n",
              " 'dentist',\n",
              " 'ny',\n",
              " 'area',\n",
              " 'nose',\n",
              " 'spent',\n",
              " 'lack',\n",
              " 'rainy',\n",
              " 'storm',\n",
              " 'chicago',\n",
              " 'chip',\n",
              " 'teeth',\n",
              " 'energy',\n",
              " 'bother',\n",
              " 'badly',\n",
              " 'rubbish',\n",
              " 'father',\n",
              " 'ruin',\n",
              " 'ache',\n",
              " 'screen',\n",
              " 'stress',\n",
              " 'scar',\n",
              " 'idk']"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMzecSSn3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e00ec2-16ff-48e9-9c79-bbfc0f04b752"
      },
      "source": [
        "len(pos_best_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB_0TxHx_Ru1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edNiU6Lt_RmT"
      },
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(min_df=5, ngram_range=(1, 3))\n",
        "# # ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('tfidf', tfidf_ngrams), ('pos', pos_best_words), ('neg', neg_best_words)])\n",
        "# clf = MultinomialNB(alpha=1)\n",
        "\n",
        "# pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)\n",
        "# y_pred_nb = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_nb, pos_label=4))\n",
        "# print(sum(y_pred_nb == y_test)/len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm6UtHHq_Rep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znOCGskI_RXI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L3PGj_tn3mQ"
      },
      "source": [
        "df['Tweet'].iloc[50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJQGu8MQn3mQ"
      },
      "source": [
        "df['Tweet_sent'].iloc[50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PgsEeJgn3mQ"
      },
      "source": [
        "data.iloc[786897,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8uV_BEUn3mQ"
      },
      "source": [
        "df.iloc[786897]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO2zLY4en3mQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOsgBKtVn3mQ"
      },
      "source": [
        "def dummy(tweet):\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PZLGWtln3mQ"
      },
      "source": [
        "cv = CountVectorizer(  \n",
        "                      tokenizer=dummy,\n",
        "                      preprocessor=dummy,\n",
        "                      ngram_range=(1,1)\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDzznVAv4pvv"
      },
      "source": [
        "print(\"Hi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTbL9jXA97B4"
      },
      "source": [
        "X = df['Polarity', 'Tweet_final_sent']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X['Tweet_final_sent'], X['Polarity'], test_size=0.25, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhpBw2z0AaNB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5XuISksn3mQ"
      },
      "source": [
        "# X = cv.fit_transform(df['Tweet_lemma']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DGeOOIin3mR"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsU_eJ5An3mR"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFtvESIZn3mR"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, df['Polarity'], test_size=0.25, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ypp_tZnn3mR"
      },
      "source": [
        "# X_train = X[:80000,:]\n",
        "# X_test = X[80000:,:]\n",
        "# y_train = df['Polarity'][:80000]\n",
        "# y_test = df['Polarity'][80000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD79Odccn3mR"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orS-wg43n3mR"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTd8FBWUn3mR"
      },
      "source": [
        "def model_run(model, X_train, y_train):\n",
        "    model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqpCXItln3mS"
      },
      "source": [
        "def model_predict(model, X_test, y_test):\n",
        "    print('Accuracy is: ', model.score(X_test, y_test)*100)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ87hKTBn3mS"
      },
      "source": [
        "model = MultinomialNB()\n",
        "model_run(model, X_train, y_train)\n",
        "model_predict(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00iZcq3hn3mS"
      },
      "source": [
        "# model = LogisticRegression()\n",
        "# model_run(model, X_train, y_train)\n",
        "# model_predict(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG2O5mrBn3mS"
      },
      "source": [
        "model_predict(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nel28Mkn3mS"
      },
      "source": [
        "model = LinearSVC()\n",
        "model_run(model, X_train, y_train)\n",
        "model_predict(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFivm_OIn3mS"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YYEKtBZn3mS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yX_9h_On3mS"
      },
      "source": [
        "tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
        "                               ngram_range=(1,1)) # number of n-grams\n",
        "tfidf_data = tfidf_counts.fit_transform(df['Tweet_sent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSrxzm9Jn3mT"
      },
      "source": [
        "# tfidf_counts = TfidfVectorizer()\n",
        "# tfidf_data = tfidf_counts.fit_transform(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9fQlzEJn3mT"
      },
      "source": [
        "tfidf_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uigcoDrPn3mT"
      },
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data, df['Polarity'], test_size=0.25, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6qOVYs8n3mT"
      },
      "source": [
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)\n",
        "print(y_train_tfidf.shape)\n",
        "print(y_test_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mESGywGun3mT"
      },
      "source": [
        "model = MultinomialNB()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZx8UNnPn3mT"
      },
      "source": [
        "model = LinearSVC()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RV7QCEqn3mT"
      },
      "source": [
        "# model = LogisticRegression()\n",
        "# model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "# model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0fs6PGyn3mT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7XlAJhKn3mT"
      },
      "source": [
        "def remove_extra_words(tweet):\n",
        "    tweet = [word for word in tweet if word in uni_top_common_list]\n",
        "    if len(tweet) == 0:\n",
        "        tweet = ['None']\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "jvmXb6ehn3mU",
        "outputId": "d38dce81-97de-441a-ac83-277822d5d5d6"
      },
      "source": [
        "df['Tweet_remove_extra'] = df['Tweet_lemma'].apply(remove_extra_words)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_clean</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pos</th>\n",
              "      <th>Tweet_lemma</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_remove_extra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[(miss, JJ), (nikki, NN), (nu, JJ), (nu, JJ), ...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>miss nikki nu nu already shes always need than...</td>\n",
              "      <td>[miss, already, shes, always, need, thank, xxx]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>So had dream last night remember sign which cl...</td>\n",
              "      <td>[So, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[(So, RB), (dream, NN), (last, JJ), (night, NN...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>So dream last night remember sign clearly tell...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, tell,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>ohh poor sickly you hugs hope you feel little ...</td>\n",
              "      <td>[ohh, poor, sickly, you, hugs, hope, you, feel...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[(ohh, JJ), (poor, JJ), (sickly, JJ), (hugs, N...</td>\n",
              "      <td>[ohh, poor, sickly, hug, hope, feel, little, g...</td>\n",
              "      <td>ohh poor sickly hug hope feel little good soon</td>\n",
              "      <td>[hug, hope, feel, little, good, soon]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[(raining, VBG)]</td>\n",
              "      <td>[rain]</td>\n",
              "      <td>rain</td>\n",
              "      <td>[rain]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>wish was in LA right now</td>\n",
              "      <td>[wish, was, in, LA, right, now]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[(wish, JJ), (LA, NNP), (right, NN)]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>wish LA right</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                 Tweet_remove_extra\n",
              "514293         0  ...    [miss, already, shes, always, need, thank, xxx]\n",
              "142282         0  ...  [So, dream, last, night, remember, sign, tell,...\n",
              "403727         0  ...              [hug, hope, feel, little, good, soon]\n",
              "649503         0  ...                                             [rain]\n",
              "610789         0  ...                                  [wish, LA, right]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "HATMatIln3mU",
        "outputId": "a58ccc56-e601-4464-d83e-340d77dd8165"
      },
      "source": [
        "df = make_sentences(df, 'Tweet_remove_extra', 'Tweet_final_sent')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_clean</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pos</th>\n",
              "      <th>Tweet_lemma</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_remove_extra</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[(miss, JJ), (nikki, NN), (nu, JJ), (nu, JJ), ...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>miss nikki nu nu already shes always need than...</td>\n",
              "      <td>[miss, already, shes, always, need, thank, xxx]</td>\n",
              "      <td>miss already shes always need thank xxx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>So had dream last night remember sign which cl...</td>\n",
              "      <td>[So, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[(So, RB), (dream, NN), (last, JJ), (night, NN...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>So dream last night remember sign clearly tell...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, tell,...</td>\n",
              "      <td>So dream last night remember sign tell get job...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>ohh poor sickly you hugs hope you feel little ...</td>\n",
              "      <td>[ohh, poor, sickly, you, hugs, hope, you, feel...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[(ohh, JJ), (poor, JJ), (sickly, JJ), (hugs, N...</td>\n",
              "      <td>[ohh, poor, sickly, hug, hope, feel, little, g...</td>\n",
              "      <td>ohh poor sickly hug hope feel little good soon</td>\n",
              "      <td>[hug, hope, feel, little, good, soon]</td>\n",
              "      <td>hug hope feel little good soon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[(raining, VBG)]</td>\n",
              "      <td>[rain]</td>\n",
              "      <td>rain</td>\n",
              "      <td>[rain]</td>\n",
              "      <td>rain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>wish was in LA right now</td>\n",
              "      <td>[wish, was, in, LA, right, now]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[(wish, JJ), (LA, NNP), (right, NN)]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>wish LA right</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>wish LA right</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                   Tweet_final_sent\n",
              "514293         0  ...            miss already shes always need thank xxx\n",
              "142282         0  ...  So dream last night remember sign tell get job...\n",
              "403727         0  ...                     hug hope feel little good soon\n",
              "649503         0  ...                                               rain\n",
              "610789         0  ...                                      wish LA right\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyEMK9Zgn3mU"
      },
      "source": [
        "tfidf_counts_clean = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
        "                               ngram_range=(1,2)) # number of n-grams\n",
        "tfidf_data_clean = tfidf_counts_clean.fit_transform(df['Tweet_final_sent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOAglaNrn3mU"
      },
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data_clean, df['Polarity'], test_size=0.25, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmdAtf87n3mU",
        "outputId": "02b91324-d84f-4518-fc46-eebcf973d06b"
      },
      "source": [
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)\n",
        "print(y_train_tfidf.shape)\n",
        "print(y_test_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 69746)\n",
            "(10000, 69746)\n",
            "(30000,)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwYS2-hIn3mV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIswNSMrn3mV",
        "outputId": "add4a826-b502-4358-e696-56cb24e8350c"
      },
      "source": [
        "model = MultinomialNB()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is:  70.34\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.70      0.70      5048\n",
            "           4       0.70      0.71      0.70      4952\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.70      0.70      0.70     10000\n",
            "weighted avg       0.70      0.70      0.70     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVWsBS3Jn3mV",
        "outputId": "111bbdeb-32c3-42e6-d263-54708a2c7208"
      },
      "source": [
        "model = LinearSVC()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is:  69.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.66      0.69      5048\n",
            "           4       0.68      0.73      0.70      4952\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.70      0.70      0.70     10000\n",
            "weighted avg       0.70      0.70      0.70     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCZZ5xLpn3mV",
        "outputId": "b2c74952-1dbf-49f7-c119-1d097b35e27b"
      },
      "source": [
        "model = LogisticRegression()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is:  71.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.69      0.71      5048\n",
            "           4       0.70      0.74      0.72      4952\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.71      0.71      0.71     10000\n",
            "weighted avg       0.71      0.71      0.71     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBmCEDNxn3mV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlctbH1y9sBJ"
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuG25I_j9r7-"
      },
      "source": [
        "text_clf = Pipeline([\n",
        "    ('tfidf',TfidfVectorizer(preprocessor=None,\n",
        "                             tokenizer=word_tokenize,\n",
        "                             analyzer='word',\n",
        "                             stop_words=None,\n",
        "                             strip_accents=None,\n",
        "                             lowercase=True,\n",
        "                             ngram_range=(1,3),\n",
        "                             min_df=0.0001,\n",
        "                             max_df=0.9,\n",
        "                             binary=False,\n",
        "                             norm='l2',\n",
        "                             use_idf=1,\n",
        "                             smooth_idf=1,\n",
        "                             sublinear_tf=1)),\n",
        "    ('clf', LogisticRegression(penalty='l2',\n",
        "                               solver='saga',\n",
        "                               multi_class='multinomial',\n",
        "                              tol=1e-5,\n",
        "                              n_jobs = -1)),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U8vp1Fu9srh"
      },
      "source": [
        "\n",
        "text_clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}