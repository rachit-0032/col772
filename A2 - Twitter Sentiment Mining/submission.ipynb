{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.13 64-bit ('col772_a2': conda)"
    },
    "interpreter": {
      "hash": "47824db9596938d92c445fad5d5791b4eb715b27479fa3e5bf7a5307ac59afb6"
    },
    "colab": {
      "name": "submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOJ3T0WWDGRo",
        "outputId": "9edc3ef1-4550-4024-d3f9-e08fc3a0d3d5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ij7awMq6n3mA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "# from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
      ],
      "outputs": [],
      "metadata": {
        "id": "iDF0OMEqn3mB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [
        "# from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import ngrams, FreqDist\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/rachitjain/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/rachitjain/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/rachitjain/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/rachitjain/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjv0vD98n3mC",
        "outputId": "1215a54f-57e1-4978-8a66-31905f6c5d5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "source": [
        "data_neg = pd.read_csv('Data/training_negative.csv', encoding='ISO-8859-1')\n",
        "data_pos = pd.read_csv('Data/training_positive.csv', encoding='ISO-8859-1')"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZX7SViZ8n3mD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "source": [
        "data = data_neg.append(data_pos)\n",
        "data = data.iloc[:,1:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "sHmk4NfMn3mD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "data.tail()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>799995</th>\n",
              "      <td>4</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799996</th>\n",
              "      <td>4</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799997</th>\n",
              "      <td>4</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799998</th>\n",
              "      <td>4</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799999</th>\n",
              "      <td>4</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet\n",
              "799995         4  Just woke up. Having no school is the best fee...\n",
              "799996         4  TheWDB.com - Very cool to hear old Walt interv...\n",
              "799997         4  Are you ready for your MoJo Makeover? Ask me f...\n",
              "799998         4  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "799999         4  happy #charitytuesday @theNSPCC @SparksCharity..."
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OeM4QErjn3mD",
        "outputId": "ba06c3ae-ded8-440e-a6d5-f275124de4e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Data for Trying out Approaches"
      ],
      "metadata": {
        "id": "8ZGO7-aen3mE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "source": [
        "df = data.sample(frac=1, random_state=1)\n",
        "# df = data.iloc[:30000]\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet\n",
              "514293         0  i miss nikki nu nu already  shes always there ...\n",
              "142282         0  So I had a dream last night. I  remember a sig...\n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...\n",
              "649503         0                               it is raining again \n",
              "610789         0          @MissKeriBaby wish I was in LA right now "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Vw9pS_JFn3mF",
        "outputId": "155cd77d-53d0-4e3a-a8db-36d93c22c565"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "source": [
        "# To convert the polarity symbols from 4 to 1\n",
        "df['Polarity'] = np.where(df['Polarity'] == 4, 1, 0)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet\n",
              "514293         0  i miss nikki nu nu already  shes always there ...\n",
              "142282         0  So I had a dream last night. I  remember a sig...\n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...\n",
              "649503         0                               it is raining again \n",
              "610789         0          @MissKeriBaby wish I was in LA right now "
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BrvtpU7FRvyX",
        "outputId": "9ceca2b9-3e0a-461c-e40e-db8c69781835"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "source": [
        "df['Polarity'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    800000\n",
              "Name: Polarity, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "metadata": {
        "id": "2JvYcGmUn3mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040d7b37-c0cb-4e2c-add7-72c2d63ec733"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "nhQDTGn9TBCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing Text"
      ],
      "metadata": {
        "id": "ZB2-ZFyun3mF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "source": [
        "def clean_text(tweet):\n",
        "    tweet = tweet.lower()                                   # Converting to lower case\n",
        "    tweet = re.sub(r'\\b\\w+@[^\\s]+', ' <MAIL> ', tweet)      # Removing email IDs\n",
        "    tweet = re.sub(r'@[^\\s]+', ' <MENTION> ', tweet)        # Removing mentions\n",
        "    tweet = re.sub(r'https?:\\/[^\\s]+', ' <URL> ', tweet)    # Removing URLs\n",
        "    tweet = re.sub(r'www.[^\\s]+', ' <WEBSITE> ', tweet)     # Removing Websites\n",
        "    tweet = re.sub(r'\\b\\w+\\.com', ' <WEBSITE> ', tweet)      # Removing email IDs\n",
        "    tweet = re.sub(r'#', ' <HASHTAG> ', tweet)              # Removing hashtags\n",
        "    tweet = re.sub(r'_', ' ', tweet)                        # Sometimes hashtags are done with _ representing break between two words\n",
        "    tweet = re.sub(r'\\.{2,}', ' ', tweet)                   # Removing sentence separators\n",
        "    tweet = re.sub(r\"[0-9]+\",' ', tweet)                    # Removing numbers as they do not indicate sentiment\n",
        "    tweet = re.sub(r\"\\bamp\\b\", ' ', tweet)                  # Removing &amp signs mis-translated\n",
        "    tweet = re.sub(r\"\\bquot\\b\", ' ', tweet)                 # Removing &quot signs mis-translated\n",
        "    tweet = re.sub(r\"\\b\\w+;[^\\s]+\\b\", ' ', tweet)\n",
        "    if len(tweet) == 0:\n",
        "      tweet = 'None'\n",
        "    return ' '.join(tweet.split())"
      ],
      "outputs": [],
      "metadata": {
        "id": "9eOthrAcn3mF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "source": [
        "clean_text(\"I can't find it\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i can't find it\""
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "source": [
        "def remove_punc(tweet):\n",
        "    tweet = re.sub(r\"[^\\w'\\s]+\",' ', tweet)                  # Removing punctuations apart from clitic\n",
        "    return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "8fjrceg8SIep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "source": [
        "remove_punc(\"I can't find it\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can't find it\""
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "source": [
        "clean_text(\"I am &amp rachit1jain@gmail.com n't #doing_exceptionally good hello.com &gt;&gt :D\") "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i am & <MAIL> n't <HASHTAG> doing exceptionally good <WEBSITE> & :d\""
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2BSMAXPen3mG",
        "outputId": "d9e72051-280b-4f35-bd9b-0612070ca79c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "source": [
        "def tweet_word_tokenizer(tweet):\n",
        "    # return word_tokenize(tweet)\n",
        "    return tweet.split(' ')"
      ],
      "outputs": [],
      "metadata": {
        "id": "JWiUBKi4n3mH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "source": [
        "# clitics = {\n",
        "#     \"nt\": 'not',\n",
        "#     \"ve\": 'have',\n",
        "#     \"s\": 'is',\n",
        "#     \"m\": 'am',\n",
        "#     \"re\": 'are',\n",
        "#     \"ll\": 'will',\n",
        "#     'd': 'would',\n",
        "#     \"bout\": 'about',\n",
        "#     'didnt': 'did not',\n",
        "#     'havent': 'have not',\n",
        "#     'hasnt': 'has not',\n",
        "#     'wont': 'will not',\n",
        "#     'wouldnt': 'will not',\n",
        "#     'shouldnt': 'should not',\n",
        "# }"
      ],
      "outputs": [],
      "metadata": {
        "id": "1575G2F4n3mH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "source": [
        "# Handling clitics\n",
        "def handle_clitics(tweet):\n",
        "    tweet = re.sub(r\"\\bwon\\'t\\b\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"\\bwont \\b\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"\\bwouldn't\\b\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"\\bwouldnt\\b\", \"would not\", tweet)\n",
        "\n",
        "    tweet = re.sub(r\"\\bcan\\'t\\b\", \"can not\", tweet)\n",
        "    tweet = re.sub(r\"\\bcant\\b\", \"can not\", tweet)\n",
        "\n",
        "    tweet = re.sub(r\"\\bdon't\\b\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"\\bdont\\b\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"\\bdoesn't\\b\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"\\bdoesnt\\b\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"\\bdidn't\\b\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"\\bdidnt\\b\", \"did not\", tweet)\n",
        "\n",
        "    tweet = re.sub(r\"\\bhasn't\\b\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"\\bhasnt\\b\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"\\bhaven't\\b\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"\\bhavent\\b\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"\\bhadn't\\b\", \"had not\", tweet)\n",
        "    tweet = re.sub(r\"\\bhadnt\\b\", \"had not\", tweet)\n",
        "\n",
        "    tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
        "    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
        "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
        "    tweet = re.sub(r\"\\'s\", \" is\", tweet)\n",
        "    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
        "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
        "    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
        "    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
        "    \n",
        "    return ' '.join(tweet.split())"
      ],
      "outputs": [],
      "metadata": {
        "id": "uBJFKCpfIGc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "source": [
        "a = handle_clitics(\"doesnt feel good.Also I can't find it good\")\n",
        "remove_punc(a)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'does not feel good Also I can not find it good'"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "BIwuwU5nn3mH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "source": [
        "short_forms = {\n",
        "    'n': 'and',\n",
        "    'nu': 'not',\n",
        "    'no': 'not',\n",
        "    'ya': 'you',\n",
        "    'luv': 'love',\n",
        "    'lol': 'laugh',\n",
        "    'k': 'okay',\n",
        "    'na': 'not',\n",
        "    'ily': 'love',\n",
        "    'im': 'am',\n",
        "    'morn': 'morning',\n",
        "    'nght': 'night',\n",
        "    'no': 'not',\n",
        "    'Ill': 'will',\n",
        "    'shoulda': 'should have',\n",
        "    'thnks': 'thanks',\n",
        "    'ty': 'thanks'\n",
        "    }"
      ],
      "outputs": [],
      "metadata": {
        "id": "rzlKv6Tqn3mI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "source": [
        "def handle_shortforms(tweet):\n",
        "    temp = ''\n",
        "    for word in tweet.split():\n",
        "        if word in short_forms.keys():\n",
        "            temp = temp + ' ' + short_forms[word]\n",
        "        else:\n",
        "            temp = temp + ' ' + word\n",
        "    return ' '.join(temp.split())"
      ],
      "outputs": [],
      "metadata": {
        "id": "XmHgKMHcn3mI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "source": [
        "handle_shortforms('I am a good boy shoulda gone')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am a good boy should have gone'"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "UVwFK1SwLLGn",
        "outputId": "8c9c3cad-6410-4858-a357-531e6893ced2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "04Jmy6C2TbQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "source": [
        "## Maintaining only letters within a tweet and removing every other information since not indicative of sentiment\n",
        "def maintain_letters(tweet):\n",
        "    tweet = re.sub(r'[^a-zA-Z]', ' ', tweet) \n",
        "    return ' '.join(tweet.split())"
      ],
      "outputs": [],
      "metadata": {
        "id": "xhLV9cvETbKJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "source": [
        "maintain_letters('i am a good boy. hero is @terohja 909')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i am a good boy hero is terohja'"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8PJMwCsuRipT",
        "outputId": "7d7e98cb-544c-49ce-d892-cef3c29705d4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "8IS3yJElT8sO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "source": [
        "# # Emoticons store a lot of information\n",
        "# def emoticon_translation(tweet):\n",
        "#     tweet = re.sub(r\":\\)\", \" happy \", tweet)\n",
        "#     tweet = re.sub(r\":-\\)\", \" happy \", tweet)\n",
        "#     tweet = re.sub(r\";\\)\", \" happy \", tweet)\n",
        "#     tweet = re.sub(r\":d\", \" laaugh \", tweet)            # 'aa' in laugh to specify that emoji has a certain significance; also used later in extended versions of ha and lol\n",
        "#     tweet = re.sub(r\";d\", \" laaugh \", tweet)\n",
        "#     tweet = re.sub(r\"xd\", \" laaugh \", tweet)\n",
        "#     tweet = re.sub(r\"<3\", \" love \", tweet)\n",
        "\n",
        "#     tweet = re.sub(r\":\\(\", \" sad \", tweet)\n",
        "#     tweet = re.sub(r\":-\\(\", \" sad \", tweet)\n",
        "#     tweet = re.sub(r\":/\", \" sad \", tweet)\n",
        "#     tweet = re.sub(r\":\\\\\", \" sad \", tweet)\n",
        "#     tweet = re.sub(r\":o\", \" surprise \", tweet)\n",
        "\n",
        "#     return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "BzAZlswVRikR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "source": [
        "# Emoticons store a lot of information\n",
        "def emoticon_translation(tweet):\n",
        "    tweet = re.sub(r\":\\)\", \" happy \", tweet)\n",
        "    tweet = re.sub(r\":-\\)\", \" happy \", tweet)\n",
        "    tweet = re.sub(r\";\\)\", \" happy \", tweet)\n",
        "    tweet = re.sub(r\":d\", \" laaugh \", tweet)            # 'aa' in laugh to specify that emoji has a certain significance; also used later in extended versions of ha and lol\n",
        "    tweet = re.sub(r\";d\", \" laaugh \", tweet)\n",
        "    tweet = re.sub(r\"xd\", \" laaugh \", tweet)\n",
        "    tweet = re.sub(r\"<3\", \" love \", tweet)\n",
        "\n",
        "    tweet = re.sub(r\":\\(\", \" sad \", tweet)\n",
        "    tweet = re.sub(r\":-\\(\", \" sad \", tweet)\n",
        "    tweet = re.sub(r\":/\", \" sad \", tweet)\n",
        "    tweet = re.sub(r\":\\\\\", \" sad \", tweet)\n",
        "    tweet = re.sub(r\":o\", \" sad \", tweet)           # Suprised might not be the correct representation\n",
        "\n",
        "    return tweet"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "source": [
        "# ### TO BE MODIFIED ####\n",
        "# emoticons_rank = [k for (k_len, k) in reversed(sorted([(len(k), k) for k in emoticons.keys()]))]"
      ],
      "outputs": [],
      "metadata": {
        "id": "KZnTnjYIRifJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "source": [
        "# ### TO BE MODIFIED ####\n",
        "# def emoticons_translation(phrase):\n",
        "#     for k in emo_info_order:\n",
        "#         phrase = phrase.replace(k, emo_info[k])\n",
        "#     return phrase"
      ],
      "outputs": [],
      "metadata": {
        "id": "efqoiUqgRiZ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "FGR_onqjRmrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "source": [
        "df['Tweet_regex'] = df['Tweet'].apply(clean_text)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \n",
              "514293  i miss nikki nu nu already shes always there w...  \n",
              "142282  so i had a dream last night. i remember a sign...  \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...  \n",
              "649503                                it is raining again  \n",
              "610789               <MENTION> wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "metadata": {
        "id": "M4BzEcEsn3mI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bd97d010-ca05-4002-ddf8-468af390073c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "source": [
        "# # EDA\n",
        "# df[df['Polarity'] == 0]['Tweet_regex'].str.contains('<MENTION>').value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdC3YLqLALfj",
        "outputId": "e8a3bb95-84f0-4199-f858-555d7396f581"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "source": [
        "# df[df['Polarity'] == 4]['Tweet_regex'].str.contains('<MENTION>').value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tc-dv3mALYG",
        "outputId": "5fd54710-5ded-41e0-f209-1e9e6e997fa6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "source": [
        "# # EDA\n",
        "# df[df['Polarity'] == 0]['Tweet_regex'].str.contains('<WEBSITE>').value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUbl4XrSALQY",
        "outputId": "a4ca6722-8dc3-4866-ffdf-198b8970ecd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "source": [
        "# # EDA\n",
        "# df[df['Polarity'] == 4]['Tweet_regex'].str.contains('<WEBSITE>').value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGKXhexIALJH",
        "outputId": "59493790-1e1c-4574-c56c-b5fc13302db2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "7yVFDX2yA-1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "source": [
        "df['Tweet_emoji'] = df['Tweet_regex'].apply(emoticon_translation)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \n",
              "514293  i miss nikki nu nu already shes always there w...  \n",
              "142282  so i had a dream last night. i remember a sign...  \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...  \n",
              "649503                                it is raining again  \n",
              "610789               <MENTION> wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uQmC6MnKSU4z",
        "outputId": "82c24d24-2743-41d9-bfbf-b4cb92544df5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "source": [
        "df['Tweet_nopunc'] = df['Tweet_emoji'].apply(remove_punc)\n",
        "# df['Tweet_nopunc'] = df['Tweet_regex'].apply(remove_punc)         # NOT USING EMOJI\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \n",
              "514293  i miss nikki nu nu already shes always there w...  \n",
              "142282  so i had a dream last night  i remember a sign...  \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...  \n",
              "649503                                it is raining again  \n",
              "610789                MENTION  wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Gm3kWdMAS7NF",
        "outputId": "dc92f3c3-fa3b-4d55-a8fa-c7fa4553e5ea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "source": [
        "df['Tweet_clitics'] = df['Tweet_nopunc'].apply(handle_clitics)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \n",
              "514293  i miss nikki nu nu already shes always there w...  \n",
              "142282  so i had a dream last night i remember a sign ...  \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...  \n",
              "649503                                it is raining again  \n",
              "610789                 MENTION wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "g8qHipQmJTN-",
        "outputId": "35a43faa-78cf-4d79-b87a-9f0fa69deb40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "source": [
        "df['Tweet_shortforms'] = df['Tweet_clitics'].apply(handle_shortforms)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \n",
              "514293  i miss nikki not not already shes always there...  \n",
              "142282  so i had a dream last night i remember a sign ...  \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...  \n",
              "649503                                it is raining again  \n",
              "610789                 MENTION wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "metadata": {
        "id": "gz_VX3C8n3mJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "34ece64b-9790-49c9-dbf9-2084cb98cfd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "source": [
        "df['Tweet_pure_string'] = df['Tweet_shortforms'].apply(maintain_letters)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \n",
              "514293  i miss nikki not not already shes always there...  \n",
              "142282  so i had a dream last night i remember a sign ...  \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...  \n",
              "649503                                it is raining again  \n",
              "610789                 MENTION wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "80twWXsWTYSc",
        "outputId": "e0d5bc0e-4751-4fbb-d137-ece146abcfcb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "source": [
        "df['Tweet_token'] = df['Tweet_pure_string'].apply(tweet_word_tokenizer)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...  \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...  \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...  \n",
              "649503                           [it, is, raining, again]  \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]  "
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "jIce-XrfJzM3",
        "outputId": "e90a1f7c-b9ca-4d58-a6b8-f3f74cc2f168"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "source": [
        "# # Was absolutely useless to use\n",
        "# stem = 0\n",
        "# def stemmer(tweet):\n",
        "#     global stem\n",
        "#     stem += 1\n",
        "#     if stem % 1000:\n",
        "#       print(stem)\n",
        "#     porter_stemmer = PorterStemmer()\n",
        "#     tweet = [porter_stemmer.stem(word) for word in tweet]\n",
        "#     return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "cn8KcPyQn3mK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "source": [
        "# stemmer(['I','am','playing','making', 'what','I','do'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "nqp-VCQin3mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57078e7d-20a6-44e6-c1c5-4a711fd24005"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "source": [
        "# df['Tweet_stem'] = df['Tweet_token'].apply(stemmer)\n",
        "# df.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "FOOQ4c5Mn3mK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "rb1Gs1MXh9fl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "source": [
        "# def pos_tagger(nltk_tag):\n",
        "#     if nltk_tag.startswith('J'):\n",
        "#         return wordnet.ADJ\n",
        "#     elif nltk_tag.startswith('V'):\n",
        "#         return wordnet.VERB\n",
        "#     elif nltk_tag.startswith('N'):\n",
        "#         return wordnet.NOUN\n",
        "#     elif nltk_tag.startswith('R'):\n",
        "#         return wordnet.ADV\n",
        "#     else:         \n",
        "#         return None"
      ],
      "outputs": [],
      "metadata": {
        "id": "Y_eo4YzXn3mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "source": [
        "# #### TO BE MODIFIED ######\n",
        "# count = 0\n",
        "# def pos_tagging(tweet):\n",
        "#     global count\n",
        "#     count += 1\n",
        "#     if count % 100 == 0:\n",
        "#       print(count)\n",
        "#     # tweet = nltk.pos_tag(tweet) \n",
        "#     tweet = nltk.pos_tag([i for i in tweet if i])\n",
        "#     return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "zzpRmJQvn3mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "RBHCcvl6kLQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "source": [
        "# pos_tagging(['','I','am','good'])"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBiJMHxPOVd",
        "outputId": "1d360348-1df3-407b-9f15-24d8fcb20b78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "source": [
        "# ### TO BE MODIFIED ################\n",
        "# def tweet_lemmatizer(tweet):\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmatized = []\n",
        "#     pos_wordnet = list(map(lambda x: (x[0], pos_tagger(x[1])), tweet))\n",
        "#     for word, tag in pos_wordnet:\n",
        "#         if tag is None:\n",
        "#             lemmatized.append(word)\n",
        "#         else:       \n",
        "#             lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
        "#     lemmatized = ' '.join(lemmatized)\n",
        "#     lemmatized_sent = ', '.join(lemmatized)\n",
        "#     # print(list(lemmatized.split()))\n",
        "#     return list(lemmatized.split())"
      ],
      "outputs": [],
      "metadata": {
        "id": "-TPMJd4On3mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "6IYdhTT6n3mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "source": [
        "# df['Tweet_pos'] = df['Tweet_token'].apply(pos_tagging)\n",
        "# df.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "eriBp7Gln3mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ce09c5c-d06e-49f8-c27f-d9f132b789e6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "source": [
        "# df['Tweet_pos'] = df['Tweet_token'].apply(pos_tagging)\n",
        "# df.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "7tmYuK9li481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e21e6be7-66c6-482a-db4d-6b238d50ce4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "source": [
        "# df['Tweet_lemma'] = df['Tweet_pos'].apply(tweet_lemmatizer)\n",
        "# df.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "nITQsdJ0n3mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "d6028300-1f71-40bf-d16b-f1e1361cf7a8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "source": [
        "def sentence_creator(df, col_name, title):\n",
        "    df[title] = df[col_name].apply(lambda x:' '.join([i for i in x]))\n",
        "    return df"
      ],
      "outputs": [],
      "metadata": {
        "id": "zhL62ykXI1x8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "source": [
        "df = sentence_creator(df, 'Tweet_token', 'Tweet_sent')\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                               Tweet_sent  \n",
              "514293  i miss nikki not not already shes always there...  \n",
              "142282  so i had a dream last night i remember a sign ...  \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...  \n",
              "649503                                it is raining again  \n",
              "610789                 MENTION wish i was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "metadata": {
        "id": "cxPIAu_vn3mN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "8604e6bc-74e0-4788-999c-03430f7c0d5c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "source": [
        "# wordnet.synsets('hello')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mkfs5-v5WG1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84207548-1a18-4c07-ddad-f44a6aec1ee4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "source": [
        "## TO BE MODIFIED #####\n",
        "def normalisation_words(tweet):\n",
        "    tweet = tweet.replace(r'([a-z])\\1{1,}', r'\\1\\1')\n",
        "    # tweet = re.sub(r'(ha){1,}', r'laugh', tweet)\n",
        "    tweet = re.sub(r'h+a+[ha]+\\b', r'laaugh', tweet)     # To give more significance\n",
        "    # tweet = re.sub(r'(lol){1,}', r'laugh', tweet)\n",
        "    tweet = re.sub(r'l+o+[lo]+\\b', r'laaugh', tweet)\n",
        "\n",
        "    tweet = re.sub(r'\\b([a-z])\\1{1,}', r' ', tweet)     # If only repeated letters are left, remove them\n",
        "    tweet = re.sub(r'\\b([a-z])\\1{1,}', r' ', tweet)     # If only repeated letters are left, remove them\n",
        "    tweet = re.sub(r'([a-z])\\1{2,}', r'\\1\\1', tweet)     # If only repeated letters are left, remove them\n",
        "    tweet = re.sub(r'([a-z])\\1{1,}\\b', r'\\1', tweet)     # If only repeated letters are left, remove them\n",
        "\n",
        "    # tweet = ' '.join([word if len(wordnet.synsets(word)) > 0 else re.sub(r'([a-z])\\1{1,}', r'\\1\\1', word) for word in tweet.split()])\n",
        "\n",
        "    tweet = re.sub(r\"\\b[a-zA-Z]{1}\\b\", ' ', tweet)        # Removing single letters\n",
        "    # tweet = re.sub(r'lo+l+o+[^\\s]+', r'lol', tweet)\n",
        "    return tweet.split()"
      ],
      "outputs": [],
      "metadata": {
        "id": "cBOLGluLW0rs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "source": [
        "normalisation_words('i dont feel llllooooooooooolllll ')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dont', 'feel', 'laaugh']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "source": [
        "normalisation_words('i awww hahahhhahahahahahahhahaaaaaaaaaaahhhahaha ppeeee lollipop looooooool happppiest day lolllll lll lool bummer get david carr third day laugh')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aw',\n",
              " 'laaugh',\n",
              " 'lollipop',\n",
              " 'laaugh',\n",
              " 'happiest',\n",
              " 'day',\n",
              " 'laaugh',\n",
              " 'laaugh',\n",
              " 'bummer',\n",
              " 'get',\n",
              " 'david',\n",
              " 'car',\n",
              " 'third',\n",
              " 'day',\n",
              " 'laugh']"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "metadata": {
        "id": "4APiy1icXJ3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b6585c-2c4a-4340-9b8c-4b38585f91c6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "source": [
        "df['Tweet_normalised'] = df['Tweet_sent'].apply(normalisation_words)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_normalised  \n",
              "514293  [mis, nikki, not, not, already, shes, always, ...  \n",
              "142282  [so, had, dream, last, night, remember, sign, ...  \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...  \n",
              "649503                           [it, is, raining, again]  \n",
              "610789           [MENTION, wish, was, in, la, right, now]  "
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "metadata": {
        "id": "3TG5vlkdWCAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "207eb227-3ec7-400b-95f6-2dd7dcac0295"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "source": [
        "df = sentence_creator(df, 'Tweet_normalised', 'Tweet_sent_normal')\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_normalised  \\\n",
              "514293  [mis, nikki, not, not, already, shes, always, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                        Tweet_sent_normal  \n",
              "514293  mis nikki not not already shes always there wh...  \n",
              "142282  so had dream last night remember sign which cl...  \n",
              "403727  MENTION oh poor sickly you hugs hope you feel ...  \n",
              "649503                                it is raining again  \n",
              "610789                   MENTION wish was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "source": [
        "def add_negation(tweet):\n",
        "    flag = False\n",
        "    count = 0\n",
        "    tweet_new = []\n",
        "\n",
        "    for word in tweet:\n",
        "        if flag and count < 2:          # Adding prefix to next three words\n",
        "            word = 'NEG_' + word\n",
        "            count += 1\n",
        "        elif word in ['not', 'cannot', 'cant']:     # Others must have already been processed\n",
        "            flag = True\n",
        "        tweet_new.append(word)\n",
        "    return tweet_new"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "source": [
        "add_negation(['I','not','good','but','bad','hehe'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'not', 'NEG_good', 'NEG_but', 'bad', 'hehe']"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "source": [
        "df['Tweet_normal_negated'] = df['Tweet_normalised'].apply(add_negation)\n",
        "df.tail()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36489</th>\n",
              "      <td>1</td>\n",
              "      <td>@beckybootsx i hope your not drinking alcohol!...</td>\n",
              "      <td>&lt;MENTION&gt; i hope your not drinking alcohol! lol</td>\n",
              "      <td>&lt;MENTION&gt; i hope your not drinking alcohol! lol</td>\n",
              "      <td>MENTION  i hope your not drinking alcohol  lol</td>\n",
              "      <td>MENTION i hope your not drinking alcohol lol</td>\n",
              "      <td>MENTION i hope your not drinking alcohol laugh</td>\n",
              "      <td>MENTION i hope your not drinking alcohol laugh</td>\n",
              "      <td>[MENTION, i, hope, your, not, drinking, alcoho...</td>\n",
              "      <td>MENTION i hope your not drinking alcohol laugh</td>\n",
              "      <td>[MENTION, hope, your, not, drinking, alcohol, ...</td>\n",
              "      <td>MENTION hope your not drinking alcohol laugh</td>\n",
              "      <td>[MENTION, hope, your, not, NEG_drinking, NEG_a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491263</th>\n",
              "      <td>0</td>\n",
              "      <td>That's it!! I can't take it no more!! After su...</td>\n",
              "      <td>that's it!! i can't take it no more!! after su...</td>\n",
              "      <td>that's it!! i can't take it no more!! after su...</td>\n",
              "      <td>that's it  i can't take it no more  after summ...</td>\n",
              "      <td>that is it i can not take it no more after sum...</td>\n",
              "      <td>that is it i can not take it not more after su...</td>\n",
              "      <td>that is it i can not take it not more after su...</td>\n",
              "      <td>[that, is, it, i, can, not, take, it, not, mor...</td>\n",
              "      <td>that is it i can not take it not more after su...</td>\n",
              "      <td>[that, is, it, can, not, take, it, not, more, ...</td>\n",
              "      <td>that is it can not take it not more after summ...</td>\n",
              "      <td>[that, is, it, can, not, NEG_take, NEG_it, not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470924</th>\n",
              "      <td>0</td>\n",
              "      <td>@JonasAustralia  i wanted to win! congrats to ...</td>\n",
              "      <td>&lt;MENTION&gt; i wanted to win! congrats to her any...</td>\n",
              "      <td>&lt;MENTION&gt; i wanted to win! congrats to her any...</td>\n",
              "      <td>MENTION  i wanted to win  congrats to her any...</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>[MENTION, i, wanted, to, win, congrats, to, he...</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>[MENTION, wanted, to, win, congrats, to, her, ...</td>\n",
              "      <td>MENTION wanted to win congrats to her anyways</td>\n",
              "      <td>[MENTION, wanted, to, win, congrats, to, her, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491755</th>\n",
              "      <td>0</td>\n",
              "      <td>Trying to amuse my cousin. It's not working! a...</td>\n",
              "      <td>trying to amuse my cousin. it's not working! a...</td>\n",
              "      <td>trying to amuse my cousin. it's not working! a...</td>\n",
              "      <td>trying to amuse my cousin  it's not working  a...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, w...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, w...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128037</th>\n",
              "      <td>0</td>\n",
              "      <td>Oh really don't wanna be awake</td>\n",
              "      <td>oh really don't wanna be awake</td>\n",
              "      <td>oh really don't wanna be awake</td>\n",
              "      <td>oh really don't wanna be awake</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>[oh, really, do, not, wanna, be, awake]</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>[oh, really, do, not, wanna, be, awake]</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>[oh, really, do, not, NEG_wanna, NEG_be, awake]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "36489          1  @beckybootsx i hope your not drinking alcohol!...   \n",
              "491263         0  That's it!! I can't take it no more!! After su...   \n",
              "470924         0  @JonasAustralia  i wanted to win! congrats to ...   \n",
              "491755         0  Trying to amuse my cousin. It's not working! a...   \n",
              "128037         0                    Oh really don't wanna be awake    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "36489     <MENTION> i hope your not drinking alcohol! lol   \n",
              "491263  that's it!! i can't take it no more!! after su...   \n",
              "470924  <MENTION> i wanted to win! congrats to her any...   \n",
              "491755  trying to amuse my cousin. it's not working! a...   \n",
              "128037                     oh really don't wanna be awake   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "36489     <MENTION> i hope your not drinking alcohol! lol   \n",
              "491263  that's it!! i can't take it no more!! after su...   \n",
              "470924  <MENTION> i wanted to win! congrats to her any...   \n",
              "491755  trying to amuse my cousin. it's not working! a...   \n",
              "128037                     oh really don't wanna be awake   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "36489      MENTION  i hope your not drinking alcohol  lol   \n",
              "491263  that's it  i can't take it no more  after summ...   \n",
              "470924   MENTION  i wanted to win  congrats to her any...   \n",
              "491755  trying to amuse my cousin  it's not working  a...   \n",
              "128037                     oh really don't wanna be awake   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "36489        MENTION i hope your not drinking alcohol lol   \n",
              "491263  that is it i can not take it no more after sum...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "36489      MENTION i hope your not drinking alcohol laugh   \n",
              "491263  that is it i can not take it not more after su...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "36489      MENTION i hope your not drinking alcohol laugh   \n",
              "491263  that is it i can not take it not more after su...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "36489   [MENTION, i, hope, your, not, drinking, alcoho...   \n",
              "491263  [that, is, it, i, can, not, take, it, not, mor...   \n",
              "470924  [MENTION, i, wanted, to, win, congrats, to, he...   \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, w...   \n",
              "128037            [oh, really, do, not, wanna, be, awake]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "36489      MENTION i hope your not drinking alcohol laugh   \n",
              "491263  that is it i can not take it not more after su...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                         Tweet_normalised  \\\n",
              "36489   [MENTION, hope, your, not, drinking, alcohol, ...   \n",
              "491263  [that, is, it, can, not, take, it, not, more, ...   \n",
              "470924  [MENTION, wanted, to, win, congrats, to, her, ...   \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, w...   \n",
              "128037            [oh, really, do, not, wanna, be, awake]   \n",
              "\n",
              "                                        Tweet_sent_normal  \\\n",
              "36489        MENTION hope your not drinking alcohol laugh   \n",
              "491263  that is it can not take it not more after summ...   \n",
              "470924      MENTION wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                     Tweet_normal_negated  \n",
              "36489   [MENTION, hope, your, not, NEG_drinking, NEG_a...  \n",
              "491263  [that, is, it, can, not, NEG_take, NEG_it, not...  \n",
              "470924  [MENTION, wanted, to, win, congrats, to, her, ...  \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, N...  \n",
              "128037    [oh, really, do, not, NEG_wanna, NEG_be, awake]  "
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "source": [
        "normalised_words = df.Tweet_sent_normal.str.split(expand=True).stack().value_counts()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "source": [
        "more_stop_words = normalised_words.iloc[np.where(normalised_words < 2)].index.tolist()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "source": [
        "stop = 0\n",
        "def stopword_removal(tweet):\n",
        "    # stoplist = stopwords.words('english')\n",
        "    # manual_stoplist = ['i', 'nu', 'it', 'u', 'you', 'tweet', 'dm', 'gm', 'gn', 'day', 'and', 'or', 'go', 'get', 'give']\n",
        "    # stoplist.extend(manual_stoplist)\n",
        "    # stoplist.remove('not')      # This is an important word\n",
        "    # stoplist.remove('against')\n",
        "    # # stoplist.extend(more_stop_words)\n",
        "    \n",
        "    # global stop\n",
        "    # stop += 1\n",
        "    # if stop % 100 == 0:\n",
        "    #   print(stop)\n",
        "    # # stopwords = stopwords.words('english')\n",
        "    # tweet = [word for word in tweet if word not in stoplist]\n",
        "\n",
        "    # # if len(tweet) == 0:\n",
        "    # #     tweet = ['None']\n",
        "    return tweet"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "source": [
        "df['Tweet_stopword'] = df['Tweet_normal_negated'].apply(stopword_removal)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_normalised  \\\n",
              "514293  [mis, nikki, not, not, already, shes, always, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                        Tweet_sent_normal  \\\n",
              "514293  mis nikki not not already shes always there wh...   \n",
              "142282  so had dream last night remember sign which cl...   \n",
              "403727  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "649503                                it is raining again   \n",
              "610789                   MENTION wish was in la right now   \n",
              "\n",
              "                                     Tweet_normal_negated  \\\n",
              "514293  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                           Tweet_stopword  \n",
              "514293  [mis, nikki, not, NEG_not, NEG_already, shes, ...  \n",
              "142282  [so, had, dream, last, night, remember, sign, ...  \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...  \n",
              "649503                           [it, is, raining, again]  \n",
              "610789           [MENTION, wish, was, in, la, right, now]  "
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "metadata": {
        "id": "IxYHa-0un3mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b15e96fe-41b3-408e-8cda-f06e9261480a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "source": [
        "len(np.where(df.Tweet_stopword.apply(len) >= 30)[0])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "787"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "source": [
        "df.iloc[29231]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Polarity                                                                1\n",
              "Tweet                   #seb-day #seb-day #seb-day #seb-day #seb-day #...\n",
              "Tweet_regex             <HASHTAG> seb-day <HASHTAG> seb-day <HASHTAG> ...\n",
              "Tweet_emoji             <HASHTAG> seb-day <HASHTAG> seb-day <HASHTAG> ...\n",
              "Tweet_nopunc             HASHTAG  seb day  HASHTAG  seb day  HASHTAG  ...\n",
              "Tweet_clitics           HASHTAG seb day HASHTAG seb day HASHTAG seb da...\n",
              "Tweet_shortforms        HASHTAG seb day HASHTAG seb day HASHTAG seb da...\n",
              "Tweet_pure_string       HASHTAG seb day HASHTAG seb day HASHTAG seb da...\n",
              "Tweet_token             [HASHTAG, seb, day, HASHTAG, seb, day, HASHTAG...\n",
              "Tweet_sent              HASHTAG seb day HASHTAG seb day HASHTAG seb da...\n",
              "Tweet_normalised        [HASHTAG, seb, day, HASHTAG, seb, day, HASHTAG...\n",
              "Tweet_sent_normal       HASHTAG seb day HASHTAG seb day HASHTAG seb da...\n",
              "Tweet_normal_negated    [HASHTAG, seb, day, HASHTAG, seb, day, HASHTAG...\n",
              "Tweet_stopword          [HASHTAG, seb, day, HASHTAG, seb, day, HASHTAG...\n",
              "Name: 620747, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "source": [
        "df = sentence_creator(df, 'Tweet_stopword', 'Tweet_final_sent')\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_normalised  \\\n",
              "514293  [mis, nikki, not, not, already, shes, always, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                        Tweet_sent_normal  \\\n",
              "514293  mis nikki not not already shes always there wh...   \n",
              "142282  so had dream last night remember sign which cl...   \n",
              "403727  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "649503                                it is raining again   \n",
              "610789                   MENTION wish was in la right now   \n",
              "\n",
              "                                     Tweet_normal_negated  \\\n",
              "514293  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                           Tweet_stopword  \\\n",
              "514293  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                         Tweet_final_sent  \n",
              "514293  mis nikki not NEG_not NEG_already shes always ...  \n",
              "142282  so had dream last night remember sign which cl...  \n",
              "403727  MENTION oh poor sickly you hugs hope you feel ...  \n",
              "649503                                it is raining again  \n",
              "610789                   MENTION wish was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "metadata": {
        "id": "mUlsuWqAZPXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "a0f414cc-c886-4425-c810-a49ec34bf56c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "DGZZH19KWw7X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "source": [
        "# df.drop(df[df[\"Tweet_final_sent\"] == ''].index, inplace=True)\n",
        "# df = df.reset_index(drop=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MJMDNZfdZPQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "source": [
        "df.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "514293         0  i miss nikki nu nu already  shes always there ...   \n",
              "142282         0  So I had a dream last night. I  remember a sig...   \n",
              "403727         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "649503         0                               it is raining again    \n",
              "610789         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night. i remember a sign...   \n",
              "403727  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "649503                                it is raining again   \n",
              "610789               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night  i remember a sign...   \n",
              "403727   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "649503                                it is raining again   \n",
              "610789                MENTION  wish i was in la right now   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "514293  i miss nikki nu nu already shes always there w...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "514293  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "142282  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "403727  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "514293  i miss nikki not not already shes always there...   \n",
              "142282  so i had a dream last night i remember a sign ...   \n",
              "403727  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "649503                                it is raining again   \n",
              "610789                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_normalised  \\\n",
              "514293  [mis, nikki, not, not, already, shes, always, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                        Tweet_sent_normal  \\\n",
              "514293  mis nikki not not already shes always there wh...   \n",
              "142282  so had dream last night remember sign which cl...   \n",
              "403727  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "649503                                it is raining again   \n",
              "610789                   MENTION wish was in la right now   \n",
              "\n",
              "                                     Tweet_normal_negated  \\\n",
              "514293  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                           Tweet_stopword  \\\n",
              "514293  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "142282  [so, had, dream, last, night, remember, sign, ...   \n",
              "403727  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "649503                           [it, is, raining, again]   \n",
              "610789           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                         Tweet_final_sent  \n",
              "514293  mis nikki not NEG_not NEG_already shes always ...  \n",
              "142282  so had dream last night remember sign which cl...  \n",
              "403727  MENTION oh poor sickly you hugs hope you feel ...  \n",
              "649503                                it is raining again  \n",
              "610789                   MENTION wish was in la right now  "
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "metadata": {
        "id": "UhBbxLJqZPKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "6cd76a4d-c70f-4df5-9777-e5947d617b90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "source": [
        "df.tail()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36489</th>\n",
              "      <td>1</td>\n",
              "      <td>@beckybootsx i hope your not drinking alcohol!...</td>\n",
              "      <td>&lt;MENTION&gt; i hope your not drinking alcohol! lol</td>\n",
              "      <td>&lt;MENTION&gt; i hope your not drinking alcohol! lol</td>\n",
              "      <td>MENTION  i hope your not drinking alcohol  lol</td>\n",
              "      <td>MENTION i hope your not drinking alcohol lol</td>\n",
              "      <td>MENTION i hope your not drinking alcohol laugh</td>\n",
              "      <td>MENTION i hope your not drinking alcohol laugh</td>\n",
              "      <td>[MENTION, i, hope, your, not, drinking, alcoho...</td>\n",
              "      <td>MENTION i hope your not drinking alcohol laugh</td>\n",
              "      <td>[MENTION, hope, your, not, drinking, alcohol, ...</td>\n",
              "      <td>MENTION hope your not drinking alcohol laugh</td>\n",
              "      <td>[MENTION, hope, your, not, NEG_drinking, NEG_a...</td>\n",
              "      <td>[MENTION, hope, your, not, NEG_drinking, NEG_a...</td>\n",
              "      <td>MENTION hope your not NEG_drinking NEG_alcohol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491263</th>\n",
              "      <td>0</td>\n",
              "      <td>That's it!! I can't take it no more!! After su...</td>\n",
              "      <td>that's it!! i can't take it no more!! after su...</td>\n",
              "      <td>that's it!! i can't take it no more!! after su...</td>\n",
              "      <td>that's it  i can't take it no more  after summ...</td>\n",
              "      <td>that is it i can not take it no more after sum...</td>\n",
              "      <td>that is it i can not take it not more after su...</td>\n",
              "      <td>that is it i can not take it not more after su...</td>\n",
              "      <td>[that, is, it, i, can, not, take, it, not, mor...</td>\n",
              "      <td>that is it i can not take it not more after su...</td>\n",
              "      <td>[that, is, it, can, not, take, it, not, more, ...</td>\n",
              "      <td>that is it can not take it not more after summ...</td>\n",
              "      <td>[that, is, it, can, not, NEG_take, NEG_it, not...</td>\n",
              "      <td>[that, is, it, can, not, NEG_take, NEG_it, not...</td>\n",
              "      <td>that is it can not NEG_take NEG_it not more af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470924</th>\n",
              "      <td>0</td>\n",
              "      <td>@JonasAustralia  i wanted to win! congrats to ...</td>\n",
              "      <td>&lt;MENTION&gt; i wanted to win! congrats to her any...</td>\n",
              "      <td>&lt;MENTION&gt; i wanted to win! congrats to her any...</td>\n",
              "      <td>MENTION  i wanted to win  congrats to her any...</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>[MENTION, i, wanted, to, win, congrats, to, he...</td>\n",
              "      <td>MENTION i wanted to win congrats to her anyways</td>\n",
              "      <td>[MENTION, wanted, to, win, congrats, to, her, ...</td>\n",
              "      <td>MENTION wanted to win congrats to her anyways</td>\n",
              "      <td>[MENTION, wanted, to, win, congrats, to, her, ...</td>\n",
              "      <td>[MENTION, wanted, to, win, congrats, to, her, ...</td>\n",
              "      <td>MENTION wanted to win congrats to her anyways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491755</th>\n",
              "      <td>0</td>\n",
              "      <td>Trying to amuse my cousin. It's not working! a...</td>\n",
              "      <td>trying to amuse my cousin. it's not working! a...</td>\n",
              "      <td>trying to amuse my cousin. it's not working! a...</td>\n",
              "      <td>trying to amuse my cousin  it's not working  a...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, w...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, w...</td>\n",
              "      <td>trying to amuse my cousin it is not working an...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, N...</td>\n",
              "      <td>[trying, to, amuse, my, cousin, it, is, not, N...</td>\n",
              "      <td>trying to amuse my cousin it is not NEG_workin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128037</th>\n",
              "      <td>0</td>\n",
              "      <td>Oh really don't wanna be awake</td>\n",
              "      <td>oh really don't wanna be awake</td>\n",
              "      <td>oh really don't wanna be awake</td>\n",
              "      <td>oh really don't wanna be awake</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>[oh, really, do, not, wanna, be, awake]</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>[oh, really, do, not, wanna, be, awake]</td>\n",
              "      <td>oh really do not wanna be awake</td>\n",
              "      <td>[oh, really, do, not, NEG_wanna, NEG_be, awake]</td>\n",
              "      <td>[oh, really, do, not, NEG_wanna, NEG_be, awake]</td>\n",
              "      <td>oh really do not NEG_wanna NEG_be awake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity                                              Tweet  \\\n",
              "36489          1  @beckybootsx i hope your not drinking alcohol!...   \n",
              "491263         0  That's it!! I can't take it no more!! After su...   \n",
              "470924         0  @JonasAustralia  i wanted to win! congrats to ...   \n",
              "491755         0  Trying to amuse my cousin. It's not working! a...   \n",
              "128037         0                    Oh really don't wanna be awake    \n",
              "\n",
              "                                              Tweet_regex  \\\n",
              "36489     <MENTION> i hope your not drinking alcohol! lol   \n",
              "491263  that's it!! i can't take it no more!! after su...   \n",
              "470924  <MENTION> i wanted to win! congrats to her any...   \n",
              "491755  trying to amuse my cousin. it's not working! a...   \n",
              "128037                     oh really don't wanna be awake   \n",
              "\n",
              "                                              Tweet_emoji  \\\n",
              "36489     <MENTION> i hope your not drinking alcohol! lol   \n",
              "491263  that's it!! i can't take it no more!! after su...   \n",
              "470924  <MENTION> i wanted to win! congrats to her any...   \n",
              "491755  trying to amuse my cousin. it's not working! a...   \n",
              "128037                     oh really don't wanna be awake   \n",
              "\n",
              "                                             Tweet_nopunc  \\\n",
              "36489      MENTION  i hope your not drinking alcohol  lol   \n",
              "491263  that's it  i can't take it no more  after summ...   \n",
              "470924   MENTION  i wanted to win  congrats to her any...   \n",
              "491755  trying to amuse my cousin  it's not working  a...   \n",
              "128037                     oh really don't wanna be awake   \n",
              "\n",
              "                                            Tweet_clitics  \\\n",
              "36489        MENTION i hope your not drinking alcohol lol   \n",
              "491263  that is it i can not take it no more after sum...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                         Tweet_shortforms  \\\n",
              "36489      MENTION i hope your not drinking alcohol laugh   \n",
              "491263  that is it i can not take it not more after su...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                        Tweet_pure_string  \\\n",
              "36489      MENTION i hope your not drinking alcohol laugh   \n",
              "491263  that is it i can not take it not more after su...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                              Tweet_token  \\\n",
              "36489   [MENTION, i, hope, your, not, drinking, alcoho...   \n",
              "491263  [that, is, it, i, can, not, take, it, not, mor...   \n",
              "470924  [MENTION, i, wanted, to, win, congrats, to, he...   \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, w...   \n",
              "128037            [oh, really, do, not, wanna, be, awake]   \n",
              "\n",
              "                                               Tweet_sent  \\\n",
              "36489      MENTION i hope your not drinking alcohol laugh   \n",
              "491263  that is it i can not take it not more after su...   \n",
              "470924    MENTION i wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                         Tweet_normalised  \\\n",
              "36489   [MENTION, hope, your, not, drinking, alcohol, ...   \n",
              "491263  [that, is, it, can, not, take, it, not, more, ...   \n",
              "470924  [MENTION, wanted, to, win, congrats, to, her, ...   \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, w...   \n",
              "128037            [oh, really, do, not, wanna, be, awake]   \n",
              "\n",
              "                                        Tweet_sent_normal  \\\n",
              "36489        MENTION hope your not drinking alcohol laugh   \n",
              "491263  that is it can not take it not more after summ...   \n",
              "470924      MENTION wanted to win congrats to her anyways   \n",
              "491755  trying to amuse my cousin it is not working an...   \n",
              "128037                    oh really do not wanna be awake   \n",
              "\n",
              "                                     Tweet_normal_negated  \\\n",
              "36489   [MENTION, hope, your, not, NEG_drinking, NEG_a...   \n",
              "491263  [that, is, it, can, not, NEG_take, NEG_it, not...   \n",
              "470924  [MENTION, wanted, to, win, congrats, to, her, ...   \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, N...   \n",
              "128037    [oh, really, do, not, NEG_wanna, NEG_be, awake]   \n",
              "\n",
              "                                           Tweet_stopword  \\\n",
              "36489   [MENTION, hope, your, not, NEG_drinking, NEG_a...   \n",
              "491263  [that, is, it, can, not, NEG_take, NEG_it, not...   \n",
              "470924  [MENTION, wanted, to, win, congrats, to, her, ...   \n",
              "491755  [trying, to, amuse, my, cousin, it, is, not, N...   \n",
              "128037    [oh, really, do, not, NEG_wanna, NEG_be, awake]   \n",
              "\n",
              "                                         Tweet_final_sent  \n",
              "36489   MENTION hope your not NEG_drinking NEG_alcohol...  \n",
              "491263  that is it can not NEG_take NEG_it not more af...  \n",
              "470924      MENTION wanted to win congrats to her anyways  \n",
              "491755  trying to amuse my cousin it is not NEG_workin...  \n",
              "128037            oh really do not NEG_wanna NEG_be awake  "
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ],
      "metadata": {
        "id": "JsjTY7SkykwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "avKai0rAZ24R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "source": [
        "# Think if you want to do stratify\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Tweet_final_sent'], df['Polarity'], stratify=df['Polarity'], test_size=0.1, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "g7ijUT5lZ2y-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(ngram_range=(1,3), min_df=5)\n",
        "# # ling_stats = LinguisticVectorizer()\n",
        "# # all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "# clf = MultinomialNB(alpha=5)\n",
        "\n",
        "# pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)\n",
        "# y_pred_nb = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_nb))\n",
        "# sum(y_pred_nb == y_test)/len(y_test)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3), min_df=1)\n",
        "# # ling_stats = LinguisticVectorizer()\n",
        "# # all_features = FeatureUnion([('tfidf', tfidf_ngrams)])\n",
        "\n",
        "# clf = LogisticRegression(penalty='l2',\n",
        "#                          solver='saga',\n",
        "#                          multi_class='multinomial',\n",
        "#                          tol=1e-5,\n",
        "#                          n_jobs = -1)\n",
        "\n",
        "# pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)\n",
        "# y_pred_lr = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "# sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "source": [
        "X_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1440000,)"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "source": [
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', ngram_range=(1,2))),\n",
        "                 ('model', LogisticRegression(penalty='l2', max_iter = 200))])\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred_lr = pipe.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/rachitjain/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.8254590481168294\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82468125"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "source": [
        "y_pred_lr = pipe.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.8254590481168294\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82468125"
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "source": [
        "y_pred_lr.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160000,)"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "source": [
        "coefs = pd.DataFrame(pipe['model'].coef_, \n",
        "                     columns=pipe['vectoriser'].get_feature_names())\n",
        "coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')\n",
        "coefs"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mis</th>\n",
              "      <td>-15.570541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <td>-14.613006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadly</th>\n",
              "      <td>-14.273278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rip</th>\n",
              "      <td>-13.396289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>poor</th>\n",
              "      <td>-12.888051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smile</th>\n",
              "      <td>8.946712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neg problem</th>\n",
              "      <td>9.657908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thanks</th>\n",
              "      <td>10.127874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neg bad</th>\n",
              "      <td>10.741498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neg wait</th>\n",
              "      <td>14.396946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3186867 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  coef\n",
              "mis         -15.570541\n",
              "sad         -14.613006\n",
              "sadly       -14.273278\n",
              "rip         -13.396289\n",
              "poor        -12.888051\n",
              "...                ...\n",
              "smile         8.946712\n",
              "neg problem   9.657908\n",
              "thanks       10.127874\n",
              "neg bad      10.741498\n",
              "neg wait     14.396946\n",
              "\n",
              "[3186867 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "source": [
        "print(coefs.loc['bad'])\n",
        "print(coefs.loc['good'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coef   -8.875133\n",
            "Name: bad, dtype: float64\n",
            "coef    7.475829\n",
            "Name: good, dtype: float64\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "source": [
        "coef_pos_set = set(coefs.iloc[np.where(coefs['coef'] > 7)].index.tolist())        # 'good' was missing if put 1\n",
        "coef_neg_set = set(coefs.iloc[np.where(coefs['coef'] < -7)].index.tolist())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "source": [
        "# coef_verypos_set = set(coefs.iloc[np.where(coefs['coef'] > 4)].index.tolist())        # 'good' was missing if put 1\n",
        "# coef_veryneg_set = set(coefs.iloc[np.where(coefs['coef'] < -4)].index.tolist())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "source": [
        "# 'good' in coef_verypos_set"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "source": [
        "# 'not wait' in coef_pos_set"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "source": [
        "def words_freq(tweet):\n",
        "\n",
        "  bigram_list = list(ngrams(tweet, 2))\n",
        "  bigram_string_list = []\n",
        "\n",
        "  for bigram in bigram_list:\n",
        "      bigram_string = bigram[0] + ' ' + bigram[1]\n",
        "      bigram_string_list.append(bigram_string)\n",
        "\n",
        "#   bigram_string_list = list(ngrams(tweet, 1))\n",
        "\n",
        "  # num_pos = len(set(tweet).intersection(coef_pos_set))\n",
        "  # num_neg = len(set(tweet).intersection(coef_neg_set))\n",
        "\n",
        "  num_pos = len(set(tweet).intersection(coef_pos_set)) + len(set(bigram_string_list).intersection(coef_pos_set))\n",
        "  num_neg = len(set(tweet).intersection(coef_neg_set)) + len(set(bigram_string_list).intersection(coef_neg_set))\n",
        "  \n",
        "#   print(set(bigram_string_list).intersection(coef_pos_set))\n",
        "#   print(set(tweet).intersection(coef_pos_set))\n",
        "\n",
        "  # # If there exist positive words in the tweet\n",
        "  # if num_pos:\n",
        "  #     for _ in range(num_pos):\n",
        "  #         tweet.append('POSITIVE')\n",
        "  # # if num_verypos:\n",
        "  # #     for num in range(num_verypos):\n",
        "  # #         tweet.append('POOSITIVE')\n",
        "  # if num_neg:\n",
        "  #     for _ in range(num_neg):\n",
        "  #         tweet.append('NEGATIVE')\n",
        "  # # if num_veryneg:\n",
        "  # #     for num in range(num_veryneg):\n",
        "  # #         tweet.append('NEEGATIVE')\n",
        "\n",
        "  if num_pos > num_neg:\n",
        "    return 1\n",
        "  elif num_neg > num_pos:\n",
        "    return 0\n",
        "  else:\n",
        "    return 0.5"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "source": [
        "def senti_lexicon(tweet):\n",
        "  num_pos = len(set(tweet).intersection(coef_pos_set))\n",
        "  num_neg = len(set(tweet).intersection(coef_neg_set))\n",
        "\n",
        "  if num_pos > num_neg:\n",
        "    return 1\n",
        "  elif num_pos < num_neg:\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "source": [
        "# X_train_token = X_train.apply(tweet_word_tokenizer)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "source": [
        "X_test_token = X_test.apply(tweet_word_tokenizer)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "source": [
        "# X_train_token"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "source": [
        "y_pred_lexi = X_test_token.apply(senti_lexicon)\n",
        "y_pred_lexi.value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    85715\n",
              " 0    57137\n",
              " 1    17148\n",
              "Name: Tweet_final_sent, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "source": [
        "y_pred_lr = pd.Series(y_pred_lr, index=X_test.index)\n",
        "y_pred_lr = np.where(y_pred_lr == 1.0, 1, 0)\n",
        "y_pred_lr = pd.Series(y_pred_lr, index=X_test.index)\n",
        "y_pred_lr.value_counts()\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    80713\n",
              "0    79287\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "source": [
        "assert(y_pred_lexi.shape == y_pred_lr.shape)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "source": [
        "hehe = 0\n",
        "for i in range(len(y_pred_lr)):\n",
        "    if y_pred_lexi.iloc[i] != y_pred_lr.iloc[i]:\n",
        "        hehe += 1\n",
        "print(hehe)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95776\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "source": [
        "y_pred_lexi.iloc[3]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "source": [
        "y_pred_lr_modified = y_pred_lr.copy()\n",
        "\n",
        "for i in range(len(y_pred_lexi)):\n",
        "    if y_pred_lexi.iloc[i] == 1:\n",
        "        y_pred_lr_modified.iloc[i] = 1\n",
        "        # if y_pred_lr.iloc[i] != y_pred_lexi.iloc[i]:\n",
        "            # print('True')\n",
        "    elif y_pred_lexi.iloc[i] == 0:\n",
        "        # if y_pred_lr.iloc[i] != y_pred_lexi.iloc[i]:\n",
        "            # print('True')\n",
        "        y_pred_lr_modified.iloc[i] = 0"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "source": [
        "y_pred_lr.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160000,)"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "source": [
        "y_pred_lr_modified.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160000,)"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "source": [
        "(y_pred_lr_modified == y_pred_lr).sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149939"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "source": [
        "print('F1 Score: ', f1_score(y_test, y_pred_lr_modified))\n",
        "sum(y_pred_lr_modified == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.7908989720421659\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8004"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 571,
      "source": [
        "words_freq(['I','am','a','good','boy', 'cant', 'wait'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 571
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 573,
      "source": [
        "# set(['I','am','a','good','boy', 'cant', 'wait']).intersection(coef_neg_set)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 575,
      "source": [
        "df['Tweet_lexicons_polarity'] = df['Tweet_stopword'].apply(words_freq)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "      <th>Tweet_lexicons_freq</th>\n",
              "      <th>Tweet_lexicons_polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>it is raining again POSITIVE NEGATIVE NEGATIVE...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now NEGATIVE NEGA...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity                                              Tweet  \\\n",
              "0         0  i miss nikki nu nu already  shes always there ...   \n",
              "1         0  So I had a dream last night. I  remember a sig...   \n",
              "2         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "3         0                               it is raining again    \n",
              "4         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                         Tweet_regex  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night. i remember a sign...   \n",
              "2  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "3                                it is raining again   \n",
              "4               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                         Tweet_emoji  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night. i remember a sign...   \n",
              "2  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "3                                it is raining again   \n",
              "4               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                        Tweet_nopunc  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night  i remember a sign...   \n",
              "2   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "3                                it is raining again   \n",
              "4                MENTION  wish i was in la right now   \n",
              "\n",
              "                                       Tweet_clitics  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                    Tweet_shortforms  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                   Tweet_pure_string  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_token  \\\n",
              "0  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "1  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "2  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "3                           [it, is, raining, again]   \n",
              "4        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                          Tweet_sent  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                    Tweet_normalised  \\\n",
              "0  [mis, nikki, not, not, already, shes, always, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3                           [it, is, raining, again]   \n",
              "4           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                   Tweet_sent_normal  \\\n",
              "0  mis nikki not not already shes always there wh...   \n",
              "1  so had dream last night remember sign which cl...   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "3                                it is raining again   \n",
              "4                   MENTION wish was in la right now   \n",
              "\n",
              "                                Tweet_normal_negated  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                                      Tweet_stopword  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                                    Tweet_final_sent  \\\n",
              "0  mis nikki not NEG_not NEG_already shes always ...   \n",
              "1  so had dream last night remember sign which cl...   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "3                                it is raining again   \n",
              "4                   MENTION wish was in la right now   \n",
              "\n",
              "                                      Tweet_lexicons  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                           Tweet_final_sent_lexicons  Tweet_lexicons_freq  \\\n",
              "0  mis nikki not NEG_not NEG_already shes always ...                  0.0   \n",
              "1  so had dream last night remember sign which cl...                  0.0   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...                  0.0   \n",
              "3  it is raining again POSITIVE NEGATIVE NEGATIVE...                  0.0   \n",
              "4  MENTION wish was in la right now NEGATIVE NEGA...                  0.0   \n",
              "\n",
              "   Tweet_lexicons_polarity  \n",
              "0                      0.0  \n",
              "1                      0.0  \n",
              "2                      0.0  \n",
              "3                      0.0  \n",
              "4                      0.0  "
            ]
          },
          "metadata": {},
          "execution_count": 575
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 562,
      "source": [
        "# df['Tweet_lexicons'] = df['Tweet_stopword'].apply(words_freq)\n",
        "# df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>it is raining again POSITIVE NEGATIVE NEGATIVE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now NEGATIVE NEGA...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity                                              Tweet  \\\n",
              "0         0  i miss nikki nu nu already  shes always there ...   \n",
              "1         0  So I had a dream last night. I  remember a sig...   \n",
              "2         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "3         0                               it is raining again    \n",
              "4         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                         Tweet_regex  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night. i remember a sign...   \n",
              "2  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "3                                it is raining again   \n",
              "4               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                         Tweet_emoji  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night. i remember a sign...   \n",
              "2  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "3                                it is raining again   \n",
              "4               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                        Tweet_nopunc  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night  i remember a sign...   \n",
              "2   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "3                                it is raining again   \n",
              "4                MENTION  wish i was in la right now   \n",
              "\n",
              "                                       Tweet_clitics  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                    Tweet_shortforms  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                   Tweet_pure_string  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_token  \\\n",
              "0  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "1  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "2  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "3                           [it, is, raining, again]   \n",
              "4        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                          Tweet_sent  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                    Tweet_normalised  \\\n",
              "0  [mis, nikki, not, not, already, shes, always, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3                           [it, is, raining, again]   \n",
              "4           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                   Tweet_sent_normal  \\\n",
              "0  mis nikki not not already shes always there wh...   \n",
              "1  so had dream last night remember sign which cl...   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "3                                it is raining again   \n",
              "4                   MENTION wish was in la right now   \n",
              "\n",
              "                                Tweet_normal_negated  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                                      Tweet_stopword  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                                    Tweet_final_sent  \\\n",
              "0  mis nikki not NEG_not NEG_already shes always ...   \n",
              "1  so had dream last night remember sign which cl...   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "3                                it is raining again   \n",
              "4                   MENTION wish was in la right now   \n",
              "\n",
              "                                      Tweet_lexicons  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                           Tweet_final_sent_lexicons  \n",
              "0  mis nikki not NEG_not NEG_already shes always ...  \n",
              "1  so had dream last night remember sign which cl...  \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...  \n",
              "3  it is raining again POSITIVE NEGATIVE NEGATIVE...  \n",
              "4  MENTION wish was in la right now NEGATIVE NEGA...  "
            ]
          },
          "metadata": {},
          "execution_count": 562
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# df = sentence_creator(df, 'Tweet_lexicons', 'Tweet_final_sent_lexicons')\n",
        "# df.head()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 576,
      "source": [
        "df = sentence_creator(df, 'Tweet_lexicons', 'Tweet_final_sent_lexicons')\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_sent_normal</th>\n",
              "      <th>Tweet_normal_negated</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "      <th>Tweet_lexicons_freq</th>\n",
              "      <th>Tweet_lexicons_polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[i, miss, nikki, not, not, already, shes, alwa...</td>\n",
              "      <td>i miss nikki not not already shes always there...</td>\n",
              "      <td>[mis, nikki, not, not, already, shes, always, ...</td>\n",
              "      <td>mis nikki not not already shes always there wh...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "      <td>[mis, nikki, not, NEG_not, NEG_already, shes, ...</td>\n",
              "      <td>mis nikki not NEG_not NEG_already shes always ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night  i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION  ohh poor sickly you  hugs  hope you ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>[MENTION, oh, poor, sickly, you, hugs, hope, y...</td>\n",
              "      <td>MENTION oh poor sickly you hugs hope you feel ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again, POSITIVE, NEGATIVE, N...</td>\n",
              "      <td>it is raining again POSITIVE NEGATIVE NEGATIVE...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION  wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now NEGATIVE NEGA...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity                                              Tweet  \\\n",
              "0         0  i miss nikki nu nu already  shes always there ...   \n",
              "1         0  So I had a dream last night. I  remember a sig...   \n",
              "2         0  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
              "3         0                               it is raining again    \n",
              "4         0          @MissKeriBaby wish I was in LA right now    \n",
              "\n",
              "                                         Tweet_regex  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night. i remember a sign...   \n",
              "2  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "3                                it is raining again   \n",
              "4               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                         Tweet_emoji  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night. i remember a sign...   \n",
              "2  <MENTION> ohh poor sickly you (((hugs)) hope y...   \n",
              "3                                it is raining again   \n",
              "4               <MENTION> wish i was in la right now   \n",
              "\n",
              "                                        Tweet_nopunc  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night  i remember a sign...   \n",
              "2   MENTION  ohh poor sickly you  hugs  hope you ...   \n",
              "3                                it is raining again   \n",
              "4                MENTION  wish i was in la right now   \n",
              "\n",
              "                                       Tweet_clitics  \\\n",
              "0  i miss nikki nu nu already shes always there w...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                    Tweet_shortforms  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                   Tweet_pure_string  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                         Tweet_token  \\\n",
              "0  [i, miss, nikki, not, not, already, shes, alwa...   \n",
              "1  [so, i, had, a, dream, last, night, i, remembe...   \n",
              "2  [MENTION, ohh, poor, sickly, you, hugs, hope, ...   \n",
              "3                           [it, is, raining, again]   \n",
              "4        [MENTION, wish, i, was, in, la, right, now]   \n",
              "\n",
              "                                          Tweet_sent  \\\n",
              "0  i miss nikki not not already shes always there...   \n",
              "1  so i had a dream last night i remember a sign ...   \n",
              "2  MENTION ohh poor sickly you hugs hope you feel...   \n",
              "3                                it is raining again   \n",
              "4                 MENTION wish i was in la right now   \n",
              "\n",
              "                                    Tweet_normalised  \\\n",
              "0  [mis, nikki, not, not, already, shes, always, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3                           [it, is, raining, again]   \n",
              "4           [MENTION, wish, was, in, la, right, now]   \n",
              "\n",
              "                                   Tweet_sent_normal  \\\n",
              "0  mis nikki not not already shes always there wh...   \n",
              "1  so had dream last night remember sign which cl...   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "3                                it is raining again   \n",
              "4                   MENTION wish was in la right now   \n",
              "\n",
              "                                Tweet_normal_negated  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                                      Tweet_stopword  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                                    Tweet_final_sent  \\\n",
              "0  mis nikki not NEG_not NEG_already shes always ...   \n",
              "1  so had dream last night remember sign which cl...   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...   \n",
              "3                                it is raining again   \n",
              "4                   MENTION wish was in la right now   \n",
              "\n",
              "                                      Tweet_lexicons  \\\n",
              "0  [mis, nikki, not, NEG_not, NEG_already, shes, ...   \n",
              "1  [so, had, dream, last, night, remember, sign, ...   \n",
              "2  [MENTION, oh, poor, sickly, you, hugs, hope, y...   \n",
              "3  [it, is, raining, again, POSITIVE, NEGATIVE, N...   \n",
              "4  [MENTION, wish, was, in, la, right, now, NEGAT...   \n",
              "\n",
              "                           Tweet_final_sent_lexicons  Tweet_lexicons_freq  \\\n",
              "0  mis nikki not NEG_not NEG_already shes always ...                  0.0   \n",
              "1  so had dream last night remember sign which cl...                  0.0   \n",
              "2  MENTION oh poor sickly you hugs hope you feel ...                  0.0   \n",
              "3  it is raining again POSITIVE NEGATIVE NEGATIVE...                  0.0   \n",
              "4  MENTION wish was in la right now NEGATIVE NEGA...                  0.0   \n",
              "\n",
              "   Tweet_lexicons_polarity  \n",
              "0                      0.0  \n",
              "1                      0.0  \n",
              "2                      0.0  \n",
              "3                      0.0  \n",
              "4                      0.0  "
            ]
          },
          "metadata": {},
          "execution_count": 576
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 578,
      "source": [
        "# # Think if you want to do stratify\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df[['Tweet_final_sent','Tweet_lexicons_polarity']], df['Polarity'], stratify=df['Polarity'], test_size=0.1, random_state=2)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 564,
      "source": [
        "# Think if you want to do stratify\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['Tweet_final_sent_lexicons']], df['Polarity'], stratify=df['Polarity'], test_size=0.1, random_state=2)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 581,
      "source": [
        "X_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1439991, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 581
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 582,
      "source": [
        "y_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1439991,)"
            ]
          },
          "metadata": {},
          "execution_count": 582
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 580,
      "source": [
        "pipe = Pipeline([('vectoriser', CountVectorizer(token_pattern=r'[a-z]+', ngram_range=(1,2))),\n",
        "                 ('model', LogisticRegression(penalty='l2'))])\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred_lr = pipe.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [2, 1439991]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-580-9311bcc4f130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m pipe = Pipeline([('vectoriser', CountVectorizer(token_pattern=r'[a-z]+', ngram_range=(1,2))),\n\u001b[1;32m      2\u001b[0m                  ('model', LogisticRegression(penalty='l2'))])\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                                    accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1347\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 320\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 1439991]"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', ngram_range=(1,2))),\n",
        "#                  ('model', LogisticRegression(penalty='l2', solver='saga', max_iter=1000, tol=1e-5, n_jobs=-1))])\n",
        "# pipe.fit(X_train, y_train)\n",
        "# y_pred_lr = pipe.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "# sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "source": [
        "# coefs = pd.DataFrame(pipe['model'].coef_, \n",
        "#                      columns=pipe['vectoriser'].get_feature_names())\n",
        "# coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')\n",
        "# coefs"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "source": [
        "coefs.loc['good']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "coef    3.072135\n",
              "Name: good, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 538
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "\n",
        "# clf = LogisticRegression(penalty='l2',\n",
        "#                          solver='saga',\n",
        "#                          multi_class='multinomial',\n",
        "#                          tol=1e-5,\n",
        "#                          n_jobs = -1,\n",
        "#                          max_iter = 100)\n",
        "\n",
        "# pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)\n",
        "# y_pred_lr = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "# sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 566,
      "source": [
        "tfidf_ngrams = CountVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "\n",
        "clf = LogisticRegression(penalty='l1',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1,\n",
        "                         max_iter = 1000)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/rachitjain/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.8147003518701247\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8133800836255226"
            ]
          },
          "metadata": {},
          "execution_count": 566
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 567,
      "source": [
        "tfidf_ngrams = CountVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "\n",
        "clf = LogisticRegression(penalty='l1',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1,\n",
        "                         max_iter = 100)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-567-2b13800feda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1414\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1416\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "source": [
        "df_curt = df[df['Tweet'].str.contains('good')][['Polarity', 'Tweet', 'Tweet_final_sent_lexicons']]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "source": [
        "# y_pred_curt = y_pred_lr[df['Tweet'].str.contains('good').index]"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 514293 is out of bounds for axis 0 with size 16000",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-00aa17d488dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_curt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'good'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 514293 is out of bounds for axis 0 with size 16000"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# assert(y_pred_curt.shape[0] == df_curt.shape[0])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "source": [
        "# df_curt.append(y_pred_curt)\n",
        "df_curt.to_csv('Data/pos.csv')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_ngrams = CountVectorizer(ngram_range=(1,3), min_df=5)\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "clf = MultinomialNB(alpha=0.6)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_nb = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_nb))\n",
        "sum(y_pred_nb == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.7334122169253183\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7562078584921003"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "source": [
        "LogisticRegression.get_params().keys()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute '_get_param_names'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-195-7086f5c91e7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[1;32m    193\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_get_param_names'"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "source": [
        "# # Create a pipeline\n",
        "# pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+')),\n",
        "#                  ('model', LogisticRegression())])\n",
        "# # Prepare a random search\n",
        "# param_distributions = {'vectoriser__min_df': np.arange(10, 1000, 10),\n",
        "#                        'vectoriser__max_df': np.linspace(.2, 1, 40),\n",
        "#                        'model__penalty': ['l2']}\n",
        "                       \n",
        "# r_search = RandomizedSearchCV(estimator=pipe, param_distributions=param_distributions, \n",
        "#                               n_iter=100, cv=5, n_jobs=4, random_state=123)\n",
        "# r_search.fit(X_train, y_train)\n",
        "# # Save results to a dataframe\n",
        "# r_search_results = pd.DataFrame(r_search.cv_results_).sort_values(by='rank_test_score')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-201-d59c6b0bb81c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m r_search = RandomizedSearchCV(estimator=pipe, param_distributions=param_distributions, \n\u001b[1;32m     10\u001b[0m                               n_iter=100, cv=5, n_jobs=4, random_state=123)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mr_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save results to a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mr_search_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rank_test_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[1;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/col772_a2/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "columns = [col for col in r_search_results.columns \n",
        "           if re.search(r\"split|param_\", col)]\n",
        "r_summary = r_search_results[columns].copy()\n",
        "r_summary.columns = [re.sub(r'_test_score|param_', '', col) \n",
        "                     for col in r_summary.columns]\n",
        "columns = [col.split('__')[1] if '__' in col else col \n",
        "           for col in r_summary.columns ]\n",
        "r_summary.columns = columns\n",
        "r_summary.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>min_df</th>\n",
              "      <th>max_df</th>\n",
              "      <th>penalty</th>\n",
              "      <th>split0</th>\n",
              "      <th>split1</th>\n",
              "      <th>split2</th>\n",
              "      <th>split3</th>\n",
              "      <th>split4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>50</td>\n",
              "      <td>0.958974</td>\n",
              "      <td>l2</td>\n",
              "      <td>0.757778</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.767986</td>\n",
              "      <td>0.763750</td>\n",
              "      <td>0.766319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>40</td>\n",
              "      <td>0.425641</td>\n",
              "      <td>l2</td>\n",
              "      <td>0.756528</td>\n",
              "      <td>0.759236</td>\n",
              "      <td>0.764514</td>\n",
              "      <td>0.760347</td>\n",
              "      <td>0.762569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>140</td>\n",
              "      <td>0.753846</td>\n",
              "      <td>l2</td>\n",
              "      <td>0.753472</td>\n",
              "      <td>0.755000</td>\n",
              "      <td>0.761875</td>\n",
              "      <td>0.758403</td>\n",
              "      <td>0.759236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>140</td>\n",
              "      <td>0.876923</td>\n",
              "      <td>l2</td>\n",
              "      <td>0.753472</td>\n",
              "      <td>0.755000</td>\n",
              "      <td>0.761875</td>\n",
              "      <td>0.758403</td>\n",
              "      <td>0.759236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>170</td>\n",
              "      <td>0.917949</td>\n",
              "      <td>l2</td>\n",
              "      <td>0.753264</td>\n",
              "      <td>0.754375</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.757986</td>\n",
              "      <td>0.759097</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   min_df    max_df penalty    split0    split1    split2    split3    split4\n",
              "7      50  0.958974      l2  0.757778  0.759375  0.767986  0.763750  0.766319\n",
              "29     40  0.425641      l2  0.756528  0.759236  0.764514  0.760347  0.762569\n",
              "15    140  0.753846      l2  0.753472  0.755000  0.761875  0.758403  0.759236\n",
              "28    140  0.876923      l2  0.753472  0.755000  0.761875  0.758403  0.759236\n",
              "22    170  0.917949      l2  0.753264  0.754375  0.762500  0.757986  0.759097"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "source": [
        "def load_sent_word_net():\n",
        "    sent_scores = collections.defaultdict(list)\n",
        "\n",
        "    with open(\"../content/drive/MyDrive/COL772_A2/SentiWordNet_3.0.0.txt\",\"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
        "\n",
        "        for line in reader:\n",
        "            if line[0].startswith(\"#\"):\n",
        "                continue\n",
        "            if len(line) == 1:\n",
        "                continue\n",
        "            POS, ID, PosScore, NegScore, SynsetTerms, Glos = line\n",
        "            if len(POS) == 0 or len(ID) == 0:\n",
        "                continue\n",
        "            for term in SynsetTerms.split(\" \"):\n",
        "                term = term.split('#')[0]\n",
        "                # print(term)\n",
        "                term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
        "                key = \"%s/%s\" % (POS, term)\n",
        "                # print(key)\n",
        "                sent_scores[key].append((float(PosScore), float(NegScore)))\n",
        "                # print(sent_scores)\n",
        "        for key, value in sent_scores.items():\n",
        "            sent_scores[key] = np.mean(value, axis=0)\n",
        "\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "sent_word_net = load_sent_word_net()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Lk-2ljkObJh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "source": [
        "class LinguisticVectorizer(BaseEstimator):\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return np.array(['sent_pos', 'sent_neg', 'nouns', 'adjectives', 'verbs', 'adverbs'])\n",
        "\n",
        "    def fit(self, documents, y=None):\n",
        "        return self\n",
        "\n",
        "    def _get_sentiments(self, d):\n",
        "        sent = tuple(d.split())\n",
        "        tagged = nltk.pos_tag(sent)\n",
        "\n",
        "        pos_vals = []\n",
        "        neg_vals = []\n",
        "\n",
        "        nouns = 0.\n",
        "        adjectives = 0.\n",
        "        verbs = 0.\n",
        "        adverbs = 0.\n",
        "\n",
        "        i = 0\n",
        "        for w, t in tagged:\n",
        "\n",
        "            p, n = 0, 0\n",
        "            sent_pos_type = None\n",
        "            if t.startswith(\"NN\"):\n",
        "                #noun\n",
        "                sent_pos_type = \"n\"\n",
        "                nouns += 1\n",
        "            elif t.startswith(\"JJ\"):\n",
        "                #adjective\n",
        "                sent_pos_type = \"a\"\n",
        "                adjectives += 1\n",
        "            elif t.startswith(\"VB\"):\n",
        "                #verb\n",
        "                sent_pos_type = \"v\"\n",
        "                verbs += 1\n",
        "            elif t.startswith(\"RB\"):\n",
        "                #adverb\n",
        "                sent_pos_type = \"r\"\n",
        "                adverbs += 1\n",
        "            else:\n",
        "                sent_pos_type = \"Nan\"\n",
        "\n",
        "                i += 1\n",
        "                l = len(sent) - i\n",
        "\n",
        "                if l == 0:\n",
        "                    l = 1\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            if sent_pos_type is not None:\n",
        "\n",
        "                sent_word = \"%s/%s\" % (sent_pos_type, w)\n",
        "\n",
        "                if sent_word in sent_word_net:\n",
        "                    p, n = sent_word_net[sent_word]\n",
        "                elif sent_word == \"Nan\":\n",
        "                    p, n = 0, 0\n",
        "\n",
        "                pos_vals.append(p)\n",
        "                neg_vals.append(n)\n",
        "\n",
        "        if i == 0:\n",
        "            l = len(sent)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        avg_pos_val = np.mean(pos_vals)\n",
        "        avg_neg_val = np.mean(neg_vals)\n",
        "\n",
        "        return [avg_pos_val, avg_neg_val, nouns / l, adjectives / l, verbs / l, adverbs / l]\n",
        "\n",
        "    # print(_get_sentiments('This be fantastic'))\n",
        "\n",
        "    def transform(self, documents):\n",
        "        pos_val, neg_val, nouns, adjectives, verbs, adverbs = np.array([self._get_sentiments(d) for d in documents]).T\n",
        "        result = np.array([pos_val, neg_val, nouns, adjectives, verbs, adverbs]).T\n",
        "\n",
        "        return result"
      ],
      "outputs": [],
      "metadata": {
        "id": "3p2X3-DiZ2YP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "-fmvMI_1Z2Sn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "source": [
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV"
      ],
      "outputs": [],
      "metadata": {
        "id": "FGyQiQdZVK4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "source": [
        "# Define functions\n",
        "def create_baseline_models():\n",
        "    \"\"\"Create list of baseline models.\"\"\"\n",
        "    models = []\n",
        "    models.append(('log', LogisticRegression(random_state=123, \n",
        "                                             max_iter=1000)))\n",
        "    models.append(('sgd', SGDClassifier(random_state=123)))\n",
        "    models.append(('mnb', MultinomialNB()))\n",
        "    return models\n",
        "\n",
        "def assess(X, y, models, cv=5, scoring=['roc_auc', \n",
        "                                        'accuracy', \n",
        "                                        'f1']):\n",
        "    \"\"\"Provide summary of cross validation results for models.\"\"\"\n",
        "    results = pd.DataFrame()\n",
        "    for name, model in models:\n",
        "        result = pd.DataFrame(cross_validate(model, X, y, cv=cv, \n",
        "                                             scoring=scoring))\n",
        "        mean = result.mean().rename('{}_mean'.format)\n",
        "        std = result.std().rename('{}_std'.format)\n",
        "        results[name] = pd.concat([mean, std], axis=0)\n",
        "    return results.sort_index()"
      ],
      "outputs": [],
      "metadata": {
        "id": "BRCvgmF8VKuF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "source": [
        "models = create_baseline_models()\n",
        "models"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('log',\n",
              "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                     intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                     multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                     random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                     warm_start=False)),\n",
              " ('sgd', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "                l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
              "                max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "                power_t=0.5, random_state=123, shuffle=True, tol=0.001,\n",
              "                validation_fraction=0.1, verbose=0, warm_start=False)),\n",
              " ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzRWuIs3W0x0",
        "outputId": "043dbb56-bb4b-4d22-c8ec-34e5fc651ffe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "source": [
        "# Preprocess the data\n",
        "vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', \n",
        "                             stop_words='english', \n",
        "                             min_df=30, \n",
        "                             max_df=.7)\n",
        "X_train_simpler = vectoriser.fit_transform(X_train)\n",
        "# Assess the model\n",
        "assess(X_train_simpler, y_train, models)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>log</th>\n",
              "      <th>sgd</th>\n",
              "      <th>mnb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fit_time_mean</th>\n",
              "      <td>0.948748</td>\n",
              "      <td>0.125900</td>\n",
              "      <td>0.022557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fit_time_std</th>\n",
              "      <td>0.114444</td>\n",
              "      <td>0.005250</td>\n",
              "      <td>0.003963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>score_time_mean</th>\n",
              "      <td>0.016027</td>\n",
              "      <td>0.016216</td>\n",
              "      <td>0.022632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>score_time_std</th>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.004119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_accuracy_mean</th>\n",
              "      <td>0.746097</td>\n",
              "      <td>0.746458</td>\n",
              "      <td>0.739278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_accuracy_std</th>\n",
              "      <td>0.003333</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.002977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_f1_mean</th>\n",
              "      <td>0.752671</td>\n",
              "      <td>0.757212</td>\n",
              "      <td>0.744791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_f1_std</th>\n",
              "      <td>0.002869</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.001948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_roc_auc_mean</th>\n",
              "      <td>0.827126</td>\n",
              "      <td>0.824481</td>\n",
              "      <td>0.820055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_roc_auc_std</th>\n",
              "      <td>0.003550</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.004196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         log       sgd       mnb\n",
              "fit_time_mean       0.948748  0.125900  0.022557\n",
              "fit_time_std        0.114444  0.005250  0.003963\n",
              "score_time_mean     0.016027  0.016216  0.022632\n",
              "score_time_std      0.000369  0.000349  0.004119\n",
              "test_accuracy_mean  0.746097  0.746458  0.739278\n",
              "test_accuracy_std   0.003333  0.002156  0.002977\n",
              "test_f1_mean        0.752671  0.757212  0.744791\n",
              "test_f1_std         0.002869  0.001636  0.001948\n",
              "test_roc_auc_mean   0.827126  0.824481  0.820055\n",
              "test_roc_auc_std    0.003550  0.003068  0.004196"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "D9oWz7bzVKlT",
        "outputId": "1576ed66-6df0-4fb9-8bce-97dbe56d1440"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "source": [
        "# Create a pipeline\n",
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+')),\n",
        "                 ('model', SGDClassifier(random_state=123))])\n",
        "# Prepare a random search\n",
        "param_distributions = {'vectoriser__min_df': np.arange(10, 1000, 10),\n",
        "                       'vectoriser__max_df': np.linspace(.2, 1, 40),\n",
        "                       'model__loss': ['log', 'hinge']}\n",
        "r_search = RandomizedSearchCV(estimator=pipe, param_distributions=param_distributions, \n",
        "                              n_iter=30, cv=5, n_jobs=-1, random_state=123)\n",
        "r_search.fit(X_train, y_train)\n",
        "# Save results to a dataframe\n",
        "r_search_results = pd.DataFrame(r_search.cv_results_).sort_values(by='rank_test_score')"
      ],
      "outputs": [],
      "metadata": {
        "id": "PqnWNoenVKcA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "source": [
        "columns = [col for col in r_search_results.columns \n",
        "           if re.search(r\"split|param_\", col)]\n",
        "r_summary = r_search_results[columns].copy()\n",
        "r_summary.columns = [re.sub(r'_test_score|param_', '', col) \n",
        "                     for col in r_summary.columns]\n",
        "columns = [col.split('__')[1] if '__' in col else col \n",
        "           for col in r_summary.columns ]\n",
        "r_summary.columns = columns\n",
        "r_summary.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>min_df</th>\n",
              "      <th>max_df</th>\n",
              "      <th>loss</th>\n",
              "      <th>split0</th>\n",
              "      <th>split1</th>\n",
              "      <th>split2</th>\n",
              "      <th>split3</th>\n",
              "      <th>split4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20</td>\n",
              "      <td>0.220513</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.768056</td>\n",
              "      <td>0.773611</td>\n",
              "      <td>0.774444</td>\n",
              "      <td>0.773819</td>\n",
              "      <td>0.774167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>0.938462</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.758750</td>\n",
              "      <td>0.761389</td>\n",
              "      <td>0.764236</td>\n",
              "      <td>0.759444</td>\n",
              "      <td>0.759514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100</td>\n",
              "      <td>0.528205</td>\n",
              "      <td>log</td>\n",
              "      <td>0.757431</td>\n",
              "      <td>0.762639</td>\n",
              "      <td>0.765139</td>\n",
              "      <td>0.754792</td>\n",
              "      <td>0.757569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>120</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.750972</td>\n",
              "      <td>0.756319</td>\n",
              "      <td>0.757153</td>\n",
              "      <td>0.754167</td>\n",
              "      <td>0.754306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>140</td>\n",
              "      <td>0.671795</td>\n",
              "      <td>hinge</td>\n",
              "      <td>0.751181</td>\n",
              "      <td>0.756528</td>\n",
              "      <td>0.757014</td>\n",
              "      <td>0.753958</td>\n",
              "      <td>0.754028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   min_df    max_df   loss    split0    split1    split2    split3    split4\n",
              "4      20  0.220513  hinge  0.768056  0.773611  0.774444  0.773819  0.774167\n",
              "1     100  0.938462  hinge  0.758750  0.761389  0.764236  0.759444  0.759514\n",
              "11    100  0.528205    log  0.757431  0.762639  0.765139  0.754792  0.757569\n",
              "18    120  0.241026  hinge  0.750972  0.756319  0.757153  0.754167  0.754306\n",
              "9     140  0.671795  hinge  0.751181  0.756528  0.757014  0.753958  0.754028"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "metadata": {
        "id": "9jw2B2RwVKSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ddfe31b4-8d80-41ee-8ff7-00419e7805a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "source": [
        "# Create a pipeline\n",
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', max_df=.6)),\n",
        "                 ('model', SGDClassifier(random_state=123, loss='hinge'))])\n",
        "# Prepare a grid search\n",
        "param_grid = {'vectoriser__min_df': [30, 90, 150],\n",
        "              'vectoriser__ngram_range': [(1,1), (1,2)],\n",
        "              'vectoriser__stop_words': [None, 'english'],\n",
        "              'model__fit_intercept': [True, False]}\n",
        "g_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "g_search.fit(X_train, y_train)\n",
        "# Save results to a dataframe\n",
        "g_search_results = pd.DataFrame(g_search.cv_results_).sort_values(by='rank_test_score')"
      ],
      "outputs": [],
      "metadata": {
        "id": "jsAUhN4MrDwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "source": [
        "# columns = [col for col in g_search_results.columns \n",
        "#            if re.search(r\"split|param_\", col)]\n",
        "# g_summary = g_search_results[columns+['mean_test_score']].copy()\n",
        "# g_summary.columns = [re.sub(r'_test_score|param_', '', col) \n",
        "#                      for col in g_summary.columns]\n",
        "# columns = [col.split('__')[1] if '__' in col else col \n",
        "#            for col in g_summary.columns ]\n",
        "# g_summary.columns = columns\n",
        "# g_summary.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Sg0MPh1tYtgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [
        "# # Create a long dataframe\n",
        "# g_summary_long = pd.melt(g_summary, \n",
        "#                          id_vars=['min_df', \n",
        "#                                   'ngram_range', \n",
        "#                                   'stop_words', \n",
        "#                                   'fit_intercept'], \n",
        "#                          value_vars=['split0', \n",
        "#                                      'split1', \n",
        "#                                      'split2', \n",
        "#                                      'split3', \n",
        "#                                      'split4'])\n",
        "# g_summary_long.replace({None: 'None'}, inplace=True)\n",
        "# # Plot performance\n",
        "# for param in ['ngram_range', 'stop_words', 'fit_intercept']:\n",
        "#     plt.figure(figsize=(8,4))\n",
        "#     plt.title(f'Performance by {param}')\n",
        "#     sns.boxplot(x='value', y=param, data=g_summary_long, orient='h')\n",
        "#     plt.xlim(.85, .95);"
      ],
      "outputs": [],
      "metadata": {
        "id": "fsRkcgE5YtYn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "D-4OIo_rYtQk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "nFLaH_xGaKaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "source": [
        "pipe = Pipeline([('vectoriser', TfidfVectorizer(token_pattern=r'[a-z]+', min_df=30, max_df=.6, ngram_range=(1,2))),\n",
        "                 ('model', SGDClassifier(random_state=123, loss='hinge'))])\n",
        "pipe.fit(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectoriser',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.6, max_features=None,\n",
              "                                 min_df=30, ngram_range=(1, 2), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False, token_pat...\n",
              "                ('model',\n",
              "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=True, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='hinge',\n",
              "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l2', power_t=0.5, random_state=123,\n",
              "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWnj9zxtZifk",
        "outputId": "952e9274-6169-4b62-b1cc-8ef7d52137ed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "source": [
        "coefs = pd.DataFrame(pipe['model'].coef_, \n",
        "                     columns=pipe['vectoriser'].get_feature_names())\n",
        "coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')\n",
        "coefs"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>not</th>\n",
              "      <td>-5.554187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miss</th>\n",
              "      <td>-5.223724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <td>-5.031330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wish</th>\n",
              "      <td>-3.460169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sorri</th>\n",
              "      <td>-3.267135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>happi</th>\n",
              "      <td>2.223506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>2.342490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>2.426037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thank</th>\n",
              "      <td>2.572014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not wait</th>\n",
              "      <td>3.232475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4900 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              coef\n",
              "not      -5.554187\n",
              "miss     -5.223724\n",
              "sad      -5.031330\n",
              "wish     -3.460169\n",
              "sorri    -3.267135\n",
              "...            ...\n",
              "happi     2.223506\n",
              "love      2.342490\n",
              "good      2.426037\n",
              "thank     2.572014\n",
              "not wait  3.232475\n",
              "\n",
              "[4900 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CHCSjZZbZiXQ",
        "outputId": "58c87f28-a724-44a9-a476-92fbc7b231ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "source": [
        "coef_pos_set = set(coefs.iloc[np.where(coefs['coef'] > 1)].index.tolist())\n",
        "coef_neg_set = set(coefs.iloc[np.where(coefs['coef'] < -1)].index.tolist())"
      ],
      "outputs": [],
      "metadata": {
        "id": "rTbsfW1pNRoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "DMPRBKbSSROu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "source": [
        "def words_freq(tweet):\n",
        "  num_pos = len(set(tweet).intersection(coef_pos_set))\n",
        "  num_neg = len(set(tweet).intersection(coef_neg_set))\n",
        "  \n",
        "  # If there exist positive words in the tweet\n",
        "  if num_pos:\n",
        "      for num in range(num_pos):\n",
        "          tweet.append('POSITIVE')\n",
        "  if num_neg:\n",
        "      for num in range(num_neg):\n",
        "          tweet.append('NEGATIVE')\n",
        "  return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "WA6drjzDODFE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "source": [
        "# df['coef_pos'] = df['Twitter_final_sent'].str.contains('').value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "id": "B2Vobuv9Nplj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "source": [
        "df['Tweet_lexicons'] = df['Tweet_stopword'].apply(words_freq)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                                     Tweet_lexicons\n",
              "0         0  ...  [miss, nikki, nu, nu, already, shes, always, t...\n",
              "1         0  ...  [so, had, dream, last, night, remember, sign, ...\n",
              "2         0  ...  [MENTION, ohh, poor, sickly, you, hugs, hope, ...\n",
              "3         0  ...                           [it, is, raining, again]\n",
              "4         0  ...  [MENTION, wish, was, in, la, right, now, NEGAT...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "YHBdfmM_N6au",
        "outputId": "2195bf87-0f62-46f0-e817-4fa26a4c5a5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "source": [
        "df = make_sentences(df, 'Tweet_lexicons', 'Tweet_final_sent_lexicons')\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_emoji</th>\n",
              "      <th>Tweet_nopunc</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pure_string</th>\n",
              "      <th>Tweet_token</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_normalised</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_lexicons</th>\n",
              "      <th>Tweet_final_sent_lexicons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[i, miss, nikki, nu, nu, already, shes, always...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>i miss nikki nu nu already shes always there w...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night. i remember a sign...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, i, had, a, dream, last, night, i, remembe...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "      <td>so i had a dream last night i remember a sign ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[so, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>so had dream last night remember sign which cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>&lt;MENTION&gt; ohh poor sickly you (((hugs)) hope y...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>[MENTION, ohh, poor, sickly, you, hugs, hope, ...</td>\n",
              "      <td>MENTION ohh poor sickly you hugs hope you feel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>it is raining again</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>&lt;MENTION&gt; wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, i, was, in, la, right, now]</td>\n",
              "      <td>MENTION wish was in la right now</td>\n",
              "      <td>MENTION wish i was in la right now</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now]</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>[MENTION, wish, was, in, la, right, now, NEGAT...</td>\n",
              "      <td>MENTION wish was in la right now NEGATIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity  ...                          Tweet_final_sent_lexicons\n",
              "0         0  ...  miss nikki nu nu already shes always there whe...\n",
              "1         0  ...  so had dream last night remember sign which cl...\n",
              "2         0  ...  MENTION ohh poor sickly you hugs hope you feel...\n",
              "3         0  ...                                it is raining again\n",
              "4         0  ...          MENTION wish was in la right now NEGATIVE\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Lxyj1GaQQvFE",
        "outputId": "bc429041-d98c-4c21-d9d4-512ae7676a93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "source": [
        "# Think if you want to do stratify\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Tweet_final_sent_lexicons'], df['Polarity'], stratify=df['Polarity'], test_size=0.1, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BKe9mpRURIfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(hello)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cej5UAk_RIPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_pred = pipe.predict(X_train)\n",
        "print(classification_report(train_pred, \n",
        "                            y_train, \n",
        "                            target_names=['negative', 'positive']))"
      ],
      "outputs": [],
      "metadata": {
        "id": "hxugvRxtrMNl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_pred = pipe.predict(X_test)\n",
        "print(classification_report(test_pred, \n",
        "                            y_test, \n",
        "                            target_names=['negative', 'positive']))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.80      0.79     77463\n",
            "    positive       0.81      0.78      0.80     82537\n",
            "\n",
            "    accuracy                           0.79    160000\n",
            "   macro avg       0.79      0.79      0.79    160000\n",
            "weighted avg       0.79      0.79      0.79    160000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23i-hga4aeUG",
        "outputId": "1b52622a-445e-4b1f-8c3d-34c8fc268109"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in range(10):\n",
        "    lead = X_test.sample(1)\n",
        "    %timeit pipe.predict(lead)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 674 µs per loop\n",
            "1000 loops, best of 5: 680 µs per loop\n",
            "1000 loops, best of 5: 669 µs per loop\n",
            "1000 loops, best of 5: 677 µs per loop\n",
            "1000 loops, best of 5: 688 µs per loop\n",
            "1000 loops, best of 5: 653 µs per loop\n",
            "1000 loops, best of 5: 697 µs per loop\n",
            "1000 loops, best of 5: 672 µs per loop\n",
            "1000 loops, best of 5: 664 µs per loop\n",
            "1000 loops, best of 5: 671 µs per loop\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-FFs86_a3Af",
        "outputId": "172f6683-b010-43d7-a988-5925e9919704"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "KWn5Me4SZiFt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_words = coefs[coefs['coef']>0].index.tolist()\n",
        "neg_words = coefs[coefs['coef']<0].index.tolist()"
      ],
      "outputs": [],
      "metadata": {
        "id": "dBeFQFF1sEWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_words_top = coefs[coefs['coef']>1].index.tolist()\n",
        "neg_words_top = coefs[coefs['coef']<-1].index.tolist()"
      ],
      "outputs": [],
      "metadata": {
        "id": "WJ8_Vpn6sqmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\n",
        "pickle.dump(pos_words_top, open('/content/drive/MyDrive/COL772_A2/pos_words.txt', 'wb'))\n",
        "pickle.dump(neg_words_top, open('/content/drive/MyDrive/COL772_A2/neg_words.txt', 'wb'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Szs5eiVxsEON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "pJnKyqzcs5HI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(min_df=5, ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "# clf = MultinomialNB(alpha=5)\n",
        "\n",
        "# pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VNENQ2ymZ2Ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1,3))\n",
        "ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "clf = MultinomialNB(alpha=5)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=5, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH7OMqaWKSem",
        "outputId": "6f2dd8b7-7ffe-4db4-a11b-5506898cf597"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "source": [
        "# pd.DataFrame(pipeline.predict(X_test)).value_counts()"
      ],
      "outputs": [],
      "metadata": {
        "id": "uwN-qih1Z2IQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "y_pred_self = pipeline.predict(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dbuFA6EhZ1uU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "source": [
        "print('F1 Score: ', f1_score(y_test, y_pred_self))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.774746687451286\n"
          ]
        }
      ],
      "metadata": {
        "id": "JMPfipGqg1hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ac22ce-e768-4150-e018-547c0e1e1698"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "source": [
        "sum(y_pred_self == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.78325"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "metadata": {
        "id": "h0LkAWXIqrn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3384b2b6-52af-4486-e2bf-3921ee239a93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_self).ravel()\n",
        "(tp, fp, tn, fn)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2982, 700, 3284, 1034)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "metadata": {
        "id": "hCwnBy18f95X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd5849d-03c4-4334-a198-2ae527e50bb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "source": [
        "confusion_matrix(y_test, y_pred_self)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3284,  700],\n",
              "       [1034, 2982]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "metadata": {
        "id": "O6dCDLoXsIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a656743-3ffd-46fb-bf00-b2d4d806d662"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='l1',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1,\n",
        "                         max_iter = 1000)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-d9f69d8cd92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# disable defaultdict behaviour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "id": "Yj4qy5w5NvBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "0b7c48c5-b336-4633-fb73-0e15092c46ce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()\n",
        "(tp, fp, tn, fn)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3025, 739, 3245, 991)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr5MV7UpRxHz",
        "outputId": "7e882593-8588-4ffc-bd7b-bccf97a690da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "2Y7eKeCYR3K8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "LdIldvMRR2-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='l2',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.789614356624666\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.793375"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAQud2jbRoBb",
        "outputId": "4a356fff-e0a1-4aab-93b6-4a128be2c41f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "PhoOjgr-Rpun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "source": [
        "tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='elasticnet',\n",
        "                         solver='saga',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1)\n",
        "\n",
        "pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-372a99530568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1495\u001b[0m                     self.l1_ratio < 0 or self.l1_ratio > 1):\n\u001b[1;32m   1496\u001b[0m                         raise ValueError(\"l1_ratio must be between 0 and 1;\"\n\u001b[0;32m-> 1497\u001b[0;31m                                          \" got (l1_ratio=%r)\" % self.l1_ratio)\n\u001b[0m\u001b[1;32m   1498\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
            "\u001b[0;31mValueError\u001b[0m: l1_ratio must be between 0 and 1; got (l1_ratio=None)"
          ]
        }
      ],
      "metadata": {
        "id": "RnnVdzBoOtF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "2ee6f025-1147-4be5-a8c9-b316b0493af7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.DataFrame(y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vlTcfz2OhEsw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(ngram_range=(1,3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "# # all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "# clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "\n",
        "# pipeline = Pipeline([('tfidf', tfidf_ngrams), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=2, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=0,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33RaqvFIjAG",
        "outputId": "39360b36-ff65-4aba-9471-1695206b00f6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "source": [
        "# y_pred_rf = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_rf))\n",
        "# sum(y_pred_rf == y_test)/len(y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.698146595883394\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.594875"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3yIwyQzMwdb",
        "outputId": "dc65eb45-eb60-4935-8573-26e33840eb6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "vOheGU4oMzQ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "res = pd.DataFrame({'Prediction': y_pred_self, 'True':y_test})"
      ],
      "outputs": [],
      "metadata": {
        "id": "WaUFD3EEhGyn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "res"
      ],
      "outputs": [],
      "metadata": {
        "id": "cRBJQ_9IhcKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.set_option('max_colwidth', 400)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xekhPSHoikJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.iloc[res[res['Prediction'] != res['True']].index][['Polarity', 'Tweet', 'Tweet_final_sent']]"
      ],
      "outputs": [],
      "metadata": {
        "id": "-g-7gSM3hcEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.iloc[res[res['Prediction'] == res['True']].index][['Polarity', 'Tweet', 'Tweet_final_sent']]"
      ],
      "outputs": [],
      "metadata": {
        "id": "NKXSUXW4zMKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.iloc[16775]"
      ],
      "outputs": [],
      "metadata": {
        "id": "4OXmr_78nC9P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# set(stopwords.words('english')).difference(['not', 'very'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "OBuK9q1IzOGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_ngrams = TfidfVectorizer(min_df=5, ngram_range=(1, 3))\n",
        "# ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('tfidf', tfidf_ngrams)])\n",
        "clf = MultinomialNB(alpha=5)\n",
        "\n",
        "pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_nb = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_nb, pos_label=4))\n",
        "print(sum(y_pred_nb == y_test)/len(y_test))"
      ],
      "outputs": [],
      "metadata": {
        "id": "cdGtab6VnCvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vKK87GH3nu9J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_ngrams = TfidfVectorizer(min_df=20, ngram_range=(1, 1))\n",
        "ling_stats = LinguisticVectorizer()\n",
        "all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n",
        "\n",
        "clf = LogisticRegression(penalty='l2',\n",
        "                         solver='lbfgs',\n",
        "                         multi_class='multinomial',\n",
        "                         tol=1e-5,\n",
        "                         n_jobs = -1)\n",
        "\n",
        "pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('F1 Score: ', f1_score(y_test, y_pred_lr, pos_label=4))\n",
        "sum(y_pred_lr == y_test)/len(y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DOMdfP6SnY68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_ngrams.get_feature_names()"
      ],
      "outputs": [],
      "metadata": {
        "id": "iZK3JHFUnYzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "axsB6UZ4zsvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle"
      ],
      "outputs": [],
      "metadata": {
        "id": "8qL19-cSZ1o3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pickle.dump(pipeline, open('/content/drive/MyDrive/COL772_A2/model_25.txt', 'wb'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "DCb7XVMPZ1j0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "l3jVKVs_Z1aP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "0iaXZEYjzQS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "snQUjqTZ-mCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "6sLewfWpzQLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_st = df[df['Polarity'] == 4]['Tweet_normalised'].apply(str.split).sum()\n",
        "neg_st = df[df['Polarity'] == 0]['Tweet_normalised'].apply(str.split).sum()"
      ],
      "outputs": [],
      "metadata": {
        "id": "vtdKRlGfn3mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VdsJJR7L8Lly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_uni_freq = FreqDist(ngrams(pos_st, 1))\n",
        "neg_uni_freq = FreqDist(ngrams(neg_st, 1))\n",
        "pos_bi_freq = FreqDist(ngrams(pos_st, 2))\n",
        "neg_bi_freq = FreqDist(ngrams(neg_st, 2))\n",
        "pos_tri_freq = FreqDist(ngrams(pos_st, 3))\n",
        "neg_tri_freq = FreqDist(ngrams(neg_st, 3))"
      ],
      "outputs": [],
      "metadata": {
        "id": "YPGrxogYn3mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_uni_top = pos_uni_freq.most_common(1000)\n",
        "neg_uni_top = neg_uni_freq.most_common(1000)\n",
        "pos_bi_top = pos_bi_freq.most_common(1000)\n",
        "neg_bi_top = neg_bi_freq.most_common(1000)\n",
        "pos_tri_top = pos_tri_freq.most_common(1000)\n",
        "neg_tri_top = neg_tri_freq.most_common(1000)"
      ],
      "outputs": [],
      "metadata": {
        "id": "83lJc7SGn3mO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_uni_top"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('get',), 3923),\n",
              " (('go',), 3437),\n",
              " (('laugh',), 3364),\n",
              " (('good',), 3222),\n",
              " (('love',), 2983),\n",
              " (('day',), 2701),\n",
              " (('like',), 2011),\n",
              " (('thanks',), 1767),\n",
              " (('time',), 1686),\n",
              " (('well',), 1591),\n",
              " (('u',), 1581),\n",
              " (('see',), 1562),\n",
              " (('today',), 1525),\n",
              " (('know',), 1443),\n",
              " (('work',), 1413),\n",
              " (('make',), 1411),\n",
              " (('one',), 1397),\n",
              " (('new',), 1373),\n",
              " (('think',), 1369),\n",
              " (('great',), 1326),\n",
              " (('night',), 1209),\n",
              " (('watch',), 1192),\n",
              " (('back',), 1185),\n",
              " (('look',), 1126),\n",
              " (('oh',), 1094),\n",
              " (('would',), 1056),\n",
              " (('twitter',), 1052),\n",
              " (('come',), 1041),\n",
              " (('morning',), 1016),\n",
              " (('happy',), 981),\n",
              " (('hope',), 963),\n",
              " (('really',), 915),\n",
              " (('fun',), 905),\n",
              " (('wait',), 898),\n",
              " (('much',), 886),\n",
              " (('want',), 876),\n",
              " (('say',), 871),\n",
              " (('need',), 854),\n",
              " (('nice',), 818),\n",
              " (('home',), 806),\n",
              " (('thank',), 805),\n",
              " (('take',), 761),\n",
              " (('hey',), 757),\n",
              " (('tomorrow',), 750),\n",
              " (('still',), 716),\n",
              " (('yeah',), 703),\n",
              " (('tweet',), 698),\n",
              " (('follow',), 698),\n",
              " (('yes',), 686),\n",
              " (('awesome',), 686),\n",
              " (('thing',), 675),\n",
              " (('right',), 670),\n",
              " (('feel',), 660),\n",
              " (('way',), 657),\n",
              " (('friend',), 648),\n",
              " (('last',), 644),\n",
              " (('tonight',), 629),\n",
              " (('yay',), 621),\n",
              " (('try',), 594),\n",
              " (('sleep',), 589),\n",
              " (('cool',), 568),\n",
              " (('best',), 564),\n",
              " (('let',), 562),\n",
              " (('movie',), 556),\n",
              " (('week',), 550),\n",
              " (('start',), 540),\n",
              " (('people',), 536),\n",
              " (('enjoy',), 529),\n",
              " (('guy',), 526),\n",
              " (('everyone',), 524),\n",
              " (('gona',), 516),\n",
              " (('could',), 509),\n",
              " (('song',), 501),\n",
              " (('show',), 498),\n",
              " (('girl',), 493),\n",
              " (('play',), 492),\n",
              " (('weekend',), 481),\n",
              " (('bed',), 480),\n",
              " (('soon',), 475),\n",
              " (('little',), 473),\n",
              " (('sure',), 470),\n",
              " (('first',), 468),\n",
              " (('though',), 466),\n",
              " (('ok',), 465),\n",
              " (('use',), 465),\n",
              " (('x',), 464),\n",
              " (('miss',), 461),\n",
              " (('next',), 455),\n",
              " (('please',), 451),\n",
              " (('lt',), 450),\n",
              " (('find',), 443),\n",
              " (('life',), 438),\n",
              " (('ready',), 430),\n",
              " (('tell',), 430),\n",
              " (('give',), 428),\n",
              " (('always',), 420),\n",
              " (('glad',), 416),\n",
              " (('sound',), 413),\n",
              " (('listen',), 412),\n",
              " (('lot',), 410),\n",
              " (('birthday',), 407),\n",
              " (('do',), 404),\n",
              " (('eat',), 403),\n",
              " (('wish',), 399),\n",
              " (('hour',), 395),\n",
              " (('keep',), 387),\n",
              " (('finally',), 386),\n",
              " (('even',), 383),\n",
              " (('pretty',), 380),\n",
              " (('check',), 379),\n",
              " (('long',), 379),\n",
              " (('ur',), 375),\n",
              " (('year',), 375),\n",
              " (('school',), 373),\n",
              " (('read',), 372),\n",
              " (('talk',), 371),\n",
              " (('us',), 368),\n",
              " (('so',), 366),\n",
              " (('never',), 364),\n",
              " (('help',), 363),\n",
              " (('big',), 359),\n",
              " (('hi',), 356),\n",
              " (('welcome',), 352),\n",
              " (('call',), 349),\n",
              " (('finish',), 348),\n",
              " (('ah',), 345),\n",
              " (('buy',), 342),\n",
              " (('maybe',), 341),\n",
              " (('another',), 338),\n",
              " (('something',), 338),\n",
              " (('pic',), 337),\n",
              " (('cute',), 334),\n",
              " (('amaze',), 334),\n",
              " (('wow',), 332),\n",
              " (('world',), 331),\n",
              " (('ever',), 331),\n",
              " (('excite',), 329),\n",
              " (('live',), 325),\n",
              " (('head',), 323),\n",
              " (('god',), 321),\n",
              " (('bad',), 321),\n",
              " (('party',), 320),\n",
              " (('beautiful',), 317),\n",
              " (('old',), 314),\n",
              " (('leave',), 314),\n",
              " (('aw',), 313),\n",
              " (('okay',), 313),\n",
              " (('summer',), 304),\n",
              " (('actually',), 304),\n",
              " (('later',), 302),\n",
              " (('house',), 301),\n",
              " (('man',), 299),\n",
              " (('game',), 296),\n",
              " (('baby',), 293),\n",
              " (('also',), 293),\n",
              " (('stuff',), 291),\n",
              " (('hear',), 290),\n",
              " (('follower',), 285),\n",
              " (('post',), 284),\n",
              " (('bit',), 283),\n",
              " (('music',), 283),\n",
              " (('already',), 282),\n",
              " (('luck',), 282),\n",
              " (('sorry',), 277),\n",
              " (('saw',), 276),\n",
              " (('meet',), 274),\n",
              " (('send',), 272),\n",
              " (('dont',), 272),\n",
              " (('thats',), 270),\n",
              " (('mean',), 270),\n",
              " (('sweet',), 270),\n",
              " (('hot',), 269),\n",
              " (('forward',), 266),\n",
              " (('funny',), 266),\n",
              " (('free',), 265),\n",
              " (('run',), 264),\n",
              " (('dinner',), 262),\n",
              " (('book',), 261),\n",
              " (('th',), 260),\n",
              " (('wana',), 257),\n",
              " (('cant',), 253),\n",
              " (('might',), 252),\n",
              " (('sunday',), 250),\n",
              " (('video',), 249),\n",
              " (('add',), 248),\n",
              " (('kid',), 248),\n",
              " (('sun',), 247),\n",
              " (('mom',), 245),\n",
              " (('someone',), 244),\n",
              " (('two',), 243),\n",
              " (('picture',), 242),\n",
              " (('ha',), 240),\n",
              " (('guess',), 235),\n",
              " (('hello',), 234),\n",
              " (('omg',), 234),\n",
              " (('nothing',), 233),\n",
              " (('put',), 232),\n",
              " (('job',), 228),\n",
              " (('many',), 226),\n",
              " (('friday',), 225),\n",
              " (('godnight',), 224),\n",
              " (('weather',), 223),\n",
              " (('yet',), 222),\n",
              " (('lunch',), 222),\n",
              " (('name',), 222),\n",
              " (('around',), 221),\n",
              " (('end',), 221),\n",
              " (('gota',), 220),\n",
              " (('may',), 220),\n",
              " (('write',), 219),\n",
              " (('win',), 218),\n",
              " (('late',), 218),\n",
              " (('away',), 217),\n",
              " (('hair',), 217),\n",
              " (('lovely',), 216),\n",
              " (('monday',), 215),\n",
              " (('ask',), 215),\n",
              " (('fan',), 215),\n",
              " (('w',), 215),\n",
              " (('early',), 214),\n",
              " (('must',), 213),\n",
              " (('phone',), 212),\n",
              " (('place',), 211),\n",
              " (('boy',), 210),\n",
              " (('family',), 210),\n",
              " (('hehe',), 210),\n",
              " (('p',), 209),\n",
              " (('rock',), 209),\n",
              " (('coffee',), 207),\n",
              " (('plan',), 205),\n",
              " (('r',), 204),\n",
              " (('update',), 204),\n",
              " (('least',), 201),\n",
              " (('smile',), 200),\n",
              " (('stay',), 198),\n",
              " (('xx',), 198),\n",
              " (('word',), 197),\n",
              " (('yesterday',), 195),\n",
              " (('sit',), 193),\n",
              " (('far',), 192),\n",
              " (('drink',), 192),\n",
              " (('every',), 191),\n",
              " (('food',), 190),\n",
              " (('wake',), 188),\n",
              " (('stop',), 186),\n",
              " (('almost',), 185),\n",
              " (('congrats',), 184),\n",
              " (('hm',), 183),\n",
              " (('bring',), 182),\n",
              " (('walk',), 182),\n",
              " (('dance',), 181),\n",
              " (('rain',), 181),\n",
              " (('till',), 180),\n",
              " (('mother',), 179),\n",
              " (('pay',), 177),\n",
              " (('hard',), 176),\n",
              " (('photo',), 175),\n",
              " (('everything',), 175),\n",
              " (('dream',), 175),\n",
              " (('blog',), 174),\n",
              " (('since',), 173),\n",
              " (('amazing',), 172),\n",
              " (('hopefully',), 171),\n",
              " (('break',), 171),\n",
              " (('change',), 171),\n",
              " (('mine',), 170),\n",
              " (('month',), 170),\n",
              " (('btw',), 169),\n",
              " (('anyone',), 169),\n",
              " (('st',), 167),\n",
              " (('fine',), 167),\n",
              " (('true',), 166),\n",
              " (('anything',), 165),\n",
              " (('car',), 164),\n",
              " (('minute',), 164),\n",
              " (('wonderful',), 163),\n",
              " (('real',), 163),\n",
              " (('course',), 161),\n",
              " (('b',), 161),\n",
              " (('tire',), 160),\n",
              " (('idea',), 159),\n",
              " (('class',), 158),\n",
              " (('hang',), 158),\n",
              " (('move',), 158),\n",
              " (('rest',), 155),\n",
              " (('tho',), 155),\n",
              " (('vote',), 154),\n",
              " (('probably',), 154),\n",
              " (('breakfast',), 154),\n",
              " (('beach',), 154),\n",
              " (('hit',), 153),\n",
              " (('room',), 153),\n",
              " (('kind',), 153),\n",
              " (('favorite',), 153),\n",
              " (('tv',), 152),\n",
              " (('xd',), 152),\n",
              " (('crazy',), 152),\n",
              " (('enough',), 151),\n",
              " (('train',), 149),\n",
              " (('believe',), 149),\n",
              " (('turn',), 149),\n",
              " (('reply',), 149),\n",
              " (('totally',), 148),\n",
              " (('visit',), 148),\n",
              " (('saturday',), 148),\n",
              " (('mind',), 147),\n",
              " (('join',), 147),\n",
              " (('la',), 147),\n",
              " (('happen',), 147),\n",
              " (('learn',), 146),\n",
              " (('ill',), 146),\n",
              " (('remember',), 145),\n",
              " (('m',), 145),\n",
              " (('busy',), 145),\n",
              " (('agree',), 144),\n",
              " (('full',), 144),\n",
              " (('link',), 143),\n",
              " (('definitely',), 143),\n",
              " (('dad',), 142),\n",
              " (('wear',), 142),\n",
              " (('lmao',), 142),\n",
              " (('shower',), 141),\n",
              " (('trip',), 141),\n",
              " (('hate',), 140),\n",
              " (('seem',), 140),\n",
              " (('drive',), 140),\n",
              " (('folowfriday',), 138),\n",
              " (('super',), 137),\n",
              " (('exam',), 137),\n",
              " (('brother',), 137),\n",
              " (('email',), 137),\n",
              " (('june',), 137),\n",
              " (('shop',), 135),\n",
              " (('lose',), 135),\n",
              " (('fuck',), 135),\n",
              " (('album',), 134),\n",
              " (('sister',), 134),\n",
              " (('study',), 134),\n",
              " (('clean',), 131),\n",
              " (('f',), 131),\n",
              " (('nite',), 131),\n",
              " (('problem',), 131),\n",
              " (('dude',), 131),\n",
              " (('pm',), 130),\n",
              " (('site',), 130),\n",
              " (('lady',), 129),\n",
              " (('share',), 129),\n",
              " (('dog',), 129),\n",
              " (('pick',), 129),\n",
              " (('face',), 128),\n",
              " (('star',), 128),\n",
              " (('damn',), 128),\n",
              " (('outside',), 128),\n",
              " (('sunny',), 126),\n",
              " (('catch',), 126),\n",
              " (('bore',), 126),\n",
              " (('kinda',), 125),\n",
              " (('part',), 125),\n",
              " (('g',), 125),\n",
              " (('wonder',), 125),\n",
              " (('whole',), 125),\n",
              " (('perfect',), 125),\n",
              " (('care',), 125),\n",
              " (('quite',), 124),\n",
              " (('proud',), 122),\n",
              " (('high',), 121),\n",
              " (('heart',), 121),\n",
              " (('c',), 121),\n",
              " (('else',), 121),\n",
              " (('final',), 121),\n",
              " (('worry',), 119),\n",
              " (('sing',), 119),\n",
              " (('None',), 119),\n",
              " (('message',), 119),\n",
              " (('lucky',), 119),\n",
              " (('anyway',), 118),\n",
              " (('open',), 118),\n",
              " (('award',), 117),\n",
              " (('half',), 117),\n",
              " (('cold',), 117),\n",
              " (('money',), 114),\n",
              " (('relax',), 113),\n",
              " (('concert',), 113),\n",
              " (('cause',), 112),\n",
              " (('season',), 112),\n",
              " (('couple',), 112),\n",
              " (('story',), 112),\n",
              " (('facebok',), 111),\n",
              " (('forget',), 111),\n",
              " (('bday',), 111),\n",
              " (('shit',), 111),\n",
              " (('list',), 110),\n",
              " (('person',), 110),\n",
              " (('afternoon',), 110),\n",
              " (('together',), 110),\n",
              " (('red',), 109),\n",
              " (('computer',), 109),\n",
              " (('yep',), 109),\n",
              " (('yea',), 108),\n",
              " (('hug',), 107),\n",
              " (('church',), 107),\n",
              " (('thought',), 107),\n",
              " (('ice',), 107),\n",
              " (('gt',), 107),\n",
              " (('sign',), 106),\n",
              " (('iphone',), 106),\n",
              " (('fall',), 106),\n",
              " (('ride',), 105),\n",
              " (('close',), 104),\n",
              " (('date',), 104),\n",
              " (('short',), 103),\n",
              " (('easy',), 103),\n",
              " (('figure',), 102),\n",
              " (('laughh',), 102),\n",
              " (('pack',), 102),\n",
              " (('news',), 101),\n",
              " (('text',), 101),\n",
              " (('point',), 101),\n",
              " (('dear',), 100),\n",
              " (('comment',), 100),\n",
              " (('thx',), 100),\n",
              " (('chocolate',), 100),\n",
              " (('sunshine',), 100),\n",
              " (('suck',), 100),\n",
              " (('bless',), 100),\n",
              " (('eye',), 100),\n",
              " (('band',), 99),\n",
              " (('online',), 99),\n",
              " (('lil',), 99),\n",
              " (('bet',), 98),\n",
              " (('top',), 98),\n",
              " (('google',), 97),\n",
              " (('dress',), 97),\n",
              " (('question',), 97),\n",
              " (('beer',), 97),\n",
              " (('ago',), 97),\n",
              " (('less',), 97),\n",
              " (('hand',), 96),\n",
              " (('able',), 96),\n",
              " (('worth',), 96),\n",
              " (('set',), 96),\n",
              " (('special',), 96),\n",
              " (('cheer',), 96),\n",
              " (('city',), 95),\n",
              " (('nd',), 95),\n",
              " (('holiday',), 95),\n",
              " (('sometimes',), 95),\n",
              " (('second',), 94),\n",
              " (('tom',), 94),\n",
              " (('excited',), 94),\n",
              " (('mr',), 94),\n",
              " (('shopping',), 94),\n",
              " (('moment',), 94),\n",
              " (('green',), 94),\n",
              " (('bye',), 93),\n",
              " (('l',), 93),\n",
              " (('bout',), 92),\n",
              " (('page',), 92),\n",
              " (('appreciate',), 92),\n",
              " (('line',), 92),\n",
              " (('weird',), 91),\n",
              " (('ticket',), 91),\n",
              " (('via',), 91),\n",
              " (('mtv',), 91),\n",
              " (('youtube',), 90),\n",
              " (('feeling',), 90),\n",
              " (('til',), 90),\n",
              " (('nap',), 90),\n",
              " (('sick',), 89),\n",
              " (('team',), 89),\n",
              " (('fix',), 88),\n",
              " (('reason',), 88),\n",
              " (('pl',), 88),\n",
              " (('test',), 88),\n",
              " (('goin',), 88),\n",
              " (('office',), 88),\n",
              " (('cake',), 88),\n",
              " (('wrong',), 88),\n",
              " (('cook',), 88),\n",
              " (('hop',), 87),\n",
              " (('cream',), 87),\n",
              " (('twilight',), 87),\n",
              " (('download',), 87),\n",
              " (('spend',), 87),\n",
              " (('woho',), 87),\n",
              " (('hell',), 86),\n",
              " (('yummy',), 86),\n",
              " (('chat',), 86),\n",
              " (('tour',), 86),\n",
              " (('e',), 86),\n",
              " (('record',), 85),\n",
              " (('decide',), 85),\n",
              " (('alright',), 85),\n",
              " (('instead',), 85),\n",
              " (('meeting',), 85),\n",
              " (('awake',), 85),\n",
              " (('kick',), 85),\n",
              " (('chill',), 84),\n",
              " (('support',), 84),\n",
              " (('park',), 84),\n",
              " (('pool',), 84),\n",
              " (('gym',), 84),\n",
              " (('black',), 84),\n",
              " (('episode',), 84),\n",
              " (('yum',), 84),\n",
              " (('vip',), 83),\n",
              " (('moon',), 83),\n",
              " (('si',), 83),\n",
              " (('without',), 83),\n",
              " (('garden',), 82),\n",
              " (('wed',), 82),\n",
              " (('either',), 81),\n",
              " (('david',), 81),\n",
              " (('interest',), 81),\n",
              " (('town',), 81),\n",
              " (('pizza',), 81),\n",
              " (('eh',), 80),\n",
              " (('miley',), 80),\n",
              " (('cd',), 80),\n",
              " (('blue',), 80),\n",
              " (('woke',), 80),\n",
              " (('em',), 80),\n",
              " (('order',), 80),\n",
              " (('project',), 80),\n",
              " (('gorgeous',), 80),\n",
              " (('film',), 79),\n",
              " (('jonas',), 79),\n",
              " (('as',), 79),\n",
              " (('answer',), 79),\n",
              " (('cuz',), 79),\n",
              " (('fly',), 78),\n",
              " (('side',), 78),\n",
              " (('save',), 78),\n",
              " (('fast',), 78),\n",
              " (('congratulation',), 77),\n",
              " (('son',), 77),\n",
              " (('cut',), 77),\n",
              " (('fantastic',), 77),\n",
              " (('beat',), 77),\n",
              " (('mum',), 76),\n",
              " (('plus',), 76),\n",
              " (('yup',), 76),\n",
              " (('water',), 76),\n",
              " (('da',), 76),\n",
              " (('warm',), 76),\n",
              " (('london',), 76),\n",
              " (('white',), 76),\n",
              " (('cousin',), 76),\n",
              " (('v',), 76),\n",
              " (('small',), 75),\n",
              " (('wine',), 75),\n",
              " (('begin',), 75),\n",
              " (('xxx',), 75),\n",
              " (('cat',), 75),\n",
              " (('woo',), 74),\n",
              " (('paper',), 74),\n",
              " (('vacation',), 74),\n",
              " (('heard',), 74),\n",
              " (('voice',), 73),\n",
              " (('number',), 73),\n",
              " (('light',), 73),\n",
              " (('different',), 73),\n",
              " (('fb',), 72),\n",
              " (('suppose',), 72),\n",
              " (('o',), 72),\n",
              " (('dm',), 72),\n",
              " (('cup',), 71),\n",
              " (('chance',), 71),\n",
              " (('tea',), 71),\n",
              " (('internet',), 71),\n",
              " (('um',), 71),\n",
              " (('deserve',), 70),\n",
              " (('twit',), 70),\n",
              " (('paint',), 70),\n",
              " (('type',), 70),\n",
              " (('xoxo',), 70),\n",
              " (('seriously',), 70),\n",
              " (('jealous',), 70),\n",
              " (('stick',), 70),\n",
              " (('hubby',), 70),\n",
              " (('whats',), 70),\n",
              " (('wo',), 69),\n",
              " (('shoe',), 69),\n",
              " (('ipod',), 69),\n",
              " (('babe',), 68),\n",
              " (('hurt',), 68),\n",
              " (('bq',), 68),\n",
              " (('ooh',), 68),\n",
              " (('wednesday',), 68),\n",
              " (('die',), 68),\n",
              " (('especially',), 68),\n",
              " (('laptop',), 68),\n",
              " (('graduation',), 68),\n",
              " (('shal',), 68),\n",
              " (('ive',), 68),\n",
              " (('interview',), 68),\n",
              " (('july',), 68),\n",
              " (('store',), 68),\n",
              " (('quote',), 68),\n",
              " (('woman',), 67),\n",
              " (('english',), 67),\n",
              " (('tune',), 67),\n",
              " (('interesting',), 67),\n",
              " (('min',), 67),\n",
              " (('ugh',), 67),\n",
              " (('absolutely',), 67),\n",
              " (('everybody',), 67),\n",
              " (('safe',), 67),\n",
              " (('de',), 66),\n",
              " (('website',), 66),\n",
              " (('whatever',), 66),\n",
              " (('xo',), 66),\n",
              " (('sad',), 66),\n",
              " (('speak',), 66),\n",
              " (('club',), 65),\n",
              " (('hilarious',), 65),\n",
              " (('shirt',), 65),\n",
              " (('deal',), 65),\n",
              " (('exactly',), 65),\n",
              " (('fabulous',), 65),\n",
              " (('thursday',), 65),\n",
              " (('apple',), 65),\n",
              " (('three',), 64),\n",
              " (('account',), 64),\n",
              " (('touch',), 64),\n",
              " (('cover',), 64),\n",
              " (('stand',), 64),\n",
              " (('mac',), 64),\n",
              " (('mention',), 63),\n",
              " (('load',), 63),\n",
              " (('french',), 63),\n",
              " (('celebrate',), 63),\n",
              " (('kill',), 63),\n",
              " (('finger',), 63),\n",
              " (('hold',), 63),\n",
              " (('taylor',), 63),\n",
              " (('huh',), 62),\n",
              " (('info',), 62),\n",
              " (('indeed',), 62),\n",
              " (('tuesday',), 62),\n",
              " (('guitar',), 62),\n",
              " (('kiss',), 62),\n",
              " (('group',), 62),\n",
              " (('radio',), 62),\n",
              " (('bro',), 62),\n",
              " (('wit',), 62),\n",
              " (('myspace',), 62),\n",
              " (('count',), 62),\n",
              " (('event',), 61),\n",
              " (('blast',), 61),\n",
              " (('past',), 61),\n",
              " (('excellent',), 61),\n",
              " (('shoot',), 61),\n",
              " (('dvd',), 61),\n",
              " (('mate',), 61),\n",
              " (('lazy',), 61),\n",
              " (('flight',), 61),\n",
              " (('sims',), 60),\n",
              " (('wot',), 60),\n",
              " (('huge',), 60),\n",
              " (('homework',), 60),\n",
              " (('hun',), 60),\n",
              " (('sense',), 60),\n",
              " (('reading',), 60),\n",
              " (('road',), 60),\n",
              " (('choice',), 60),\n",
              " (('mood',), 59),\n",
              " (('present',), 59),\n",
              " (('joke',), 59),\n",
              " (('web',), 59),\n",
              " (('hill',), 59),\n",
              " (('random',), 59),\n",
              " (('he',), 59),\n",
              " (('sweetie',), 59),\n",
              " (('yo',), 58),\n",
              " (('h',), 58),\n",
              " (('nah',), 58),\n",
              " (('understand',), 58),\n",
              " (('tip',), 58),\n",
              " (('sort',), 58),\n",
              " (('yr',), 58),\n",
              " (('sell',), 58),\n",
              " (('asleep',), 58),\n",
              " (('fire',), 58),\n",
              " (('wont',), 58),\n",
              " (('trek',), 57),\n",
              " (('bf',), 57),\n",
              " (('stupid',), 57),\n",
              " (('air',), 57),\n",
              " (('bitch',), 57),\n",
              " (('grow',), 57),\n",
              " (('lakers',), 57),\n",
              " (('surprise',), 57),\n",
              " (('profile',), 57),\n",
              " (('others',), 57),\n",
              " (('roll',), 57),\n",
              " (('forever',), 57),\n",
              " (('parent',), 57),\n",
              " (('wife',), 56),\n",
              " (('young',), 56),\n",
              " (('swim',), 56),\n",
              " (('pray',), 56),\n",
              " (('bike',), 56),\n",
              " (('chilin',), 56),\n",
              " (('power',), 56),\n",
              " (('tired',), 56),\n",
              " (('college',), 56),\n",
              " (('business',), 56),\n",
              " (('practice',), 56),\n",
              " (('chicken',), 56),\n",
              " (('peace',), 56),\n",
              " (('anymore',), 56),\n",
              " (('self',), 56),\n",
              " (('release',), 55),\n",
              " (('fresh',), 55),\n",
              " (('pass',), 55),\n",
              " (('drop',), 55),\n",
              " (('tan',), 55),\n",
              " (('shout',), 55),\n",
              " (('lay',), 55),\n",
              " (('uk',), 54),\n",
              " (('ahead',), 54),\n",
              " (('clothes',), 54),\n",
              " (('gig',), 54),\n",
              " (('delicious',), 54),\n",
              " (('sexy',), 54),\n",
              " (('include',), 54),\n",
              " (('rather',), 54),\n",
              " (('pop',), 54),\n",
              " (('child',), 54),\n",
              " (('complete',), 54),\n",
              " (('ima',), 54),\n",
              " (('bag',), 54),\n",
              " (('return',), 53),\n",
              " (('round',), 53),\n",
              " (('notice',), 53),\n",
              " (('bus',), 53),\n",
              " (('fav',), 53),\n",
              " (('smell',), 53),\n",
              " (('thanx',), 53),\n",
              " (('boyfriend',), 53),\n",
              " (('drunk',), 53),\n",
              " (('fact',), 52),\n",
              " (('age',), 52),\n",
              " (('officially',), 52),\n",
              " (('step',), 52),\n",
              " (('joe',), 52),\n",
              " (('body',), 52),\n",
              " (('taste',), 52),\n",
              " (('art',), 52),\n",
              " (('quiet',), 52),\n",
              " (('math',), 52),\n",
              " (('apparently',), 52),\n",
              " (('secret',), 51),\n",
              " (('bc',), 51),\n",
              " (('demi',), 51),\n",
              " (('alot',), 51),\n",
              " (('fit',), 51),\n",
              " (('watchin',), 51),\n",
              " (('travel',), 51),\n",
              " (('cheese',), 51),\n",
              " (('brilliant',), 51),\n",
              " (('john',), 51),\n",
              " (('fight',), 51),\n",
              " (('promise',), 50),\n",
              " (('background',), 50),\n",
              " (('favourite',), 50),\n",
              " (('bird',), 50),\n",
              " (('cross',), 50),\n",
              " (('godmorning',), 50),\n",
              " (('door',), 50),\n",
              " (('slow',), 50),\n",
              " (('glass',), 50),\n",
              " (('burn',), 50),\n",
              " (('ap',), 50),\n",
              " (('hanah',), 50),\n",
              " (('mommy',), 50),\n",
              " (('front',), 49),\n",
              " (('search',), 49),\n",
              " (('upload',), 49),\n",
              " (('mail',), 49),\n",
              " (('gift',), 49),\n",
              " (('epic',), 49),\n",
              " (('heh',), 49),\n",
              " (('version',), 49),\n",
              " (('aha',), 49),\n",
              " (('hungry',), 49),\n",
              " (('realize',), 49),\n",
              " (('pink',), 49),\n",
              " (('fair',), 49),\n",
              " (('crap',), 49),\n",
              " (('you',), 49),\n",
              " (('pas',), 48),\n",
              " (('nope',), 48),\n",
              " (('experience',), 48),\n",
              " (('ball',), 48),\n",
              " (('bear',), 48),\n",
              " (('etc',), 48),\n",
              " (('case',), 48),\n",
              " (('graduate',), 48),\n",
              " (('tweps',), 48),\n",
              " (('review',), 48),\n",
              " (('become',), 47),\n",
              " (('addict',), 47),\n",
              " (('king',), 47),\n",
              " (('hangover',), 47),\n",
              " (('silly',), 47),\n",
              " (('bar',), 47),\n",
              " (('j',), 47),\n",
              " (('matter',), 47),\n",
              " (('design',), 47),\n",
              " (('act',), 47),\n",
              " (('foot',), 47),\n",
              " (('peep',), 47),\n",
              " (('mad',), 47),\n",
              " (('along',), 47),\n",
              " (('except',), 47),\n",
              " (('camera',), 46),\n",
              " (('memory',), 46),\n",
              " (('getin',), 46),\n",
              " (('freak',), 46),\n",
              " (('def',), 46),\n",
              " (('fill',), 46),\n",
              " (('fat',), 46),\n",
              " (('company',), 46),\n",
              " (('wave',), 46),\n",
              " (('hotel',), 46),\n",
              " (('annoy',), 46),\n",
              " (('daughter',), 46),\n",
              " (('future',), 45),\n",
              " (('lame',), 45),\n",
              " (('important',), 45),\n",
              " (('camp',), 45),\n",
              " (('england',), 45),\n",
              " (('jk',), 45),\n",
              " (('state',), 45),\n",
              " (('chris',), 45),\n",
              " (('american',), 45),\n",
              " (('land',), 45),\n",
              " (('mall',), 44),\n",
              " (('nick',), 44),\n",
              " (('war',), 44),\n",
              " (('jump',), 44),\n",
              " (('color',), 44),\n",
              " (('card',), 44),\n",
              " (('ho',), 44),\n",
              " (('ai',), 44),\n",
              " (('dark',), 44),\n",
              " (('lately',), 44),\n",
              " (('earlier',), 44),\n",
              " (('topic',), 44),\n",
              " (('prom',), 44),\n",
              " (('ate',), 44),\n",
              " (('positive',), 44),\n",
              " (('adorable',), 44),\n",
              " (('article',), 44),\n",
              " (('evening',), 44),\n",
              " (('mcfly',), 44),\n",
              " (('sale',), 44),\n",
              " (('race',), 44),\n",
              " (('pc',), 44),\n",
              " (('anyways',), 44),\n",
              " (('bug',), 43),\n",
              " (('perhaps',), 43),\n",
              " (('alone',), 43),\n",
              " (('daddy',), 43),\n",
              " (('recommend',), 43),\n",
              " (('fam',), 43),\n",
              " (('doubt',), 43),\n",
              " (('sir',), 43),\n",
              " (('straight',), 43),\n",
              " (('currently',), 43),\n",
              " (('usually',), 43),\n",
              " (('bottle',), 43),\n",
              " (('buddy',), 43),\n",
              " (('track',), 43),\n",
              " (('pleasure',), 43),\n",
              " (('channel',), 43),\n",
              " (('gosh',), 43),\n",
              " (('spending',), 43),\n",
              " (('country',), 43),\n",
              " (('wall',), 43),\n",
              " (('treat',), 43),\n",
              " (('folk',), 43),\n",
              " (('mess',), 43),\n",
              " (('bath',), 43),\n",
              " (('island',), 42),\n",
              " (('sigh',), 42),\n",
              " (('lie',), 42),\n",
              " (('germany',), 42),\n",
              " (('remind',), 42),\n",
              " (('single',), 42),\n",
              " (('teach',), 42),\n",
              " (('boo',), 42),\n",
              " (('view',), 42),\n",
              " (('style',), 42),\n",
              " (('service',), 42),\n",
              " (('consider',), 42),\n",
              " (('puppy',), 42),\n",
              " (('usual',), 42),\n",
              " (('note',), 42),\n",
              " (('trailer',), 42),\n",
              " (('shin',), 41),\n",
              " (('nail',), 41),\n",
              " (('copy',), 41),\n",
              " (('price',), 41),\n",
              " (('hr',), 41),\n",
              " (('mile',), 41),\n",
              " (('dead',), 41),\n",
              " (('nearly',), 41),\n",
              " (('jus',), 41),\n",
              " (('expect',), 41),\n",
              " (('everyday',), 41),\n",
              " (('space',), 41),\n",
              " (('lesson',), 41),\n",
              " (('arrive',), 41),\n",
              " (('fail',), 41),\n",
              " (('rise',), 41),\n",
              " (('felt',), 41),\n",
              " (('station',), 41),\n",
              " (('near',), 41),\n",
              " (('prepare',), 41),\n",
              " (('wedding',), 41),\n",
              " (('doin',), 40),\n",
              " (('form',), 40),\n",
              " (('normal',), 40),\n",
              " (('canot',), 40),\n",
              " (('itunes',), 40),\n",
              " (('model',), 40),\n",
              " (('bake',), 40),\n",
              " (('pain',), 40),\n",
              " (('simple',), 40),\n",
              " (('low',), 40),\n",
              " (('create',), 40),\n",
              " (('studio',), 40),\n",
              " (('lake',), 40),\n",
              " (('window',), 40),\n",
              " (('gr',), 40),\n",
              " (('south',), 40),\n",
              " (('alaugh',), 40),\n",
              " (('cry',), 40),\n",
              " (('bright',), 40),\n",
              " (('sky',), 40),\n",
              " (('official',), 40),\n",
              " (('inside',), 40),\n",
              " (('anytime',), 39),\n",
              " (('toy',), 39),\n",
              " (('extra',), 39),\n",
              " (('draw',), 39),\n",
              " (('fish',), 39),\n",
              " (('men',), 39),\n",
              " (('angel',), 39),\n",
              " (('thinking',), 39),\n",
              " (('ops',), 39),\n",
              " (('sex',), 39),\n",
              " (('shot',), 39),\n",
              " (('manage',), 38),\n",
              " (('degree',), 38),\n",
              " (('thankyou',), 38),\n",
              " (('box',), 38),\n",
              " (('street',), 38),\n",
              " (('invite',), 38),\n",
              " (('airport',), 38),\n",
              " (('friends',), 38),\n",
              " (('print',), 38),\n",
              " (('direct',), 38),\n",
              " (('n',), 38),\n",
              " (('dany',), 38),\n",
              " (('stage',), 38),\n",
              " (('cheap',), 38),\n",
              " (('pull',), 38),\n",
              " (('unles',), 38),\n",
              " (('although',), 38),\n",
              " (('productive',), 38),\n",
              " (('sushi',), 38),\n",
              " (('poor',), 38),\n",
              " (('hero',), 38),\n",
              " (('result',), 38),\n",
              " (('hehehe',), 37),\n",
              " (('mark',), 37),\n",
              " (('anniversary',), 37),\n",
              " (('gay',), 37),\n",
              " (('magic',), 37),\n",
              " (('matt',), 37),\n",
              " (('august',), 37),\n",
              " (('click',), 37),\n",
              " (('detail',), 37),\n",
              " (('marathon',), 37),\n",
              " (('choose',), 37),\n",
              " (('quick',), 37),\n",
              " (('aka',), 37),\n",
              " (('tomorow',), 37),\n",
              " (('drinking',), 37),\n",
              " (('often',), 37),\n",
              " (('behind',), 37),\n",
              " (('michael',), 37),\n",
              " (('wash',), 37),\n",
              " (('lovin',), 37),\n",
              " (('rd',), 37),\n",
              " (('wop',), 37)]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "metadata": {
        "id": "4LQYUjw2n3mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfaa15e-21d7-4bfa-c28e-c71ee24eefb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_top_words(sent_list):\n",
        "    word_list = []\n",
        "    for i in range(len(sent_list)):\n",
        "        word_list.append(sent_list[i][0][0])\n",
        "    return word_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "XYwMmhK7n3mO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_uni_top_words = get_top_words(pos_uni_top)\n",
        "neg_uni_top_words = get_top_words(neg_uni_top)\n",
        "pos_bi_top_words = get_top_words(pos_bi_top)\n",
        "neg_bi_top_words = get_top_words(neg_bi_top)\n",
        "pos_tri_top_words = get_top_words(pos_tri_top)\n",
        "neg_tri_top_words = get_top_words(neg_tri_top)"
      ],
      "outputs": [],
      "metadata": {
        "id": "FNpIVe1En3mO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(pos_uni_top_words))\n",
        "print(len(neg_uni_top_words))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ],
      "metadata": {
        "id": "K_8TSTCln3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd09b523-5b78-4a97-a80c-c42a4687c3cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "uni_top_common = set(set(pos_uni_top_words) & set(neg_uni_top_words))\n",
        "pos_best_words = list(set(pos_uni_top_words) - uni_top_common)\n",
        "neg_best_words = list(set(neg_uni_top_words) - uni_top_common)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AtR4fAUzn3mP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "uni_top_common_list = list(uni_top_common)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ViSkXIhKn3mP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pos_best_words"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aha',\n",
              " 'usually',\n",
              " 'interesting',\n",
              " 'hilarious',\n",
              " 'nah',\n",
              " 'proud',\n",
              " 'choose',\n",
              " 'lovin',\n",
              " 'thanx',\n",
              " 'promise',\n",
              " 'draw',\n",
              " 'excited',\n",
              " 'thankyou',\n",
              " 'bath',\n",
              " 'appreciate',\n",
              " 'gorgeous',\n",
              " 'wo',\n",
              " 'ahead',\n",
              " 'sexy',\n",
              " 'wop',\n",
              " 'j',\n",
              " 'ai',\n",
              " 'positive',\n",
              " 'official',\n",
              " 'review',\n",
              " 'heh',\n",
              " 'quote',\n",
              " 'surprise',\n",
              " 'germany',\n",
              " 'hun',\n",
              " 'pizza',\n",
              " 'consider',\n",
              " 'john',\n",
              " 'sex',\n",
              " 'war',\n",
              " 'perfect',\n",
              " 'profile',\n",
              " 'topic',\n",
              " 'matt',\n",
              " 'choice',\n",
              " 'demi',\n",
              " 'fav',\n",
              " 'design',\n",
              " 'jk',\n",
              " 'rd',\n",
              " 'shal',\n",
              " 'anyways',\n",
              " 'indeed',\n",
              " 'dany',\n",
              " 'often',\n",
              " 'congratulation',\n",
              " 'paint',\n",
              " 'usual',\n",
              " 'etc',\n",
              " 'hanah',\n",
              " 'experience',\n",
              " 'adorable',\n",
              " 'shin',\n",
              " 'anniversary',\n",
              " 'aka',\n",
              " 'brilliant',\n",
              " 'sweetie',\n",
              " 'bake',\n",
              " 'unles',\n",
              " 'prepare',\n",
              " 'direct',\n",
              " 'count',\n",
              " 'folk',\n",
              " 'sky',\n",
              " 'studio',\n",
              " 'view',\n",
              " 'quiet',\n",
              " 'gift',\n",
              " 'tweps',\n",
              " 'everybody',\n",
              " 'space',\n",
              " 'color',\n",
              " 'fresh',\n",
              " 'fabulous',\n",
              " 'lake',\n",
              " 'joe',\n",
              " 'include',\n",
              " 'bright',\n",
              " 'peace',\n",
              " 'info',\n",
              " 'detail',\n",
              " 'chilin',\n",
              " 'treat',\n",
              " 'michael',\n",
              " 'doin',\n",
              " 'wave',\n",
              " 'american',\n",
              " 'tip',\n",
              " 'drinking',\n",
              " 'secret',\n",
              " 'king',\n",
              " 'fam',\n",
              " 'recommend',\n",
              " 'pop',\n",
              " 'england',\n",
              " 'price',\n",
              " 'dvd',\n",
              " 'mention',\n",
              " 'thx',\n",
              " 'silly',\n",
              " 'watchin',\n",
              " 'style',\n",
              " 'cross',\n",
              " 'marathon',\n",
              " 'self',\n",
              " 'bottle',\n",
              " 'pleasure',\n",
              " 'alright',\n",
              " 'shout',\n",
              " 'welcome',\n",
              " 'gig',\n",
              " 'sir',\n",
              " 'quick',\n",
              " 'nick',\n",
              " 'present',\n",
              " 'celebrate',\n",
              " 'taylor',\n",
              " 'currently',\n",
              " 'teach',\n",
              " 'straight',\n",
              " 'prom',\n",
              " 'spending',\n",
              " 'hero',\n",
              " 'nail',\n",
              " 'mcfly',\n",
              " 'model',\n",
              " 'rise',\n",
              " 'men',\n",
              " 'hehehe',\n",
              " 'create',\n",
              " 'epic',\n",
              " 'magic',\n",
              " 'bless',\n",
              " 'important',\n",
              " 'toy',\n",
              " 'fill',\n",
              " 'lesson',\n",
              " 'favourite',\n",
              " 'yum',\n",
              " 'reading',\n",
              " 'track',\n",
              " 'smile',\n",
              " 'blast',\n",
              " 'de',\n",
              " 'delicious',\n",
              " 'mate',\n",
              " 'trek',\n",
              " 'jump',\n",
              " 'friends',\n",
              " 'pink',\n",
              " 'godmorning',\n",
              " 'click',\n",
              " 'future',\n",
              " 'island',\n",
              " 'fantastic',\n",
              " 'vip',\n",
              " 'sense',\n",
              " 'woho',\n",
              " 'evening',\n",
              " 'article',\n",
              " 'twilight',\n",
              " 'yummy',\n",
              " 'ooh',\n",
              " 'background',\n",
              " 'anytime',\n",
              " 'tune',\n",
              " 'wall',\n",
              " 'woo',\n",
              " 'print',\n",
              " 'wot',\n",
              " 'copy',\n",
              " 'xo',\n",
              " 'stage',\n",
              " 'form',\n",
              " 'deserve',\n",
              " 'addict',\n",
              " 'congrats',\n",
              " 'alaugh',\n",
              " 'chris',\n",
              " 'wedding',\n",
              " 'mall',\n",
              " 'everyday',\n",
              " 'interest',\n",
              " 'simple',\n",
              " 'angel',\n",
              " 'trailer',\n",
              " 'excellent',\n",
              " 'def',\n",
              " 'productive',\n",
              " 'cheap',\n",
              " 'doubt',\n",
              " 'roll',\n",
              " 'perhaps',\n",
              " 'peep',\n",
              " 'folowfriday',\n",
              " 'he',\n",
              " 'sushi',\n",
              " 'step',\n",
              " 'land',\n",
              " 'graduate']"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "metadata": {
        "id": "pwgQaTDzn3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dfb6a5-36b0-440c-d601-f7a7f067a3d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "neg_best_words"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['delay',\n",
              " 'completely',\n",
              " 'cough',\n",
              " 'disappointed',\n",
              " 'fell',\n",
              " 'cancel',\n",
              " 'thunder',\n",
              " 'dnt',\n",
              " 'kate',\n",
              " 'ew',\n",
              " 'piss',\n",
              " 'blow',\n",
              " 'terrible',\n",
              " 'uh',\n",
              " 'shift',\n",
              " 'lonely',\n",
              " 'throat',\n",
              " 'nightmare',\n",
              " 'fever',\n",
              " 'however',\n",
              " 'blood',\n",
              " 'confuse',\n",
              " 'stuck',\n",
              " 'starbucks',\n",
              " 'to',\n",
              " 'broken',\n",
              " 'traffic',\n",
              " 'trouble',\n",
              " 'sat',\n",
              " 'somewhere',\n",
              " 'unfortunately',\n",
              " 'screw',\n",
              " 'ring',\n",
              " 'ac',\n",
              " 'shitty',\n",
              " 'bank',\n",
              " 'rid',\n",
              " 'board',\n",
              " 'entire',\n",
              " 'couldnt',\n",
              " 'shes',\n",
              " 'painful',\n",
              " 'wasnt',\n",
              " 'wah',\n",
              " 'floor',\n",
              " 'rip',\n",
              " 'crappy',\n",
              " 'doctor',\n",
              " 'mouth',\n",
              " 'freeze',\n",
              " 'key',\n",
              " 'dread',\n",
              " 'blackberry',\n",
              " 'block',\n",
              " 'laundry',\n",
              " 'anywhere',\n",
              " 'co',\n",
              " 'exhaust',\n",
              " 'burnt',\n",
              " 'vet',\n",
              " 'bummer',\n",
              " 'blah',\n",
              " 'no',\n",
              " 'bo',\n",
              " 'atm',\n",
              " 'there',\n",
              " 'san',\n",
              " 'shut',\n",
              " 'slept',\n",
              " 'arm',\n",
              " 'tummy',\n",
              " 'accident',\n",
              " 'left',\n",
              " 'wat',\n",
              " 'france',\n",
              " 'knee',\n",
              " 'shame',\n",
              " 'ear',\n",
              " 'bored',\n",
              " 'guted',\n",
              " 'duno',\n",
              " 'science',\n",
              " 'sadly',\n",
              " 'ouch',\n",
              " 'leg',\n",
              " 'report',\n",
              " 'fml',\n",
              " 'aint',\n",
              " 'headache',\n",
              " 'assignment',\n",
              " 'darn',\n",
              " 'revision',\n",
              " 'depress',\n",
              " 'weight',\n",
              " 'schedule',\n",
              " 'doesnt',\n",
              " 'gah',\n",
              " 'bill',\n",
              " 'bloody',\n",
              " 'none',\n",
              " 'hat',\n",
              " 'size',\n",
              " 'horrible',\n",
              " 'upset',\n",
              " 'apart',\n",
              " 'swear',\n",
              " 'cell',\n",
              " 'wtf',\n",
              " 'middle',\n",
              " 'delete',\n",
              " 'forgot',\n",
              " 'throw',\n",
              " 'due',\n",
              " 'match',\n",
              " 'dc',\n",
              " 'argh',\n",
              " 'sunburn',\n",
              " 'nyc',\n",
              " 'cost',\n",
              " 'battery',\n",
              " 'bite',\n",
              " 'barely',\n",
              " 'ow',\n",
              " 'neck',\n",
              " 'jon',\n",
              " 'sleepy',\n",
              " 'nobody',\n",
              " 'loss',\n",
              " 'outa',\n",
              " 'file',\n",
              " 'serious',\n",
              " 'stomach',\n",
              " 'disappoint',\n",
              " 'damit',\n",
              " 'allow',\n",
              " 'major',\n",
              " 'sore',\n",
              " 'error',\n",
              " 'death',\n",
              " 'me',\n",
              " 'kno',\n",
              " 'felin',\n",
              " 'realy',\n",
              " 'crash',\n",
              " 'boring',\n",
              " 'longer',\n",
              " 'dang',\n",
              " 'heat',\n",
              " 'thru',\n",
              " 'waste',\n",
              " 'history',\n",
              " 'sum',\n",
              " 'bum',\n",
              " 'issue',\n",
              " 'milk',\n",
              " 'plane',\n",
              " 'swine',\n",
              " 'feed',\n",
              " 'goodbye',\n",
              " 'upgrade',\n",
              " 'suffer',\n",
              " 'flu',\n",
              " 'fault',\n",
              " 'isnt',\n",
              " 'tear',\n",
              " 'revise',\n",
              " 'awful',\n",
              " 'twetdeck',\n",
              " 'essay',\n",
              " 'empty',\n",
              " 'bb',\n",
              " 'wet',\n",
              " 'series',\n",
              " 'afraid',\n",
              " 'tried',\n",
              " 'piece',\n",
              " 'scary',\n",
              " 'hospital',\n",
              " 'afford',\n",
              " 'smh',\n",
              " 'kitty',\n",
              " 'bah',\n",
              " 'nervous',\n",
              " 'dentist',\n",
              " 'ny',\n",
              " 'area',\n",
              " 'nose',\n",
              " 'spent',\n",
              " 'lack',\n",
              " 'rainy',\n",
              " 'storm',\n",
              " 'chicago',\n",
              " 'chip',\n",
              " 'teeth',\n",
              " 'energy',\n",
              " 'bother',\n",
              " 'badly',\n",
              " 'rubbish',\n",
              " 'father',\n",
              " 'ruin',\n",
              " 'ache',\n",
              " 'screen',\n",
              " 'stress',\n",
              " 'scar',\n",
              " 'idk']"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "metadata": {
        "id": "fyDW7ZHEn3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d7b765-a462-4301-8a38-ef0e9042d480"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len(pos_best_words)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "metadata": {
        "id": "_OMzecSSn3mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e00ec2-16ff-48e9-9c79-bbfc0f04b752"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "KB_0TxHx_Ru1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# tfidf_ngrams = TfidfVectorizer(min_df=5, ngram_range=(1, 3))\n",
        "# # ling_stats = LinguisticVectorizer()\n",
        "# all_features = FeatureUnion([('tfidf', tfidf_ngrams), ('pos', pos_best_words), ('neg', neg_best_words)])\n",
        "# clf = MultinomialNB(alpha=1)\n",
        "\n",
        "# pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
        "\n",
        "# pipeline.fit(X_train, y_train)\n",
        "# y_pred_nb = pipeline.predict(X_test)\n",
        "# print('F1 Score: ', f1_score(y_test, y_pred_nb, pos_label=4))\n",
        "# print(sum(y_pred_nb == y_test)/len(y_test))"
      ],
      "outputs": [],
      "metadata": {
        "id": "edNiU6Lt_RmT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "Wm6UtHHq_Rep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "znOCGskI_RXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['Tweet'].iloc[50]"
      ],
      "outputs": [],
      "metadata": {
        "id": "5L3PGj_tn3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['Tweet_sent'].iloc[50]"
      ],
      "outputs": [],
      "metadata": {
        "id": "hJQGu8MQn3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data.iloc[786897,:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "3PgsEeJgn3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.iloc[786897]"
      ],
      "outputs": [],
      "metadata": {
        "id": "y8uV_BEUn3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "jO2zLY4en3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def dummy(tweet):\n",
        "    return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "TOsgBKtVn3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cv = CountVectorizer(  \n",
        "                      tokenizer=dummy,\n",
        "                      preprocessor=dummy,\n",
        "                      ngram_range=(1,1)\n",
        "                    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "8PZLGWtln3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"Hi\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "QDzznVAv4pvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X = df['Polarity', 'Tweet_final_sent']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X['Tweet_final_sent'], X['Polarity'], test_size=0.25, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XTbL9jXA97B4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "mhpBw2z0AaNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# X = cv.fit_transform(df['Tweet_lemma']).toarray()"
      ],
      "outputs": [],
      "metadata": {
        "id": "A5XuISksn3mQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X"
      ],
      "outputs": [],
      "metadata": {
        "id": "3DGeOOIin3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "XsU_eJ5An3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, df['Polarity'], test_size=0.25, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nFtvESIZn3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# X_train = X[:80000,:]\n",
        "# X_test = X[80000:,:]\n",
        "# y_train = df['Polarity'][:80000]\n",
        "# y_test = df['Polarity'][80000:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "_ypp_tZnn3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "pD79Odccn3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train"
      ],
      "outputs": [],
      "metadata": {
        "id": "orS-wg43n3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model_run(model, X_train, y_train):\n",
        "    model.fit(X_train, y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mTd8FBWUn3mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model_predict(model, X_test, y_test):\n",
        "    print('Accuracy is: ', model.score(X_test, y_test)*100)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "outputs": [],
      "metadata": {
        "id": "wqpCXItln3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = MultinomialNB()\n",
        "model_run(model, X_train, y_train)\n",
        "model_predict(model, X_test, y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "uQ87hKTBn3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# model = LogisticRegression()\n",
        "# model_run(model, X_train, y_train)\n",
        "# model_predict(model, X_test, y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "00iZcq3hn3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_predict(model, X_test, y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gG2O5mrBn3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = LinearSVC()\n",
        "model_run(model, X_train, y_train)\n",
        "model_predict(model, X_test, y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9nel28Mkn3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "iFivm_OIn3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "2YYEKtBZn3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
        "                               ngram_range=(1,1)) # number of n-grams\n",
        "tfidf_data = tfidf_counts.fit_transform(df['Tweet_sent'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "4yX_9h_On3mS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# tfidf_counts = TfidfVectorizer()\n",
        "# tfidf_data = tfidf_counts.fit_transform(a)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cSrxzm9Jn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_data.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "A9fQlzEJn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data, df['Polarity'], test_size=0.25, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "uigcoDrPn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)\n",
        "print(y_train_tfidf.shape)\n",
        "print(y_test_tfidf.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "H6qOVYs8n3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = MultinomialNB()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mESGywGun3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = LinearSVC()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AZx8UNnPn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# model = LogisticRegression()\n",
        "# model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "# model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-RV7QCEqn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "t0fs6PGyn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def remove_extra_words(tweet):\n",
        "    tweet = [word for word in tweet if word in uni_top_common_list]\n",
        "    if len(tweet) == 0:\n",
        "        tweet = ['None']\n",
        "    return tweet"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z7XlAJhKn3mT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['Tweet_remove_extra'] = df['Tweet_lemma'].apply(remove_extra_words)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_clean</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pos</th>\n",
              "      <th>Tweet_lemma</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_remove_extra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[(miss, JJ), (nikki, NN), (nu, JJ), (nu, JJ), ...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>miss nikki nu nu already shes always need than...</td>\n",
              "      <td>[miss, already, shes, always, need, thank, xxx]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>So had dream last night remember sign which cl...</td>\n",
              "      <td>[So, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[(So, RB), (dream, NN), (last, JJ), (night, NN...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>So dream last night remember sign clearly tell...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, tell,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>ohh poor sickly you hugs hope you feel little ...</td>\n",
              "      <td>[ohh, poor, sickly, you, hugs, hope, you, feel...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[(ohh, JJ), (poor, JJ), (sickly, JJ), (hugs, N...</td>\n",
              "      <td>[ohh, poor, sickly, hug, hope, feel, little, g...</td>\n",
              "      <td>ohh poor sickly hug hope feel little good soon</td>\n",
              "      <td>[hug, hope, feel, little, good, soon]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[(raining, VBG)]</td>\n",
              "      <td>[rain]</td>\n",
              "      <td>rain</td>\n",
              "      <td>[rain]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>wish was in LA right now</td>\n",
              "      <td>[wish, was, in, LA, right, now]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[(wish, JJ), (LA, NNP), (right, NN)]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>wish LA right</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                 Tweet_remove_extra\n",
              "514293         0  ...    [miss, already, shes, always, need, thank, xxx]\n",
              "142282         0  ...  [So, dream, last, night, remember, sign, tell,...\n",
              "403727         0  ...              [hug, hope, feel, little, good, soon]\n",
              "649503         0  ...                                             [rain]\n",
              "610789         0  ...                                  [wish, LA, right]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "jvmXb6ehn3mU",
        "outputId": "d38dce81-97de-441a-ac83-277822d5d5d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = make_sentences(df, 'Tweet_remove_extra', 'Tweet_final_sent')\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_regex</th>\n",
              "      <th>Tweet_clean</th>\n",
              "      <th>Tweet_stopword</th>\n",
              "      <th>Tweet_clitics</th>\n",
              "      <th>Tweet_shortforms</th>\n",
              "      <th>Tweet_pos</th>\n",
              "      <th>Tweet_lemma</th>\n",
              "      <th>Tweet_sent</th>\n",
              "      <th>Tweet_remove_extra</th>\n",
              "      <th>Tweet_final_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>514293</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
              "      <td>miss nikki nu nu already shes always there whe...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, t...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>[(miss, JJ), (nikki, NN), (nu, JJ), (nu, JJ), ...</td>\n",
              "      <td>[miss, nikki, nu, nu, already, shes, always, n...</td>\n",
              "      <td>miss nikki nu nu already shes always need than...</td>\n",
              "      <td>[miss, already, shes, always, need, thank, xxx]</td>\n",
              "      <td>miss already shes always need thank xxx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142282</th>\n",
              "      <td>0</td>\n",
              "      <td>So I had a dream last night. I  remember a sig...</td>\n",
              "      <td>So had dream last night remember sign which cl...</td>\n",
              "      <td>[So, had, dream, last, night, remember, sign, ...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>[(So, RB), (dream, NN), (last, JJ), (night, NN...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, clear...</td>\n",
              "      <td>So dream last night remember sign clearly tell...</td>\n",
              "      <td>[So, dream, last, night, remember, sign, tell,...</td>\n",
              "      <td>So dream last night remember sign tell get job...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403727</th>\n",
              "      <td>0</td>\n",
              "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
              "      <td>ohh poor sickly you hugs hope you feel little ...</td>\n",
              "      <td>[ohh, poor, sickly, you, hugs, hope, you, feel...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[ohh, poor, sickly, hugs, hope, feel, little, ...</td>\n",
              "      <td>[(ohh, JJ), (poor, JJ), (sickly, JJ), (hugs, N...</td>\n",
              "      <td>[ohh, poor, sickly, hug, hope, feel, little, g...</td>\n",
              "      <td>ohh poor sickly hug hope feel little good soon</td>\n",
              "      <td>[hug, hope, feel, little, good, soon]</td>\n",
              "      <td>hug hope feel little good soon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649503</th>\n",
              "      <td>0</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>it is raining again</td>\n",
              "      <td>[it, is, raining, again]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[raining]</td>\n",
              "      <td>[(raining, VBG)]</td>\n",
              "      <td>[rain]</td>\n",
              "      <td>rain</td>\n",
              "      <td>[rain]</td>\n",
              "      <td>rain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610789</th>\n",
              "      <td>0</td>\n",
              "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
              "      <td>wish was in LA right now</td>\n",
              "      <td>[wish, was, in, LA, right, now]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>[(wish, JJ), (LA, NNP), (right, NN)]</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>wish LA right</td>\n",
              "      <td>[wish, LA, right]</td>\n",
              "      <td>wish LA right</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Polarity  ...                                   Tweet_final_sent\n",
              "514293         0  ...            miss already shes always need thank xxx\n",
              "142282         0  ...  So dream last night remember sign tell get job...\n",
              "403727         0  ...                     hug hope feel little good soon\n",
              "649503         0  ...                                               rain\n",
              "610789         0  ...                                      wish LA right\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "HATMatIln3mU",
        "outputId": "a58ccc56-e601-4464-d83e-340d77dd8165"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf_counts_clean = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
        "                               ngram_range=(1,2)) # number of n-grams\n",
        "tfidf_data_clean = tfidf_counts_clean.fit_transform(df['Tweet_final_sent'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "fyEMK9Zgn3mU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data_clean, df['Polarity'], test_size=0.25, random_state=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MOAglaNrn3mU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X_train_tfidf.shape)\n",
        "print(X_test_tfidf.shape)\n",
        "print(y_train_tfidf.shape)\n",
        "print(y_test_tfidf.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 69746)\n",
            "(10000, 69746)\n",
            "(30000,)\n",
            "(10000,)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmdAtf87n3mU",
        "outputId": "02b91324-d84f-4518-fc46-eebcf973d06b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "zwYS2-hIn3mV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = MultinomialNB()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is:  70.34\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.70      0.70      5048\n",
            "           4       0.70      0.71      0.70      4952\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.70      0.70      0.70     10000\n",
            "weighted avg       0.70      0.70      0.70     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIswNSMrn3mV",
        "outputId": "add4a826-b502-4358-e696-56cb24e8350c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = LinearSVC()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is:  69.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.66      0.69      5048\n",
            "           4       0.68      0.73      0.70      4952\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.70      0.70      0.70     10000\n",
            "weighted avg       0.70      0.70      0.70     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVWsBS3Jn3mV",
        "outputId": "111bbdeb-32c3-42e6-d263-54708a2c7208"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = LogisticRegression()\n",
        "model_run(model, X_train_tfidf, y_train_tfidf)\n",
        "model_predict(model, X_test_tfidf, y_test_tfidf)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is:  71.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.69      0.71      5048\n",
            "           4       0.70      0.74      0.72      4952\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.71      0.71      0.71     10000\n",
            "weighted avg       0.71      0.71      0.71     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCZZ5xLpn3mV",
        "outputId": "b2c74952-1dbf-49f7-c119-1d097b35e27b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "OBmCEDNxn3mV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "outputs": [],
      "metadata": {
        "id": "tlctbH1y9sBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text_clf = Pipeline([\n",
        "    ('tfidf',TfidfVectorizer(preprocessor=None,\n",
        "                             tokenizer=word_tokenize,\n",
        "                             analyzer='word',\n",
        "                             stop_words=None,\n",
        "                             strip_accents=None,\n",
        "                             lowercase=True,\n",
        "                             ngram_range=(1,3),\n",
        "                             min_df=0.0001,\n",
        "                             max_df=0.9,\n",
        "                             binary=False,\n",
        "                             norm='l2',\n",
        "                             use_idf=1,\n",
        "                             smooth_idf=1,\n",
        "                             sublinear_tf=1)),\n",
        "    ('clf', LogisticRegression(penalty='l2',\n",
        "                               solver='saga',\n",
        "                               multi_class='multinomial',\n",
        "                              tol=1e-5,\n",
        "                              n_jobs = -1)),\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "xuG25I_j9r7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "text_clf.fit(X_train,y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "4U8vp1Fu9srh"
      }
    }
  ]
}