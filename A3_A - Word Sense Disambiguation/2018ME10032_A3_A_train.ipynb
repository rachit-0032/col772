{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018ME10032_A3_A_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjwfx8T5OAJ"
      },
      "source": [
        "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
        "\n",
        "Give read access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spAvH1fF0Rhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8b4ab0-c07b-4290-dc9b-b71d13844bc8"
      },
      "source": [
        "## DONT CHANGE THIS CELL\n",
        "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-11 13:32:28--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
            "--2021-10-11 13:32:29--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217313 (212K) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 212.22K   115KB/s    in 1.8s    \n",
            "\n",
            "2021-10-11 13:32:31 (115 KB/s) - ‘data.zip’ saved [217313/217313]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd5KLYRl3T5G",
        "outputId": "7d343f09-530a-4871-aa7d-8fbfa677f248"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/train/\n",
            "  inflating: data/train/train.gold.txt  \n",
            "  inflating: data/train/train.data.txt  \n",
            "   creating: data/validation/\n",
            "  inflating: data/validation/validation.data.txt  \n",
            "  inflating: data/validation/validation.gold.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrnkLN2LzlDB"
      },
      "source": [
        "## Import relevant packages\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "from torchtext.vocab import GloVe"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g13Lsp5DKjth"
      },
      "source": [
        "## Limiting randomness\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSN3xR8POwWx"
      },
      "source": [
        "## Creating .CSV Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG74LdzAQGFE"
      },
      "source": [
        "def csv_creator(path_data='/content/data/train/train.data.txt', path_gold='/content/data/train/train.gold.txt', path_dir='content/', is_label=False, kind='train'):\n",
        "    ## Reading Data File\n",
        "    train_list = []\n",
        "    count = 1\n",
        "    with open(path_data, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip('\\n')\n",
        "            line = line.split('\\t')\n",
        "            token = line[0]\n",
        "            entity = line[1]\n",
        "            pos = line[2].split('-')\n",
        "            pos1 = pos[0]\n",
        "            pos2 = pos[1]\n",
        "            sent1 = line[3]\n",
        "            ## Punctuation if removed shall change the current order of words \n",
        "            ## and thus the significance of the indices given shall be lost\n",
        "            sent1 = ' '.join(sent1.split())\n",
        "            sent2 = line[4]\n",
        "            sent2 = ' '.join(sent2.split())\n",
        "            train_list.append({\"Token\": token, \"Entity\": entity, \"Position1\": pos1, \n",
        "                              \"Position2\": pos2, \"Sentence1\": sent1, \"Sentence2\": sent2})\n",
        "        f.close()\n",
        "\n",
        "    ## Creating DataFrame for the data\n",
        "    train_df = pd.DataFrame(train_list)\n",
        "\n",
        "    ## Label Encoding\n",
        "    train_df['Entity'] = np.where(train_df['Entity'] == 'V', 1, 0)\n",
        "\n",
        "    # Reading Gold Labels\n",
        "    if is_label:\n",
        "        label_list = []\n",
        "        with open(path_gold, 'r') as f:\n",
        "            label_list = f.read().splitlines()\n",
        "            f.close()\n",
        "\n",
        "        ## Label Encoding\n",
        "        for i in range(len(label_list)):\n",
        "            if label_list[i] == 'T':\n",
        "                label_list[i] = 1\n",
        "            else:\n",
        "                label_list[i] = 0\n",
        "        train_df['Label'] = label_list\n",
        "    \n",
        "    ## Saving Data File\n",
        "    path_csv = path_dir + str(kind) + '.csv'\n",
        "    train_df.to_csv(path_csv)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4BgJCwCdpO-"
      },
      "source": [
        "## Creating dataset for Train File\n",
        "csv_creator(path_data='/content/data/train/train.data.txt', path_gold='/content/data/train/train.gold.txt', path_dir = '/content/', is_label=True, kind='train')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3zJp-i3fG-d"
      },
      "source": [
        "## Creating dataset for Validation File\n",
        "csv_creator(path_data='/content/data/validation/validation.data.txt', path_gold='/content/data/validation/validation.gold.txt', path_dir = '/content/', is_label=True, kind='validation')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imwAHxGxOz1L"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJPM6EXzO0OL"
      },
      "source": [
        "## Converting Dataset to Iteratable Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXmccLMRWCn8",
        "outputId": "d7b1a475-3956-44f6-ed71-34785937bdb2"
      },
      "source": [
        "## Checking available device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gXqTnez9L3I"
      },
      "source": [
        "tokenizer = lambda x: x.split()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBGw0exyWclL"
      },
      "source": [
        "## Creating two necessary fields to encode the structure of the datasets\n",
        "text_field = Field(sequential=True, use_vocab=True, tokenize=tokenizer, lower=True, batch_first=True, include_lengths=True)\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwGGFINwfrIs"
      },
      "source": [
        "## Defining Fields for Training and Validation Data\n",
        "fields = {'Token': ('t', text_field), 'Entity': ('e', label_field), 'Position1': ('p1', label_field), \n",
        "          'Position2': ('p2', label_field), 'Sentence1': ('s1', text_field), 'Sentence2': ('s2', text_field), 'Label': ('l', label_field)}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a10xbjbCOMar"
      },
      "source": [
        "## Converting dataset into torch readable format using TorchText\n",
        "train_data, test_data = TabularDataset.splits(\n",
        "    path = '/content/',\n",
        "    train = 'train.csv',\n",
        "    test = 'validation.csv',\n",
        "    format = 'csv',\n",
        "    fields = fields,\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "645LwL47gCHs",
        "outputId": "915b965f-6a44-43da-f5f3-5a104f7ce603"
      },
      "source": [
        "print(train_data[0].__dict__.keys())\n",
        "print(train_data[0].__dict__.values())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['t', 'e', 'p1', 'p2', 's1', 's2', 'l'])\n",
            "dict_values([['carry'], '1', '2', '1', ['you', 'must', 'carry', 'your', 'camping', 'gear', '.'], ['sound', 'carries', 'well', 'over', 'water', '.'], '0'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRISgwauh2ZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df35015e-946b-414c-ebe4-037f7abd01e8"
      },
      "source": [
        "# Building Vocabulary\n",
        "text_field.build_vocab(train_data, min_freq=1, vectors='glove.6B.300d')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:45<00:00, 8750.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcsR0GN4iTyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74166feb-b03f-4c53-e0b8-ccae7354fe57"
      },
      "source": [
        "len(text_field.vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7460"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG0MFNfniZ4C"
      },
      "source": [
        "# print(list(text_field.vocab.stoi.items()))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjd5Groai0JR"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjTDXamHOMYX"
      },
      "source": [
        "## Splitting data into batches\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data),\n",
        "    batch_size=32,\n",
        "    sort_key=lambda x: len(list(x.s1)),\n",
        "    device=device,\n",
        "    sort_within_batch=True,\n",
        "    shuffle=False         ## To ensure no shuffling of data is there\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeHadmtx7uA1"
      },
      "source": [
        "## Checking shuffling (if there)\n",
        "# count = 0\n",
        "# for batch_no, batch in enumerate(train_iterator):\n",
        "#   if count == 0:\n",
        "#     text, batch_len = batch.s1\n",
        "#     print(text, batch_len)\n",
        "#     print(batch.l)\n",
        "#     count += 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXMeqzmbd5W"
      },
      "source": [
        "## Checking batches\n",
        "# count = 0\n",
        "# for batch in train_iterator:\n",
        "#     if count < 5:\n",
        "#         print(batch)\n",
        "#         count += 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APu09fLTSkF_"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVXQUCxSi1J"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKAxN1hEe2A9"
      },
      "source": [
        "class BiLSTM_WordMeaningComparison(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim,\n",
        "                 hidden, n_layers):\n",
        "        super(BiLSTM_WordMeaningComparison, self).__init__()    ## loading from parent class\n",
        "        self.hidden = hidden\n",
        "        self.n_layers = n_layers\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.embed.weight.data.copy_(text_field.vocab.vectors)\n",
        "        self.embed.weight.requires_grad = False     ## Freezing the embeddings\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=True,     ## makes it a BiLSTM\n",
        "                            batch_first=True)\n",
        "        drp1 = 0.5      # dropout probability\n",
        "        self.dropout1 = nn.Dropout(drp1)\n",
        "        self.fc1 = nn.Linear(hidden * 2, 32)        ## fully connected layer\n",
        "\n",
        "    def forward(self, input, actual_batch_len):\n",
        "        embed_out = self.embed(input)\n",
        "        hidden = torch.zeros(self.n_layers * 2, input.shape[0], self.hidden, requires_grad=True).to(device)   ## hidden state\n",
        "        cell = torch.zeros( self.n_layers * 2, input.shape[0], self.hidden, requires_grad=True).to(device)    ## cell state\n",
        "        out_lstm, (hidden, cell) = self.lstm(embed_out,(hidden, cell)) # Applying BiLSTM Model\n",
        "        out_lstm = self.dropout1(out_lstm)                             # Applying dropout\n",
        "        out_lstm = self.fc1(out_lstm)                                  # Applying Fully connected layer\n",
        "        return out_lstm\n",
        "\n",
        "## Defining input parameters for the model\n",
        "vocab_dim = len(text_field.vocab)\n",
        "embedding_dim = text_field.vocab.vectors.shape[1]\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "model = BiLSTM_WordMeaningComparison(vocab_dim, embedding_dim, hidden_dim, num_layers)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjAbfdLUyFnt",
        "outputId": "97082cdf-ebed-4adb-d32d-1c8b675787aa"
      },
      "source": [
        "## Defining Optimizer and Criterion\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CosineEmbeddingLoss()          ## to gain a measure of similarity between the two word embeddings\n",
        "model.to(device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_WordMeaningComparison(\n",
              "  (embed): Embedding(7460, 300)\n",
              "  (lstm): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
              "  (dropout1): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtd2SFYAShAg"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMfCwALBzMko"
      },
      "source": [
        "## Defining accuracy measurements\n",
        "def accuracy(preds, y):\n",
        "    acc = torch.sum(preds == y) / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "## Function to Calculate Loss\n",
        "def calculateLoss(model, batch, criterion):\n",
        "    ## Running BiLSTM for Sentence 1\n",
        "    text, text_len = batch.s1\n",
        "    p1 = batch.p1\n",
        "    embeddings = model(text, text_len.to(device))\n",
        "\n",
        "    ## Running BiLSTM for Sentence 2\n",
        "    text_2, text_len_2 = batch.s2\n",
        "    p2 = batch.p2\n",
        "    embeddings_2 = model(text_2, text_len_2.to(device))\n",
        "\n",
        "    ## Picking out the specific word embeddings from the output for sentence 1\n",
        "    j = 1\n",
        "    first = embeddings[0, p1[0], :]\n",
        "    for i in p1[1:]:\n",
        "        first = torch.vstack((first, embeddings[j, i, :]))\n",
        "        j += 1\n",
        "\n",
        "    ## Picking out the specific word embeddings from the output for sentence 2\n",
        "    j = 1\n",
        "    second = embeddings_2[0, p2[0], :]\n",
        "    for i in p2[1:]:\n",
        "        second = torch.vstack((second, embeddings_2[j, i, :]))\n",
        "        j += 1\n",
        "\n",
        "    ## Creating similarity and dissimilarity vector for segregating results\n",
        "    pred = batch.l.float()\n",
        "    y = pred.clone().detach()       ## creating copy of the batch labels\n",
        "    for i in range(len(y)):\n",
        "      if y[i] == 0:\n",
        "          y[i] = -1\n",
        "\n",
        "    ## Calculating Loss\n",
        "    loss = criterion(first, second, y)\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    res = cos(first, second).float()\n",
        "\n",
        "    for i in range(len(res)):\n",
        "        if res[i] > 0.75:      \n",
        "          res[i] = 1\n",
        "        else:\n",
        "          res[i] = 0\n",
        "\n",
        "    ## Generating Accuracy\n",
        "    acc = accuracy(res, pred)\n",
        "    return loss, batch.l.shape[0], acc"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1fmhDx9WWj"
      },
      "source": [
        "## Checkpointing: Storing model state\n",
        "def save_ckp(path, epoch, model, opt, loss):\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, path)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g2vyew_zL0b",
        "outputId": "ea167153-873d-436f-c729-d05139dfa94b"
      },
      "source": [
        "epochs_total = 20\n",
        "val_loss_min = 10\n",
        "for epoch in range(epochs_total):\n",
        "    model.train()       ## Training the model\n",
        "    train_len, train_acc, train_loss  = 0, [], []\n",
        "    for batch_no, batch in enumerate(train_iterator):\n",
        "        opt.zero_grad()\n",
        "        loss, batch_len, acc = calculateLoss(model, batch, criterion)\n",
        "        train_loss.append(loss * batch_len)\n",
        "        train_acc.append(acc * batch_len)\n",
        "        train_len = train_len + batch_len\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    ## Calculating the loss and accuracy terms for each epoch\n",
        "    train_epoch_loss = np.sum(train_loss)/train_len\n",
        "    train_epoch_acc = np.sum(train_acc)/train_len\n",
        "    model.eval()\n",
        "\n",
        "    ## Finding out the validation results; backward step not needed here since this is not be trained on\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iterator:\n",
        "            val_results = [calculateLoss(model, batch,criterion) for batch in test_iterator]\n",
        "            loss, batch_len, valid_acc = zip(*val_results)\n",
        "            epoch_loss = np.sum(np.multiply(loss, batch_len))/np.sum(batch_len)\n",
        "            epoch_acc  = np.sum(np.multiply(valid_acc , batch_len))/np.sum(batch_len)\n",
        "        print('Epoch Number:{}/{} | Training Loss: {:.4f}, Training Accuracy: {:.4f}'\n",
        "              ' Validation Loss: {:.4f}, Validation Accuracy: {:.4f}'\n",
        "              .format(epoch+1, epochs_total, train_epoch_loss.item(), \n",
        "                      train_epoch_acc.item(), epoch_loss.item(), epoch_acc.item()))\n",
        "\n",
        "        ## Saving the model for least val_loss\n",
        "        val_loss = epoch_loss.item()\n",
        "        val_acc = epoch_acc.item()\n",
        "        if val_loss < val_loss_min:\n",
        "            val_loss_min = val_loss\n",
        "            print('Validation Loss Decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
        "            ## Saving checkpoint\n",
        "            save_ckp('model.txt', epoch+1, model, opt, val_loss_min)\n",
        "            "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Number:1/20 | Training Loss: 0.4506, Training Accuracy: 0.5621 Validation Loss: 0.4773, Validation Accuracy: 0.5313\n",
            "Validation Loss Decreased (0.477338 --> 0.477338).  Saving model ...\n",
            "Epoch Number:2/20 | Training Loss: 0.4108, Training Accuracy: 0.6275 Validation Loss: 0.4719, Validation Accuracy: 0.5564\n",
            "Validation Loss Decreased (0.471850 --> 0.471850).  Saving model ...\n",
            "Epoch Number:3/20 | Training Loss: 0.3636, Training Accuracy: 0.6630 Validation Loss: 0.4576, Validation Accuracy: 0.5627\n",
            "Validation Loss Decreased (0.457590 --> 0.457590).  Saving model ...\n",
            "Epoch Number:4/20 | Training Loss: 0.3321, Training Accuracy: 0.6995 Validation Loss: 0.4527, Validation Accuracy: 0.5517\n",
            "Validation Loss Decreased (0.452698 --> 0.452698).  Saving model ...\n",
            "Epoch Number:5/20 | Training Loss: 0.3017, Training Accuracy: 0.7229 Validation Loss: 0.4423, Validation Accuracy: 0.5752\n",
            "Validation Loss Decreased (0.442340 --> 0.442340).  Saving model ...\n",
            "Epoch Number:6/20 | Training Loss: 0.2698, Training Accuracy: 0.7623 Validation Loss: 0.4505, Validation Accuracy: 0.5799\n",
            "Epoch Number:7/20 | Training Loss: 0.2498, Training Accuracy: 0.7808 Validation Loss: 0.4594, Validation Accuracy: 0.5768\n",
            "Epoch Number:8/20 | Training Loss: 0.2196, Training Accuracy: 0.8134 Validation Loss: 0.4587, Validation Accuracy: 0.5658\n",
            "Epoch Number:9/20 | Training Loss: 0.2013, Training Accuracy: 0.8316 Validation Loss: 0.4524, Validation Accuracy: 0.5784\n",
            "Epoch Number:10/20 | Training Loss: 0.1756, Training Accuracy: 0.8550 Validation Loss: 0.4507, Validation Accuracy: 0.5815\n",
            "Epoch Number:11/20 | Training Loss: 0.1620, Training Accuracy: 0.8716 Validation Loss: 0.4385, Validation Accuracy: 0.5987\n",
            "Validation Loss Decreased (0.438517 --> 0.438517).  Saving model ...\n",
            "Epoch Number:12/20 | Training Loss: 0.1486, Training Accuracy: 0.8832 Validation Loss: 0.4292, Validation Accuracy: 0.6003\n",
            "Validation Loss Decreased (0.429190 --> 0.429190).  Saving model ...\n",
            "Epoch Number:13/20 | Training Loss: 0.1372, Training Accuracy: 0.8937 Validation Loss: 0.4361, Validation Accuracy: 0.5956\n",
            "Epoch Number:14/20 | Training Loss: 0.1229, Training Accuracy: 0.9099 Validation Loss: 0.4412, Validation Accuracy: 0.5846\n",
            "Epoch Number:15/20 | Training Loss: 0.1091, Training Accuracy: 0.9237 Validation Loss: 0.4433, Validation Accuracy: 0.5940\n",
            "Epoch Number:16/20 | Training Loss: 0.0979, Training Accuracy: 0.9346 Validation Loss: 0.4496, Validation Accuracy: 0.5925\n",
            "Epoch Number:17/20 | Training Loss: 0.0892, Training Accuracy: 0.9412 Validation Loss: 0.4595, Validation Accuracy: 0.5815\n",
            "Epoch Number:18/20 | Training Loss: 0.0798, Training Accuracy: 0.9512 Validation Loss: 0.4517, Validation Accuracy: 0.5737\n",
            "Epoch Number:19/20 | Training Loss: 0.0724, Training Accuracy: 0.9573 Validation Loss: 0.4509, Validation Accuracy: 0.6003\n",
            "Epoch Number:20/20 | Training Loss: 0.0714, Training Accuracy: 0.9580 Validation Loss: 0.4471, Validation Accuracy: 0.5752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5DW3tHmiP31"
      },
      "source": [
        "vocab = text_field.vocab\n",
        "torch.save(vocab, 'vocab.txt')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEwqREFPziX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7189dc7b-2f73-4186-ec2c-e0dcd2b741d3"
      },
      "source": [
        "## Zip the final model and all the required files, such as vocabulary\n",
        "# Replace USERID with your own, such as 2017CSZ8058\n",
        "!zip -r 2018ME10032_A_model.zip model.txt vocab.txt\n",
        "\n",
        "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model.txt (deflated 9%)\n",
            "  adding: vocab.txt (deflated 11%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbNCw4Gcm4Mg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}